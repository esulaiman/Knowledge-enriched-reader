{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtR9EZKWcGIw",
        "outputId": "8b10d5a1-3650-47d8-dfe0-df0c72d6d46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 11.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n",
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.2.0.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from camel-tools) (4.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.3.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.10.0+cu111)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from camel-tools) (4.16.2)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from camel-tools) (2.23.0)\n",
            "Collecting camel-kenlm\n",
            "  Downloading camel-kenlm-2021.12.27.tar.gz (418 kB)\n",
            "\u001b[K     |████████████████████████████████| 418 kB 25.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->camel-tools) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (0.0.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (4.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (0.4.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (0.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.0.2->camel-tools) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.2->camel-tools) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->camel-tools) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->camel-tools) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->camel-tools) (3.0.0)\n",
            "Building wheels for collected packages: camel-tools, camel-kenlm\n",
            "  Building wheel for camel-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-tools: filename=camel_tools-1.2.0-py3-none-any.whl size=99048 sha256=818aed93b9bb6ac721e49f62d8a62b37f56f749ace09badb135434d267eabe40\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ad/a1/e8aa569c102f0b8b3522ae515b7d0696046e4490c0ff4edb0a\n",
            "  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2021.12.27-cp37-cp37m-linux_x86_64.whl size=2336835 sha256=4092bfb0fa2394714382ab8769786b8038743a00f624688a1497a8f8f01c2803\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/72/74/982f8c435f15b7feaf6dc8a03e212ff34e93f1f2d747059332\n",
            "Successfully built camel-tools camel-kenlm\n",
            "Installing collected packages: camel-kenlm, camel-tools\n",
            "Successfully installed camel-kenlm-2021.12.27 camel-tools-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install camel-tools\n",
        "!camel_data full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdaGIpsavp3D"
      },
      "outputs": [],
      "source": [
        "# preprocess data to remove diacratics and be consistent with BERT\n",
        "!git clone https://github.com/aub-mind/arabert\n",
        "!cp arabert/examples/question-answering/utils_qa.py .\n",
        "!cp arabert/examples/question-answering/trainer_qa.py .\n",
        "!cp arabert/examples/question-answering/run_qa.py .\n",
        "!cp arabert/examples/question-answering/squad_preprocessing.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtHzLcQ4v60I",
        "outputId": "610e193a-b58d-4e34-853c-7d5d1106d6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.18.2-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 12.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.10.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 52.4 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.10)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.2 frozenlist-1.3.0 fsspec-2022.1.0 multidict-6.0.2 xxhash-2.0.2 yarl-1.7.2\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 12.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farasapy) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (2021.10.8)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.14\n",
            "Collecting fuzzysearch\n",
            "  Downloading fuzzysearch-0.7.3.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 11.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.3 in /usr/local/lib/python3.7/dist-packages (from fuzzysearch) (21.4.0)\n",
            "Building wheels for collected packages: fuzzysearch\n",
            "  Building wheel for fuzzysearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzysearch: filename=fuzzysearch-0.7.3-cp37-cp37m-linux_x86_64.whl size=280757 sha256=215dc822ee144852475810ef9deb1e5fdf34d39bebbc92c1d868fdcd1960d69c\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/11/b5/bae34cc61880d909103c30a1c547df857f5a46adeabfd922b5\n",
            "Successfully built fuzzysearch\n",
            "Installing collected packages: fuzzysearch\n",
            "Successfully installed fuzzysearch-0.7.3\n"
          ]
        }
      ],
      "source": [
        "model_name=\"lanwuwei/GigaBERT-v3-Arabic-and-English\"\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "#only needed for AraBERTv1 and v2\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!pip install fuzzysearch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pok8il7bwC9k"
      },
      "source": [
        "# Preprocess the data to remove diacracticts and be consistant with BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB7i3I-4BSi-",
        "outputId": "6e6da359-4e3d-46aa-d263-abed41c73761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwmkUPG5wCJD"
      },
      "outputs": [],
      "source": [
        "!rm -rf *-pre.json\n",
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/data/tydiqa-goldp-v1.1-train-ar.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-train-ar-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9ieE6YIyjUv"
      },
      "outputs": [],
      "source": [
        "!rm -rf *-pre.json\n",
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/data/tydiqa-goldp-v1.1-dev-ar.json\" \\\n",
        "  --output_file \"/content/drive/MyDrive/KGE_checkpoints/kge-master/tydiqa-goldp-v1.1-dev-ar-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDvzbVutwwSY"
      },
      "source": [
        "# load data, concepts and concept embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pqJxOTWUC87a",
        "outputId": "82b5970f-89e0-4235-f212-bed0b8537426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KGE_checkpoints/kge-master\n",
            "Obtaining file:///content/drive/MyDrive/KGE_checkpoints/kge-master\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 13 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from libkge==0.1) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from libkge==0.1) (1.1.5)\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting path\n",
            "  Downloading path-16.3.0-py3-none-any.whl (21 kB)\n",
            "Collecting ax-platform==0.1.19\n",
            "  Downloading ax_platform-0.1.19-py3-none-any.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from libkge==0.1) (1.4.31)\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Collecting numba==0.53.1\n",
            "  Downloading numba-0.53.1-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.19->libkge==0.1) (5.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.19->libkge==0.1) (1.0.2)\n",
            "Collecting botorch>=0.3.3\n",
            "  Downloading botorch-0.6.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.19->libkge==0.1) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.19->libkge==0.1) (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.19->libkge==0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba==0.53.1->libkge==0.1) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53.1->libkge==0.1) (57.4.0)\n",
            "Collecting llvmlite<0.37,>=0.36.0rc1\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->libkge==0.1) (3.10.0.2)\n",
            "Collecting botorch>=0.3.3\n",
            "  Downloading botorch-0.5.1-py3-none-any.whl (486 kB)\n",
            "\u001b[K     |████████████████████████████████| 486 kB 64.9 MB/s \n",
            "\u001b[?25h  Downloading botorch-0.5.0-py3-none-any.whl (475 kB)\n",
            "\u001b[K     |████████████████████████████████| 475 kB 68.2 MB/s \n",
            "\u001b[?25h  Downloading botorch-0.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[K     |████████████████████████████████| 395 kB 74.3 MB/s \n",
            "\u001b[?25hCollecting gpytorch>=1.4\n",
            "  Downloading gpytorch-1.6.0.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->ax-platform==0.1.19->libkge==0.1) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->libkge==0.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->libkge==0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->libkge==0.1) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->ax-platform==0.1.19->libkge==0.1) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ax-platform==0.1.19->libkge==0.1) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ax-platform==0.1.19->libkge==0.1) (1.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->libkge==0.1) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->libkge==0.1) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->libkge==0.1) (3.7.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz->libkge==0.1) (0.10.1)\n",
            "Building wheels for collected packages: gpytorch, torchviz\n",
            "  Building wheel for gpytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpytorch: filename=gpytorch-1.6.0-py2.py3-none-any.whl size=509889 sha256=bc73fedcfbe585c717ee81f58a0303ad8e1527385dc79bd80232324f7848096d\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/b5/89/34c06ad393a6feb72b4cdde46d0f1c667f3e2632960f9df109\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=d1cdb61f8029157d29e2f9cabc5a7355b711c66e950eef0b94d3c7f58535bc18\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built gpytorch torchviz\n",
            "Installing collected packages: torch, gpytorch, llvmlite, botorch, torchviz, path, numba, ax-platform, argparse, libkge\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Running setup.py develop for libkge\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed argparse-1.4.0 ax-platform-0.1.19 botorch-0.4.0 gpytorch-1.6.0 libkge-0.1 llvmlite-0.36.0 numba-0.53.1 path-16.3.0 torch-1.7.1 torchviz-0.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# retrieve and install project in development mode\n",
        "%cd /content/drive/MyDrive/KGE_checkpoints/kge-master\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDXH8r55FZyc",
        "outputId": "e6aec5c3-e766-4c71-fe69-446100ba3b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arabic-reshaper in /usr/local/lib/python3.7/dist-packages (2.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper) (57.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper) (0.16.0)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install arabic-reshaper\n",
        "!pip install python-bidi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR0k6ZSFaRZz"
      },
      "source": [
        "# Preprocess data and concepts and save into pt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "Jo1dBS2oOBCD",
        "outputId": "17e169d4-0951-4c97-b8e8-f994e0a78576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading kg embeddings from /content/drive/MyDrive/KGE_checkpoints/kge-master/local/experiments/20210728-053757-config/checkpoint_best.pt\n",
            "Loading configuration of dataset wikidata from /content/drive/MyDrive/KGE_checkpoints/kge-master/data/wikidata ...\n",
            "Setting complex.relation_embedder.dropout to 0., was set to -0.4746062345802784.\n",
            "Loading entity_ids.del\n",
            "{'id': 'arabic--7873889237709442006-0', 'doc_entities': [{'entity': 'أوروبا', 'start': 129, 'label': 'B-LOC', 'end': 135, 'kb_entity': ['اوروبا', 'اوروبا'], 'concepts': [834, 2768, 332, 35546, 361, 4963, 23115, 343887, 23115, 14186, 21170, 167117, 285741, 814544, 35962, 565229, 65990, 14420, 269077, 777852, 290390, 639910, 79313, 143948, 1081, 469182, 993, 958644, 23328, 1119624, 1146796, 553440, 58349, 98795, 565228, 1069668, 581822, 782128, 831282]}, {'entity': 'القسطنطينية', 'start': 157, 'label': 'B-LOC', 'end': 168, 'kb_entity': ['القسطنطينيه', 'القسطنطينيه'], 'concepts': [10434, 4390, 13503, 14186, 43509, 125820, 10434, 8640, 19770, 4045]}, {'entity': 'أوروبا', 'start': 284, 'label': 'B-LOC', 'end': 290, 'kb_entity': ['اوروبا', 'اوروبا'], 'concepts': [834, 2768, 332, 35546, 361, 4963, 23115, 343887, 23115, 14186, 21170, 167117, 285741, 814544, 35962, 565229, 65990, 14420, 269077, 777852, 290390, 639910, 79313, 143948, 1081, 469182, 993, 958644, 23328, 1119624, 1146796, 553440, 58349, 98795, 565228, 1069668, 581822, 782128, 831282]}, {'entity': 'البلقان', 'start': 301, 'label': 'B-LOC', 'end': 308, 'kb_entity': ['البلقان', 'البلقان'], 'concepts': [110838, 4231, 50970, 6079]}], 'question_entities': []}\n",
            "all_input_id shape:  torch.Size([15429, 384])\n",
            "all_attention_masks shape:  torch.Size([15429, 384])\n",
            "all_token_type_ids shape:  torch.Size([15429, 384])\n",
            "all_start_positions shape:  torch.Size([15429])\n",
            "all_end_positions shape:  torch.Size([15429])\n",
            "all_concept_ids shape:  torch.Size([15429, 384, 40])\n",
            "all_padded_concepts_mask shape:  torch.Size([15429, 384, 40])\n",
            "all_input_id shape:  torch.Size([945, 384])\n",
            "all_attention_masks shape:  torch.Size([945, 384])\n",
            "all_token_type_ids shape:  torch.Size([945, 384])\n",
            "all_concept_ids shape:  torch.Size([945, 384, 40])\n",
            "all_padded_concepts_mask shape:  torch.Size([945, 384, 40])\n",
            "all_start_positions shape:  torch.Size([945])\n",
            "all_end_positions shape:  torch.Size([945])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# 5)preprocess and generate TensorDataset for test set\\n\\n#prepeare tensor of data and concept and save it\\ntest_features=[]\\ntest_offsetmapping=[]\\ntest_example_id=[]\\ni=0\\n\\nfor ex in test_data:\\n   ft, offset_mapping, example_id = proc.prepare_validation_features(ex, wikidata_test_concepts[i])\\n   for f in ft:\\n       test_features.append(f)\\n   for off in offset_mapping:\\n     test_offsetmapping.append(off)\\n   for ex in example_id:\\n     test_example_id.append(ex)\\n   \\n   i+=1\\n\\n# Convert to Tensors and build dev set\\nall_input_ids = torch.tensor([f['input_ids'] for f in test_features], dtype=torch.long)\\nall_attention_masks = torch.tensor([f['attention_masks'] for f in test_features], dtype=torch.long)\\nall_token_type_ids = torch.tensor([f['token_type_ids'] for f in test_features], dtype=torch.long)\\nall_concept_ids = torch.tensor([f['concept_ids'] for f in test_features], dtype=torch.long)\\nall_padded_concepts_mask = torch.tensor([f['padded_concepts_mask'] for f in test_features], dtype=torch.bool)\\nall_start_positions = torch.tensor([f['start_positions'] for f in test_features], dtype=torch.long)\\nall_end_positions = torch.tensor([f['end_positions'] for f in test_features], dtype=torch.long)\\n\\nprint('all_input_id shape: ', all_input_ids.shape)\\nprint('all_attention_masks shape: ', all_attention_masks.shape)\\nprint('all_token_type_ids shape: ', all_token_type_ids.shape)\\nprint('all_concept_ids shape: ', all_concept_ids.shape)\\nprint('all_padded_concepts_mask shape: ', all_padded_concepts_mask.shape)\\nprint('all_start_positions shape: ', all_start_positions.shape)\\nprint('all_end_positions shape: ', all_end_positions.shape)\\n\\n\\n\\ntest_data_tensor = TensorDataset(\\n                all_input_ids,\\n                all_attention_masks,\\n                all_token_type_ids,\\n                all_start_positions,\\n                all_end_positions, \\n                all_concept_ids,\\n                all_padded_concepts_mask\\n                \\n            )\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from kge.model import KgeModel\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.utils.data \n",
        "import pickle\n",
        "import json\n",
        "import sys\n",
        "from typing import List\n",
        "\n",
        "\n",
        "from KGE_enriched_PLM.dataloader import DatasetArabicTyDiQA, DataLoaderTyDiQA\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from KGE_enriched_PLM.model_main import KGEEnrichment\n",
        "from KGE_enriched_PLM.preprocess_main import Processing\n",
        "from tqdm import tqdm\n",
        "\n",
        "from kge.util.io import load_checkpoint\n",
        "import torch\n",
        "import pandas as pd\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "# preprocess data and save data and concepts into pt file\n",
        "\n",
        "def normalize_entity(s):\n",
        "    s= normalize_alef_maksura_ar(s)\n",
        "    s= normalize_teh_marbuta_ar(s)\n",
        "    s= normalize_alef_ar(s)\n",
        "\n",
        "    return s\n",
        "\n",
        "# build a dictinary of embedding. key: entity name, value: embedding\n",
        "def getEntityEmbeddings(kge_model):\n",
        "    e = {}\n",
        "    entity_dict = '/content/drive/MyDrive/KGE_checkpoints/kge-master/data/wikidata/entity_ids.del'\n",
        "   \n",
        "    print('Loading entity_ids.del')\n",
        "    embedder = kge_model._entity_embedder\n",
        "    f = open(entity_dict, 'r')\n",
        "    for line in f:\n",
        "        line = line[:-1].split('\\t')\n",
        "        ent_id = int(line[0])\n",
        "        ent_name = line[1]\n",
        "        ent_name=normalize_entity(ent_name)\n",
        "        e[ent_name] = embedder._embeddings(torch.LongTensor([ent_id]))[0]\n",
        "    f.close()\n",
        "    return e\n",
        "\n",
        "# build consistent entity2idx dict, idx2entity dict and embedding matrix( contains embedding) \n",
        "def prepare_embeddings(embedding_dict):\n",
        "    entity2idx = {}\n",
        "    idx2entity = {}\n",
        "    embedding2entity={}\n",
        "    ents=[]\n",
        "    i = 0\n",
        "    embedding_matrix = []\n",
        "    for key, entity in embedding_dict.items():\n",
        "        entity2idx[key] = i\n",
        "        idx2entity[i] = key\n",
        "        embedding2entity[entity]=key\n",
        "        ents.append(key)\n",
        "        i += 1\n",
        "        embedding_matrix.append(entity)\n",
        "    return entity2idx, idx2entity, embedding_matrix, embedding2entity, ents\n",
        "\n",
        "def load_data(data_path):\n",
        "   \n",
        "  with open(data_path, 'rb') as f:\n",
        "       squad_dict = json.load(f)\n",
        "\n",
        "  data=[]\n",
        "  for group in squad_dict['data']:\n",
        "     for passage in group['paragraphs']:\n",
        "      \n",
        "        for qa in passage['qas']:\n",
        "           ans=[]\n",
        "           example={'title':str,'id':str, 'context':str, 'question':str, 'answers':[]}\n",
        "           example['question']=qa['question']\n",
        "           example['context']=passage['context']\n",
        "           example['id']=qa['id']\n",
        "           example['title']=group['title']\n",
        "           for answer in qa['answers']:\n",
        "              ans.append(answer)\n",
        "\n",
        "           example['answers']=ans\n",
        "           data.append(example)\n",
        "\n",
        "  return data\n",
        "\n",
        "def load_kaiflematha_data(data_path):\n",
        "   \n",
        "  with open(data_path, 'rb') as f:\n",
        "       squad_dict = json.load(f)\n",
        "\n",
        "  data=[]\n",
        "  for group in squad_dict['data']:\n",
        "     #for passage in group['paragraphs']:\n",
        "      \n",
        "        for qa in group['qas']:\n",
        "           ans=[]\n",
        "           example={'title':str,'id':str, 'context':str, 'question':str, 'answers':[]}\n",
        "           example['question']=qa['question']\n",
        "           example['context']=group['context']\n",
        "           example['id']=qa['id']\n",
        "           example['title']=group['title']\n",
        "           for answer in qa['answers']:\n",
        "              ans.append(answer)\n",
        "\n",
        "           example['answers']=ans\n",
        "           data.append(example)\n",
        "\n",
        "  return data\n",
        "\n",
        "def load_concepts(concepts_path):\n",
        "\n",
        "  with open(concepts_path, 'rb') as f:\n",
        "\n",
        "    data = pickle.load(f)\n",
        "  return data\n",
        "\n",
        "#*******************\n",
        "\n",
        "# 1)retrieve embedding\n",
        "checkpoint_file = '/content/drive/MyDrive/KGE_checkpoints/kge-master/local/experiments/20210728-053757-config/checkpoint_best.pt'\n",
        "print('Loading kg embeddings from', checkpoint_file)\n",
        "\n",
        "kge_checkpoint = load_checkpoint(checkpoint_file)\n",
        "kge_model = KgeModel.create_from(kge_checkpoint)\n",
        "\n",
        "entity_embeddings = kge_checkpoint[\"model\"][0][\"_entity_embedder._embeddings.weight\"]\n",
        "relation_embeddings = kge_checkpoint[\"model\"][0][\"_relation_embedder._embeddings.weight\"]\n",
        "\n",
        "# build a dictinary of embedding. key: entity name, value: embedding\n",
        "e = getEntityEmbeddings(kge_model)\n",
        "# build consistent entity2idx dict, idx2entity dict and embedding matrix( contains embedding)\n",
        "entity2idx, idx2entity, embedding_matrix, embedding2entity, ents = prepare_embeddings(e)\n",
        "#**********************************\n",
        "model_name=\"lanwuwei/GigaBERT-v3-Arabic-and-English\"\n",
        "\n",
        "\n",
        "#2) Load  dataset and concepts\n",
        "train_data_path='/content/drive/MyDrive/KGE_checkpoints/Direct_concepts/Datasets/TyDiQA/train-pre.json'\n",
        "dev_data_path='/content/drive/MyDrive/KGE_checkpoints/Direct_concepts/Datasets/TyDiQA/dev-pre.json'\n",
        "#test_data_path='/content/drive/MyDrive/KGE_checkpoints/Direct_concepts/Datasets/KaifLematha/test-pre.json'\n",
        "train_concepts_path= '/content/drive/MyDrive/KGE_checkpoints/Direct_concepts/TyDiQA/wikidata_tydiqa_train_scored_40_concepts.bin'\n",
        "dev_concepts_path= '/content/drive/MyDrive/KGE_checkpoints/Direct_concepts/TyDiQA/wikidata_tydiqa_dev_scored_40_concepts.bin'\n",
        "#test_concepts_path= '/content/drive/MyDrive/KGE_checkpoints/Direct_concepts/KaifLematha/final/test/wikidata_test_scored_40.bin'\n",
        "\n",
        "\n",
        "wikidata_train_concepts= load_concepts(train_concepts_path)\n",
        "wikidata_dev_concepts= load_concepts(dev_concepts_path)\n",
        "#wikidata_test_concepts= load_concepts(test_concepts_path)\n",
        "\n",
        "print(wikidata_train_concepts[0])\n",
        "\n",
        "train_data= load_data(train_data_path)\n",
        "dev_data= load_data(dev_data_path)\n",
        "#test_data= load_data(test_data_path)\n",
        "\n",
        "\n",
        "\n",
        "# 3)preprocess and generate TensorDataset for training set\n",
        "\n",
        "\n",
        "#prepeare tensor of data and concept and save it\n",
        "proc = Processing(entity2idx, model_name)\n",
        "train_features=[]\n",
        "\n",
        "i=0\n",
        "for ex in train_data:\n",
        "   ft, offset_mapping, example_id  = proc.prepare_train_features(ex, wikidata_train_concepts[i])\n",
        "   for f in ft:\n",
        "       train_features.append(f)\n",
        "   \n",
        "\n",
        "   i+=1\n",
        "\n",
        "# Convert to Tensors and build training set\n",
        "all_input_ids = torch.tensor([f['input_ids'] for f in train_features], dtype=torch.long)\n",
        "all_attention_masks = torch.tensor([f['attention_masks'] for f in train_features], dtype=torch.long)\n",
        "all_token_type_ids = torch.tensor([f['token_type_ids'] for f in train_features], dtype=torch.long)\n",
        "all_start_positions = torch.tensor([f['start_positions'] for f in train_features], dtype=torch.long)\n",
        "all_end_positions = torch.tensor([f['end_positions'] for f in train_features], dtype=torch.long)\n",
        "all_concept_ids = torch.tensor([f['concept_ids'] for f in train_features], dtype=torch.long)\n",
        "all_padded_concepts_mask = torch.tensor([f['padded_concepts_mask'] for f in train_features], dtype=torch.bool)\n",
        "\n",
        "print('all_input_id shape: ', all_input_ids.shape)\n",
        "print('all_attention_masks shape: ', all_attention_masks.shape)\n",
        "print('all_token_type_ids shape: ', all_token_type_ids.shape)\n",
        "print('all_start_positions shape: ', all_start_positions.shape)\n",
        "print('all_end_positions shape: ', all_end_positions.shape)\n",
        "print('all_concept_ids shape: ', all_concept_ids.shape)\n",
        "print('all_padded_concepts_mask shape: ', all_padded_concepts_mask.shape)\n",
        "\n",
        "\n",
        "train_data_tensor = TensorDataset(\n",
        "                all_input_ids,\n",
        "                all_attention_masks,\n",
        "                all_token_type_ids,\n",
        "                all_start_positions,\n",
        "                all_end_positions,\n",
        "                all_concept_ids,\n",
        "                all_padded_concepts_mask\n",
        "                \n",
        "            )\n",
        "\n",
        "# 4)preprocess and generate TensorDataset for dev set\n",
        "\n",
        "#prepeare tensor of data and concept and save it\n",
        "val_features=[]\n",
        "all_offsetmapping=[]\n",
        "val_example_id=[]\n",
        "i=0\n",
        "\n",
        "for ex in dev_data:\n",
        "   ft, offset_mapping, example_id = proc.prepare_validation_features(ex, wikidata_dev_concepts[i])\n",
        "   for f in ft:\n",
        "       val_features.append(f)\n",
        "   for off in offset_mapping:\n",
        "     all_offsetmapping.append(off)\n",
        "   for ex in example_id:\n",
        "     val_example_id.append(ex)\n",
        "   \n",
        "   i+=1\n",
        "\n",
        "# Convert to Tensors and build dev set\n",
        "all_input_ids = torch.tensor([f['input_ids'] for f in val_features], dtype=torch.long)\n",
        "all_attention_masks = torch.tensor([f['attention_masks'] for f in val_features], dtype=torch.long)\n",
        "all_token_type_ids = torch.tensor([f['token_type_ids'] for f in val_features], dtype=torch.long)\n",
        "all_concept_ids = torch.tensor([f['concept_ids'] for f in val_features], dtype=torch.long)\n",
        "all_padded_concepts_mask = torch.tensor([f['padded_concepts_mask'] for f in val_features], dtype=torch.bool)\n",
        "all_start_positions = torch.tensor([f['start_positions'] for f in val_features], dtype=torch.long)\n",
        "all_end_positions = torch.tensor([f['end_positions'] for f in val_features], dtype=torch.long)\n",
        "\n",
        "print('all_input_id shape: ', all_input_ids.shape)\n",
        "print('all_attention_masks shape: ', all_attention_masks.shape)\n",
        "print('all_token_type_ids shape: ', all_token_type_ids.shape)\n",
        "print('all_concept_ids shape: ', all_concept_ids.shape)\n",
        "print('all_padded_concepts_mask shape: ', all_padded_concepts_mask.shape)\n",
        "print('all_start_positions shape: ', all_start_positions.shape)\n",
        "print('all_end_positions shape: ', all_end_positions.shape)\n",
        "\n",
        "\n",
        "\n",
        "dev_data_tensor = TensorDataset(\n",
        "                all_input_ids,\n",
        "                all_attention_masks,\n",
        "                all_token_type_ids,\n",
        "                all_start_positions,\n",
        "                all_end_positions, \n",
        "                all_concept_ids,\n",
        "                all_padded_concepts_mask\n",
        "                \n",
        "            )\n",
        "'''\n",
        "# 5)preprocess and generate TensorDataset for test set\n",
        "\n",
        "#prepeare tensor of data and concept and save it\n",
        "test_features=[]\n",
        "test_offsetmapping=[]\n",
        "test_example_id=[]\n",
        "i=0\n",
        "\n",
        "for ex in test_data:\n",
        "   ft, offset_mapping, example_id = proc.prepare_validation_features(ex, wikidata_test_concepts[i])\n",
        "   for f in ft:\n",
        "       test_features.append(f)\n",
        "   for off in offset_mapping:\n",
        "     test_offsetmapping.append(off)\n",
        "   for ex in example_id:\n",
        "     test_example_id.append(ex)\n",
        "   \n",
        "   i+=1\n",
        "\n",
        "# Convert to Tensors and build dev set\n",
        "all_input_ids = torch.tensor([f['input_ids'] for f in test_features], dtype=torch.long)\n",
        "all_attention_masks = torch.tensor([f['attention_masks'] for f in test_features], dtype=torch.long)\n",
        "all_token_type_ids = torch.tensor([f['token_type_ids'] for f in test_features], dtype=torch.long)\n",
        "all_concept_ids = torch.tensor([f['concept_ids'] for f in test_features], dtype=torch.long)\n",
        "all_padded_concepts_mask = torch.tensor([f['padded_concepts_mask'] for f in test_features], dtype=torch.bool)\n",
        "all_start_positions = torch.tensor([f['start_positions'] for f in test_features], dtype=torch.long)\n",
        "all_end_positions = torch.tensor([f['end_positions'] for f in test_features], dtype=torch.long)\n",
        "\n",
        "print('all_input_id shape: ', all_input_ids.shape)\n",
        "print('all_attention_masks shape: ', all_attention_masks.shape)\n",
        "print('all_token_type_ids shape: ', all_token_type_ids.shape)\n",
        "print('all_concept_ids shape: ', all_concept_ids.shape)\n",
        "print('all_padded_concepts_mask shape: ', all_padded_concepts_mask.shape)\n",
        "print('all_start_positions shape: ', all_start_positions.shape)\n",
        "print('all_end_positions shape: ', all_end_positions.shape)\n",
        "\n",
        "\n",
        "\n",
        "test_data_tensor = TensorDataset(\n",
        "                all_input_ids,\n",
        "                all_attention_masks,\n",
        "                all_token_type_ids,\n",
        "                all_start_positions,\n",
        "                all_end_positions, \n",
        "                all_concept_ids,\n",
        "                all_padded_concepts_mask\n",
        "                \n",
        "            )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kqfDy4G9xM6j",
        "outputId": "db97df91-f6d2-45fa-801d-bd9ec2959484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945\n",
            "Creating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at lanwuwei/GigaBERT-v3-Arabic-and-English were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning:\n",
            "\n",
            "This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "\n",
            "  0%|          | 0/1929 [00:00<?, ?batches/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1929/1929 [11:28<00:00,  2.80batches/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Evaluation**\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/119 [51:31<?, ?batches/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.483 | epoch: 1\n",
            "Test Loss: 0.921 | epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1929 [00:00<?, ?batches/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 1/1929 [00:00<11:34,  2.78batches/s]\u001b[A\n",
            "  0%|          | 2/1929 [00:00<11:36,  2.76batches/s]\u001b[A\n",
            "  0%|          | 3/1929 [00:01<11:35,  2.77batches/s]\u001b[A\n",
            "  0%|          | 4/1929 [00:01<11:37,  2.76batches/s]\u001b[A\n",
            "  0%|          | 5/1929 [00:01<11:34,  2.77batches/s]\u001b[A\n",
            "  0%|          | 6/1929 [00:02<11:36,  2.76batches/s]\u001b[A\n",
            "  0%|          | 7/1929 [00:02<11:34,  2.77batches/s]\u001b[A\n",
            "  0%|          | 8/1929 [00:02<11:35,  2.76batches/s]\u001b[A\n",
            "  0%|          | 9/1929 [00:03<11:33,  2.77batches/s]\u001b[A\n",
            "  1%|          | 10/1929 [00:03<11:32,  2.77batches/s]\u001b[A\n",
            "  1%|          | 11/1929 [00:03<11:31,  2.77batches/s]\u001b[A\n",
            "  1%|          | 12/1929 [00:04<11:30,  2.78batches/s]\u001b[A\n",
            "  1%|          | 13/1929 [00:04<11:30,  2.78batches/s]\u001b[A\n",
            "  1%|          | 14/1929 [00:05<11:29,  2.78batches/s]\u001b[A\n",
            "  1%|          | 15/1929 [00:05<11:29,  2.78batches/s]\u001b[A\n",
            "  1%|          | 16/1929 [00:05<11:28,  2.78batches/s]\u001b[A\n",
            "  1%|          | 17/1929 [00:06<11:28,  2.78batches/s]\u001b[A\n",
            "  1%|          | 18/1929 [00:06<11:28,  2.78batches/s]\u001b[A\n",
            "  1%|          | 19/1929 [00:06<11:31,  2.76batches/s]\u001b[A\n",
            "  1%|          | 20/1929 [00:07<11:30,  2.76batches/s]\u001b[A\n",
            "  1%|          | 21/1929 [00:07<11:29,  2.77batches/s]\u001b[A\n",
            "  1%|          | 22/1929 [00:07<11:27,  2.77batches/s]\u001b[A\n",
            "  1%|          | 23/1929 [00:08<11:26,  2.77batches/s]\u001b[A\n",
            "  1%|          | 24/1929 [00:08<11:26,  2.78batches/s]\u001b[A\n",
            "  1%|▏         | 25/1929 [00:09<11:26,  2.78batches/s]\u001b[A\n",
            "  1%|▏         | 26/1929 [00:09<11:25,  2.78batches/s]\u001b[A\n",
            "  1%|▏         | 27/1929 [00:09<11:24,  2.78batches/s]\u001b[A\n",
            "  1%|▏         | 28/1929 [00:10<11:23,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 29/1929 [00:10<11:25,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 30/1929 [00:10<11:25,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 31/1929 [00:11<11:25,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 32/1929 [00:11<11:24,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 33/1929 [00:11<11:23,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 34/1929 [00:12<11:22,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 35/1929 [00:12<11:22,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 36/1929 [00:12<11:22,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 37/1929 [00:13<11:21,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 38/1929 [00:13<11:21,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 39/1929 [00:14<11:20,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 40/1929 [00:14<11:20,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 41/1929 [00:14<11:19,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 42/1929 [00:15<11:18,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 43/1929 [00:15<11:20,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 44/1929 [00:15<11:19,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 45/1929 [00:16<11:19,  2.77batches/s]\u001b[A\n",
            "  2%|▏         | 46/1929 [00:16<11:17,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 47/1929 [00:16<11:16,  2.78batches/s]\u001b[A\n",
            "  2%|▏         | 48/1929 [00:17<11:15,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 49/1929 [00:17<11:15,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 50/1929 [00:18<11:15,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 51/1929 [00:18<11:15,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 52/1929 [00:18<11:14,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 53/1929 [00:19<11:14,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 54/1929 [00:19<11:16,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 55/1929 [00:19<11:15,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 56/1929 [00:20<11:15,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 57/1929 [00:20<11:14,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 58/1929 [00:20<11:12,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 59/1929 [00:21<11:13,  2.78batches/s]\u001b[A\n",
            "  3%|▎         | 60/1929 [00:21<11:13,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 61/1929 [00:21<11:14,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 62/1929 [00:22<11:13,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 63/1929 [00:22<11:14,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 64/1929 [00:23<11:13,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 65/1929 [00:23<11:12,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 66/1929 [00:23<11:12,  2.77batches/s]\u001b[A\n",
            "  3%|▎         | 67/1929 [00:24<11:12,  2.77batches/s]\u001b[A\n",
            "  4%|▎         | 68/1929 [00:24<11:11,  2.77batches/s]\u001b[A\n",
            "  4%|▎         | 69/1929 [00:24<11:12,  2.77batches/s]\u001b[A\n",
            "  4%|▎         | 70/1929 [00:25<11:10,  2.77batches/s]\u001b[A\n",
            "  4%|▎         | 71/1929 [00:25<11:09,  2.77batches/s]\u001b[A\n",
            "  4%|▎         | 72/1929 [00:25<11:09,  2.77batches/s]\u001b[A\n",
            "  4%|▍         | 73/1929 [00:26<11:08,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 74/1929 [00:26<11:07,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 75/1929 [00:27<11:07,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 76/1929 [00:27<11:06,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 77/1929 [00:27<11:05,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 78/1929 [00:28<11:06,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 79/1929 [00:28<11:07,  2.77batches/s]\u001b[A\n",
            "  4%|▍         | 80/1929 [00:28<11:05,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 81/1929 [00:29<11:05,  2.77batches/s]\u001b[A\n",
            "  4%|▍         | 82/1929 [00:29<11:04,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 83/1929 [00:29<11:03,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 84/1929 [00:30<11:03,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 85/1929 [00:30<11:03,  2.78batches/s]\u001b[A\n",
            "  4%|▍         | 86/1929 [00:30<11:03,  2.78batches/s]\u001b[A\n",
            "  5%|▍         | 87/1929 [00:31<11:01,  2.78batches/s]\u001b[A\n",
            "  5%|▍         | 88/1929 [00:31<11:01,  2.78batches/s]\u001b[A\n",
            "  5%|▍         | 89/1929 [00:32<11:02,  2.78batches/s]\u001b[A\n",
            "  5%|▍         | 90/1929 [00:32<11:03,  2.77batches/s]\u001b[A\n",
            "  5%|▍         | 91/1929 [00:32<11:02,  2.77batches/s]\u001b[A\n",
            "  5%|▍         | 92/1929 [00:33<11:02,  2.77batches/s]\u001b[A\n",
            "  5%|▍         | 93/1929 [00:33<11:01,  2.77batches/s]\u001b[A\n",
            "  5%|▍         | 94/1929 [00:33<11:01,  2.77batches/s]\u001b[A\n",
            "  5%|▍         | 95/1929 [00:34<11:01,  2.77batches/s]\u001b[A\n",
            "  5%|▍         | 96/1929 [00:34<11:00,  2.77batches/s]\u001b[A\n",
            "  5%|▌         | 97/1929 [00:34<11:01,  2.77batches/s]\u001b[A\n",
            "  5%|▌         | 98/1929 [00:35<11:01,  2.77batches/s]\u001b[A\n",
            "  5%|▌         | 99/1929 [00:35<11:00,  2.77batches/s]\u001b[A\n",
            "  5%|▌         | 100/1929 [00:36<11:00,  2.77batches/s]\u001b[A\n",
            "  5%|▌         | 101/1929 [00:36<10:58,  2.77batches/s]\u001b[A\n",
            "  5%|▌         | 102/1929 [00:36<10:58,  2.77batches/s]\u001b[A\n",
            "  5%|▌         | 103/1929 [00:37<10:57,  2.78batches/s]\u001b[A\n",
            "  5%|▌         | 104/1929 [00:37<10:57,  2.78batches/s]\u001b[A\n",
            "  5%|▌         | 105/1929 [00:37<10:58,  2.77batches/s]\u001b[A\n",
            "  5%|▌         | 106/1929 [00:38<10:57,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 107/1929 [00:38<10:55,  2.78batches/s]\u001b[A\n",
            "  6%|▌         | 108/1929 [00:38<10:55,  2.78batches/s]\u001b[A\n",
            "  6%|▌         | 109/1929 [00:39<10:56,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 110/1929 [00:39<10:57,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 111/1929 [00:40<10:56,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 112/1929 [00:40<10:57,  2.76batches/s]\u001b[A\n",
            "  6%|▌         | 113/1929 [00:40<10:56,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 114/1929 [00:41<10:55,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 115/1929 [00:41<10:55,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 116/1929 [00:41<10:53,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 117/1929 [00:42<10:52,  2.78batches/s]\u001b[A\n",
            "  6%|▌         | 118/1929 [00:42<10:53,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 119/1929 [00:42<10:52,  2.77batches/s]\u001b[A\n",
            "  6%|▌         | 120/1929 [00:43<10:52,  2.77batches/s]\u001b[A\n",
            "  6%|▋         | 121/1929 [00:43<10:51,  2.78batches/s]\u001b[A\n",
            "  6%|▋         | 122/1929 [00:43<10:50,  2.78batches/s]\u001b[A\n",
            "  6%|▋         | 123/1929 [00:44<10:50,  2.78batches/s]\u001b[A\n",
            "  6%|▋         | 124/1929 [00:44<10:50,  2.77batches/s]\u001b[A\n",
            "  6%|▋         | 125/1929 [00:45<10:49,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 126/1929 [00:45<10:49,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 127/1929 [00:45<10:48,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 128/1929 [00:46<10:48,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 129/1929 [00:46<10:48,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 130/1929 [00:46<10:49,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 131/1929 [00:47<10:49,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 132/1929 [00:47<10:47,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 133/1929 [00:47<10:48,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 134/1929 [00:48<10:48,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 135/1929 [00:48<10:48,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 136/1929 [00:49<10:47,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 137/1929 [00:49<10:46,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 138/1929 [00:49<10:46,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 139/1929 [00:50<10:46,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 140/1929 [00:50<10:46,  2.77batches/s]\u001b[A\n",
            "  7%|▋         | 141/1929 [00:50<10:44,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 142/1929 [00:51<10:43,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 143/1929 [00:51<10:43,  2.78batches/s]\u001b[A\n",
            "  7%|▋         | 144/1929 [00:51<10:41,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 145/1929 [00:52<10:41,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 146/1929 [00:52<10:41,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 147/1929 [00:52<10:40,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 148/1929 [00:53<10:40,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 149/1929 [00:53<10:40,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 150/1929 [00:54<10:44,  2.76batches/s]\u001b[A\n",
            "  8%|▊         | 151/1929 [00:54<10:42,  2.77batches/s]\u001b[A\n",
            "  8%|▊         | 152/1929 [00:54<10:41,  2.77batches/s]\u001b[A\n",
            "  8%|▊         | 153/1929 [00:55<10:41,  2.77batches/s]\u001b[A\n",
            "  8%|▊         | 154/1929 [00:55<10:41,  2.77batches/s]\u001b[A\n",
            "  8%|▊         | 155/1929 [00:55<10:39,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 156/1929 [00:56<10:38,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 157/1929 [00:56<10:37,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 158/1929 [00:56<10:38,  2.78batches/s]\u001b[A\n",
            "  8%|▊         | 159/1929 [00:57<10:38,  2.77batches/s]\u001b[A\n",
            "  8%|▊         | 160/1929 [00:57<10:39,  2.76batches/s]\u001b[A\n",
            "  8%|▊         | 161/1929 [00:58<10:39,  2.76batches/s]\u001b[A\n",
            "  8%|▊         | 162/1929 [00:58<10:38,  2.77batches/s]\u001b[A\n",
            "  8%|▊         | 163/1929 [00:58<10:37,  2.77batches/s]\u001b[A\n",
            "  9%|▊         | 164/1929 [00:59<10:39,  2.76batches/s]\u001b[A\n",
            "  9%|▊         | 165/1929 [00:59<10:39,  2.76batches/s]\u001b[A\n",
            "  9%|▊         | 166/1929 [00:59<10:37,  2.77batches/s]\u001b[A\n",
            "  9%|▊         | 167/1929 [01:00<10:36,  2.77batches/s]\u001b[A\n",
            "  9%|▊         | 168/1929 [01:00<10:35,  2.77batches/s]\u001b[A\n",
            "  9%|▉         | 169/1929 [01:00<10:34,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 170/1929 [01:01<10:33,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 171/1929 [01:01<10:32,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 172/1929 [01:02<10:31,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 173/1929 [01:02<10:30,  2.79batches/s]\u001b[A\n",
            "  9%|▉         | 174/1929 [01:02<10:29,  2.79batches/s]\u001b[A\n",
            "  9%|▉         | 175/1929 [01:03<10:29,  2.79batches/s]\u001b[A\n",
            "  9%|▉         | 176/1929 [01:03<10:29,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 177/1929 [01:03<10:29,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 178/1929 [01:04<10:29,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 179/1929 [01:04<10:28,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 180/1929 [01:04<10:28,  2.78batches/s]\u001b[A\n",
            "  9%|▉         | 181/1929 [01:05<10:32,  2.76batches/s]\u001b[A\n",
            "  9%|▉         | 182/1929 [01:05<10:32,  2.76batches/s]\u001b[A\n",
            "  9%|▉         | 183/1929 [01:05<10:32,  2.76batches/s]\u001b[A\n",
            " 10%|▉         | 184/1929 [01:06<10:32,  2.76batches/s]\u001b[A\n",
            " 10%|▉         | 185/1929 [01:06<10:31,  2.76batches/s]\u001b[A\n",
            " 10%|▉         | 186/1929 [01:07<10:29,  2.77batches/s]\u001b[A\n",
            " 10%|▉         | 187/1929 [01:07<10:28,  2.77batches/s]\u001b[A\n",
            " 10%|▉         | 188/1929 [01:07<10:27,  2.77batches/s]\u001b[A\n",
            " 10%|▉         | 189/1929 [01:08<10:26,  2.78batches/s]\u001b[A\n",
            " 10%|▉         | 190/1929 [01:08<10:27,  2.77batches/s]\u001b[A\n",
            " 10%|▉         | 191/1929 [01:08<10:27,  2.77batches/s]\u001b[A\n",
            " 10%|▉         | 192/1929 [01:09<10:26,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 193/1929 [01:09<10:25,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 194/1929 [01:09<10:25,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 195/1929 [01:10<10:25,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 196/1929 [01:10<10:25,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 197/1929 [01:11<10:25,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 198/1929 [01:11<10:24,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 199/1929 [01:11<10:25,  2.76batches/s]\u001b[A\n",
            " 10%|█         | 200/1929 [01:12<10:25,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 201/1929 [01:12<10:23,  2.77batches/s]\u001b[A\n",
            " 10%|█         | 202/1929 [01:12<10:22,  2.78batches/s]\u001b[A\n",
            " 11%|█         | 203/1929 [01:13<10:21,  2.78batches/s]\u001b[A\n",
            " 11%|█         | 204/1929 [01:13<10:21,  2.78batches/s]\u001b[A\n",
            " 11%|█         | 205/1929 [01:13<10:21,  2.78batches/s]\u001b[A\n",
            " 11%|█         | 206/1929 [01:14<10:20,  2.78batches/s]\u001b[A\n",
            " 11%|█         | 207/1929 [01:14<10:20,  2.77batches/s]\u001b[A\n",
            " 11%|█         | 208/1929 [01:14<10:20,  2.77batches/s]\u001b[A\n",
            " 11%|█         | 209/1929 [01:15<10:20,  2.77batches/s]\u001b[A\n",
            " 11%|█         | 210/1929 [01:15<10:19,  2.77batches/s]\u001b[A\n",
            " 11%|█         | 211/1929 [01:16<10:19,  2.77batches/s]\u001b[A\n",
            " 11%|█         | 212/1929 [01:16<10:19,  2.77batches/s]\u001b[A\n",
            " 11%|█         | 213/1929 [01:16<10:18,  2.77batches/s]\u001b[A\n",
            " 11%|█         | 214/1929 [01:17<10:17,  2.78batches/s]\u001b[A\n",
            " 11%|█         | 215/1929 [01:17<10:17,  2.77batches/s]\u001b[A\n",
            " 11%|█         | 216/1929 [01:17<10:16,  2.78batches/s]\u001b[A\n",
            " 11%|█         | 217/1929 [01:18<10:16,  2.78batches/s]\u001b[A\n",
            " 11%|█▏        | 218/1929 [01:18<10:16,  2.77batches/s]\u001b[A\n",
            " 11%|█▏        | 219/1929 [01:18<10:16,  2.77batches/s]\u001b[A\n",
            " 11%|█▏        | 220/1929 [01:19<10:17,  2.77batches/s]\u001b[A\n",
            " 11%|█▏        | 221/1929 [01:19<10:17,  2.76batches/s]\u001b[A\n",
            " 12%|█▏        | 222/1929 [01:20<10:17,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 223/1929 [01:20<10:14,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 224/1929 [01:20<10:14,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 225/1929 [01:21<10:14,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 226/1929 [01:21<10:14,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 227/1929 [01:21<10:15,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 228/1929 [01:22<10:15,  2.76batches/s]\u001b[A\n",
            " 12%|█▏        | 229/1929 [01:22<10:14,  2.76batches/s]\u001b[A\n",
            " 12%|█▏        | 230/1929 [01:22<10:14,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 231/1929 [01:23<10:13,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 232/1929 [01:23<10:12,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 233/1929 [01:24<10:12,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 234/1929 [01:24<10:11,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 235/1929 [01:24<10:11,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 236/1929 [01:25<10:10,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 237/1929 [01:25<10:09,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 238/1929 [01:25<10:08,  2.78batches/s]\u001b[A\n",
            " 12%|█▏        | 239/1929 [01:26<10:07,  2.78batches/s]\u001b[A\n",
            " 12%|█▏        | 240/1929 [01:26<10:09,  2.77batches/s]\u001b[A\n",
            " 12%|█▏        | 241/1929 [01:26<10:09,  2.77batches/s]\u001b[A\n",
            " 13%|█▎        | 242/1929 [01:27<10:07,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 243/1929 [01:27<10:10,  2.76batches/s]\u001b[A\n",
            " 13%|█▎        | 244/1929 [01:27<10:08,  2.77batches/s]\u001b[A\n",
            " 13%|█▎        | 245/1929 [01:28<10:08,  2.77batches/s]\u001b[A\n",
            " 13%|█▎        | 246/1929 [01:28<10:06,  2.77batches/s]\u001b[A\n",
            " 13%|█▎        | 247/1929 [01:29<10:05,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 248/1929 [01:29<10:05,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 249/1929 [01:29<10:04,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 250/1929 [01:30<10:04,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 251/1929 [01:30<10:03,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 252/1929 [01:30<10:04,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 253/1929 [01:31<10:04,  2.77batches/s]\u001b[A\n",
            " 13%|█▎        | 254/1929 [01:31<10:04,  2.77batches/s]\u001b[A\n",
            " 13%|█▎        | 255/1929 [01:31<10:03,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 256/1929 [01:32<10:02,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 257/1929 [01:32<10:02,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 258/1929 [01:33<10:01,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 259/1929 [01:33<10:01,  2.78batches/s]\u001b[A\n",
            " 13%|█▎        | 260/1929 [01:33<10:02,  2.77batches/s]\u001b[A\n",
            " 14%|█▎        | 261/1929 [01:34<10:02,  2.77batches/s]\u001b[A\n",
            " 14%|█▎        | 262/1929 [01:34<10:00,  2.78batches/s]\u001b[A\n",
            " 14%|█▎        | 263/1929 [01:34<09:59,  2.78batches/s]\u001b[A\n",
            " 14%|█▎        | 264/1929 [01:35<09:59,  2.78batches/s]\u001b[A\n",
            " 14%|█▎        | 265/1929 [01:35<09:59,  2.77batches/s]\u001b[A\n",
            " 14%|█▍        | 266/1929 [01:35<09:59,  2.77batches/s]\u001b[A\n",
            " 14%|█▍        | 267/1929 [01:36<09:58,  2.78batches/s]\u001b[A\n",
            " 14%|█▍        | 268/1929 [01:36<09:57,  2.78batches/s]\u001b[A\n",
            " 14%|█▍        | 269/1929 [01:36<09:56,  2.78batches/s]\u001b[A\n",
            " 14%|█▍        | 270/1929 [01:37<09:57,  2.78batches/s]\u001b[A\n",
            " 14%|█▍        | 271/1929 [01:37<09:55,  2.78batches/s]\u001b[A\n",
            " 14%|█▍        | 272/1929 [01:38<09:55,  2.78batches/s]\u001b[A\n",
            " 14%|█▍        | 273/1929 [01:38<09:55,  2.78batches/s]\u001b[A\n",
            " 14%|█▍        | 274/1929 [01:38<09:57,  2.77batches/s]\u001b[A\n",
            " 14%|█▍        | 275/1929 [01:39<09:57,  2.77batches/s]\u001b[A\n",
            " 14%|█▍        | 276/1929 [01:39<09:57,  2.77batches/s]\u001b[A\n",
            " 14%|█▍        | 277/1929 [01:39<09:56,  2.77batches/s]\u001b[A\n",
            " 14%|█▍        | 278/1929 [01:40<09:55,  2.77batches/s]\u001b[A\n",
            " 14%|█▍        | 279/1929 [01:40<09:54,  2.78batches/s]\u001b[A\n",
            " 15%|█▍        | 280/1929 [01:40<09:54,  2.77batches/s]\u001b[A\n",
            " 15%|█▍        | 281/1929 [01:41<09:54,  2.77batches/s]\u001b[A\n",
            " 15%|█▍        | 282/1929 [01:41<09:53,  2.78batches/s]\u001b[A\n",
            " 15%|█▍        | 283/1929 [01:42<09:52,  2.78batches/s]\u001b[A\n",
            " 15%|█▍        | 284/1929 [01:42<09:51,  2.78batches/s]\u001b[A\n",
            " 15%|█▍        | 285/1929 [01:42<09:52,  2.77batches/s]\u001b[A\n",
            " 15%|█▍        | 286/1929 [01:43<09:52,  2.77batches/s]\u001b[A\n",
            " 15%|█▍        | 287/1929 [01:43<09:52,  2.77batches/s]\u001b[A\n",
            " 15%|█▍        | 288/1929 [01:43<09:51,  2.77batches/s]\u001b[A\n",
            " 15%|█▍        | 289/1929 [01:44<09:51,  2.77batches/s]\u001b[A\n",
            " 15%|█▌        | 290/1929 [01:44<09:52,  2.77batches/s]\u001b[A\n",
            " 15%|█▌        | 291/1929 [01:44<09:54,  2.76batches/s]\u001b[A\n",
            " 15%|█▌        | 292/1929 [01:45<09:53,  2.76batches/s]\u001b[A\n",
            " 15%|█▌        | 293/1929 [01:45<09:51,  2.76batches/s]\u001b[A\n",
            " 15%|█▌        | 294/1929 [01:46<09:50,  2.77batches/s]\u001b[A\n",
            " 15%|█▌        | 295/1929 [01:46<09:50,  2.77batches/s]\u001b[A\n",
            " 15%|█▌        | 296/1929 [01:46<09:49,  2.77batches/s]\u001b[A\n",
            " 15%|█▌        | 297/1929 [01:47<09:49,  2.77batches/s]\u001b[A\n",
            " 15%|█▌        | 298/1929 [01:47<09:49,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 299/1929 [01:47<09:48,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 300/1929 [01:48<09:48,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 301/1929 [01:48<09:48,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 302/1929 [01:48<09:46,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 303/1929 [01:49<09:46,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 304/1929 [01:49<09:45,  2.78batches/s]\u001b[A\n",
            " 16%|█▌        | 305/1929 [01:49<09:44,  2.78batches/s]\u001b[A\n",
            " 16%|█▌        | 306/1929 [01:50<09:44,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 307/1929 [01:50<09:43,  2.78batches/s]\u001b[A\n",
            " 16%|█▌        | 308/1929 [01:51<09:43,  2.78batches/s]\u001b[A\n",
            " 16%|█▌        | 309/1929 [01:51<09:43,  2.78batches/s]\u001b[A\n",
            " 16%|█▌        | 310/1929 [01:51<09:43,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 311/1929 [01:52<09:42,  2.78batches/s]\u001b[A\n",
            " 16%|█▌        | 312/1929 [01:52<09:42,  2.77batches/s]\u001b[A\n",
            " 16%|█▌        | 313/1929 [01:52<09:43,  2.77batches/s]\u001b[A\n",
            " 16%|█▋        | 314/1929 [01:53<09:43,  2.77batches/s]\u001b[A\n",
            " 16%|█▋        | 315/1929 [01:53<09:43,  2.77batches/s]\u001b[A\n",
            " 16%|█▋        | 316/1929 [01:53<09:41,  2.77batches/s]\u001b[A\n",
            " 16%|█▋        | 317/1929 [01:54<09:41,  2.77batches/s]\u001b[A\n",
            " 16%|█▋        | 318/1929 [01:54<09:40,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 319/1929 [01:55<09:40,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 320/1929 [01:55<09:41,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 321/1929 [01:55<09:39,  2.78batches/s]\u001b[A\n",
            " 17%|█▋        | 322/1929 [01:56<09:41,  2.76batches/s]\u001b[A\n",
            " 17%|█▋        | 323/1929 [01:56<09:40,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 324/1929 [01:56<09:40,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 325/1929 [01:57<09:40,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 326/1929 [01:57<09:39,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 327/1929 [01:57<09:38,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 328/1929 [01:58<09:38,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 329/1929 [01:58<09:37,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 330/1929 [01:58<09:36,  2.78batches/s]\u001b[A\n",
            " 17%|█▋        | 331/1929 [01:59<09:35,  2.78batches/s]\u001b[A\n",
            " 17%|█▋        | 332/1929 [01:59<09:35,  2.78batches/s]\u001b[A\n",
            " 17%|█▋        | 333/1929 [02:00<09:34,  2.78batches/s]\u001b[A\n",
            " 17%|█▋        | 334/1929 [02:00<09:34,  2.78batches/s]\u001b[A\n",
            " 17%|█▋        | 335/1929 [02:00<09:34,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 336/1929 [02:01<09:34,  2.77batches/s]\u001b[A\n",
            " 17%|█▋        | 337/1929 [02:01<09:35,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 338/1929 [02:01<09:33,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 339/1929 [02:02<09:32,  2.78batches/s]\u001b[A\n",
            " 18%|█▊        | 340/1929 [02:02<09:32,  2.78batches/s]\u001b[A\n",
            " 18%|█▊        | 341/1929 [02:02<09:32,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 342/1929 [02:03<09:32,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 343/1929 [02:03<09:31,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 344/1929 [02:04<09:32,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 345/1929 [02:04<09:33,  2.76batches/s]\u001b[A\n",
            " 18%|█▊        | 346/1929 [02:04<09:32,  2.76batches/s]\u001b[A\n",
            " 18%|█▊        | 347/1929 [02:05<09:31,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 348/1929 [02:05<09:30,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 349/1929 [02:05<09:31,  2.76batches/s]\u001b[A\n",
            " 18%|█▊        | 350/1929 [02:06<09:30,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 351/1929 [02:06<09:28,  2.78batches/s]\u001b[A\n",
            " 18%|█▊        | 352/1929 [02:06<09:27,  2.78batches/s]\u001b[A\n",
            " 18%|█▊        | 353/1929 [02:07<09:29,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 354/1929 [02:07<09:27,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 355/1929 [02:08<09:27,  2.77batches/s]\u001b[A\n",
            " 18%|█▊        | 356/1929 [02:08<09:26,  2.78batches/s]\u001b[A\n",
            " 19%|█▊        | 357/1929 [02:08<09:26,  2.77batches/s]\u001b[A\n",
            " 19%|█▊        | 358/1929 [02:09<09:26,  2.77batches/s]\u001b[A\n",
            " 19%|█▊        | 359/1929 [02:09<09:26,  2.77batches/s]\u001b[A\n",
            " 19%|█▊        | 360/1929 [02:09<09:27,  2.77batches/s]\u001b[A\n",
            " 19%|█▊        | 361/1929 [02:10<09:26,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 362/1929 [02:10<09:25,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 363/1929 [02:10<09:25,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 364/1929 [02:11<09:24,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 365/1929 [02:11<09:25,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 366/1929 [02:11<09:23,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 367/1929 [02:12<09:22,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 368/1929 [02:12<09:23,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 369/1929 [02:13<09:23,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 370/1929 [02:13<09:23,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 371/1929 [02:13<09:23,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 372/1929 [02:14<09:23,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 373/1929 [02:14<09:21,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 374/1929 [02:14<09:21,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 375/1929 [02:15<09:20,  2.77batches/s]\u001b[A\n",
            " 19%|█▉        | 376/1929 [02:15<09:18,  2.78batches/s]\u001b[A\n",
            " 20%|█▉        | 377/1929 [02:15<09:18,  2.78batches/s]\u001b[A\n",
            " 20%|█▉        | 378/1929 [02:16<09:18,  2.78batches/s]\u001b[A\n",
            " 20%|█▉        | 379/1929 [02:16<09:17,  2.78batches/s]\u001b[A\n",
            " 20%|█▉        | 380/1929 [02:17<09:17,  2.78batches/s]\u001b[A\n",
            " 20%|█▉        | 381/1929 [02:17<09:17,  2.78batches/s]\u001b[A\n",
            " 20%|█▉        | 382/1929 [02:17<09:17,  2.78batches/s]\u001b[A\n",
            " 20%|█▉        | 383/1929 [02:18<09:17,  2.77batches/s]\u001b[A\n",
            " 20%|█▉        | 384/1929 [02:18<09:18,  2.77batches/s]\u001b[A\n",
            " 20%|█▉        | 385/1929 [02:18<09:17,  2.77batches/s]\u001b[A\n",
            " 20%|██        | 386/1929 [02:19<09:17,  2.77batches/s]\u001b[A\n",
            " 20%|██        | 387/1929 [02:19<09:16,  2.77batches/s]\u001b[A\n",
            " 20%|██        | 388/1929 [02:19<09:14,  2.78batches/s]\u001b[A\n",
            " 20%|██        | 389/1929 [02:20<09:13,  2.78batches/s]\u001b[A\n",
            " 20%|██        | 390/1929 [02:20<09:14,  2.78batches/s]\u001b[A\n",
            " 20%|██        | 391/1929 [02:20<09:13,  2.78batches/s]\u001b[A\n",
            " 20%|██        | 392/1929 [02:21<09:13,  2.78batches/s]\u001b[A\n",
            " 20%|██        | 393/1929 [02:21<09:12,  2.78batches/s]\u001b[A\n",
            " 20%|██        | 394/1929 [02:22<09:12,  2.78batches/s]\u001b[A\n",
            " 20%|██        | 395/1929 [02:22<09:12,  2.78batches/s]\u001b[A\n",
            " 21%|██        | 396/1929 [02:22<09:12,  2.78batches/s]\u001b[A\n",
            " 21%|██        | 397/1929 [02:23<09:12,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 398/1929 [02:23<09:11,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 399/1929 [02:23<09:11,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 400/1929 [02:24<09:11,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 401/1929 [02:24<09:12,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 402/1929 [02:24<09:12,  2.76batches/s]\u001b[A\n",
            " 21%|██        | 403/1929 [02:25<09:10,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 404/1929 [02:25<09:10,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 405/1929 [02:26<09:09,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 406/1929 [02:26<09:09,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 407/1929 [02:26<09:08,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 408/1929 [02:27<09:08,  2.77batches/s]\u001b[A\n",
            " 21%|██        | 409/1929 [02:27<09:12,  2.75batches/s]\u001b[A\n",
            " 21%|██▏       | 410/1929 [02:27<09:11,  2.76batches/s]\u001b[A\n",
            " 21%|██▏       | 411/1929 [02:28<09:09,  2.76batches/s]\u001b[A\n",
            " 21%|██▏       | 412/1929 [02:28<09:07,  2.77batches/s]\u001b[A\n",
            " 21%|██▏       | 413/1929 [02:28<09:08,  2.77batches/s]\u001b[A\n",
            " 21%|██▏       | 414/1929 [02:29<09:08,  2.76batches/s]\u001b[A\n",
            " 22%|██▏       | 415/1929 [02:29<09:08,  2.76batches/s]\u001b[A\n",
            " 22%|██▏       | 416/1929 [02:30<09:06,  2.77batches/s]\u001b[A\n",
            " 22%|██▏       | 417/1929 [02:30<09:04,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 418/1929 [02:30<09:04,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 419/1929 [02:31<09:03,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 420/1929 [02:31<09:05,  2.76batches/s]\u001b[A\n",
            " 22%|██▏       | 421/1929 [02:31<09:05,  2.77batches/s]\u001b[A\n",
            " 22%|██▏       | 422/1929 [02:32<09:03,  2.77batches/s]\u001b[A\n",
            " 22%|██▏       | 423/1929 [02:32<09:02,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 424/1929 [02:32<09:03,  2.77batches/s]\u001b[A\n",
            " 22%|██▏       | 425/1929 [02:33<09:02,  2.77batches/s]\u001b[A\n",
            " 22%|██▏       | 426/1929 [02:33<09:00,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 427/1929 [02:33<09:00,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 428/1929 [02:34<09:00,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 429/1929 [02:34<08:59,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 430/1929 [02:35<08:59,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 431/1929 [02:35<08:59,  2.78batches/s]\u001b[A\n",
            " 22%|██▏       | 432/1929 [02:35<09:00,  2.77batches/s]\u001b[A\n",
            " 22%|██▏       | 433/1929 [02:36<08:59,  2.77batches/s]\u001b[A\n",
            " 22%|██▏       | 434/1929 [02:36<08:59,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 435/1929 [02:36<08:59,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 436/1929 [02:37<08:58,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 437/1929 [02:37<08:58,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 438/1929 [02:37<08:58,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 439/1929 [02:38<08:58,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 440/1929 [02:38<08:57,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 441/1929 [02:39<08:55,  2.78batches/s]\u001b[A\n",
            " 23%|██▎       | 442/1929 [02:39<08:54,  2.78batches/s]\u001b[A\n",
            " 23%|██▎       | 443/1929 [02:39<08:54,  2.78batches/s]\u001b[A\n",
            " 23%|██▎       | 444/1929 [02:40<08:53,  2.78batches/s]\u001b[A\n",
            " 23%|██▎       | 445/1929 [02:40<08:55,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 446/1929 [02:40<08:54,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 447/1929 [02:41<08:56,  2.76batches/s]\u001b[A\n",
            " 23%|██▎       | 448/1929 [02:41<08:55,  2.76batches/s]\u001b[A\n",
            " 23%|██▎       | 449/1929 [02:41<08:55,  2.76batches/s]\u001b[A\n",
            " 23%|██▎       | 450/1929 [02:42<08:54,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 451/1929 [02:42<08:53,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 452/1929 [02:43<08:52,  2.77batches/s]\u001b[A\n",
            " 23%|██▎       | 453/1929 [02:43<08:51,  2.78batches/s]\u001b[A\n",
            " 24%|██▎       | 454/1929 [02:43<08:50,  2.78batches/s]\u001b[A\n",
            " 24%|██▎       | 455/1929 [02:44<08:51,  2.77batches/s]\u001b[A\n",
            " 24%|██▎       | 456/1929 [02:44<08:51,  2.77batches/s]\u001b[A\n",
            " 24%|██▎       | 457/1929 [02:44<08:52,  2.77batches/s]\u001b[A\n",
            " 24%|██▎       | 458/1929 [02:45<08:51,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 459/1929 [02:45<08:51,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 460/1929 [02:45<08:50,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 461/1929 [02:46<08:49,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 462/1929 [02:46<08:48,  2.78batches/s]\u001b[A\n",
            " 24%|██▍       | 463/1929 [02:46<08:49,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 464/1929 [02:47<08:48,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 465/1929 [02:47<08:48,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 466/1929 [02:48<08:47,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 467/1929 [02:48<08:47,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 468/1929 [02:48<08:47,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 469/1929 [02:49<08:46,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 470/1929 [02:49<08:47,  2.77batches/s]\u001b[A\n",
            " 24%|██▍       | 471/1929 [02:49<08:48,  2.76batches/s]\u001b[A\n",
            " 24%|██▍       | 472/1929 [02:50<08:48,  2.76batches/s]\u001b[A\n",
            " 25%|██▍       | 473/1929 [02:50<08:47,  2.76batches/s]\u001b[A\n",
            " 25%|██▍       | 474/1929 [02:50<08:46,  2.76batches/s]\u001b[A\n",
            " 25%|██▍       | 475/1929 [02:51<08:46,  2.76batches/s]\u001b[A\n",
            " 25%|██▍       | 476/1929 [02:51<08:44,  2.77batches/s]\u001b[A\n",
            " 25%|██▍       | 477/1929 [02:52<08:45,  2.76batches/s]\u001b[A\n",
            " 25%|██▍       | 478/1929 [02:52<08:43,  2.77batches/s]\u001b[A\n",
            " 25%|██▍       | 479/1929 [02:52<08:43,  2.77batches/s]\u001b[A\n",
            " 25%|██▍       | 480/1929 [02:53<08:42,  2.77batches/s]\u001b[A\n",
            " 25%|██▍       | 481/1929 [02:53<08:42,  2.77batches/s]\u001b[A\n",
            " 25%|██▍       | 482/1929 [02:53<08:42,  2.77batches/s]\u001b[A\n",
            " 25%|██▌       | 483/1929 [02:54<08:42,  2.77batches/s]\u001b[A\n",
            " 25%|██▌       | 484/1929 [02:54<08:42,  2.77batches/s]\u001b[A\n",
            " 25%|██▌       | 485/1929 [02:54<08:42,  2.77batches/s]\u001b[A\n",
            " 25%|██▌       | 486/1929 [02:55<08:41,  2.77batches/s]\u001b[A\n",
            " 25%|██▌       | 487/1929 [02:55<08:40,  2.77batches/s]\u001b[A\n",
            " 25%|██▌       | 488/1929 [02:56<08:39,  2.78batches/s]\u001b[A\n",
            " 25%|██▌       | 489/1929 [02:56<08:37,  2.78batches/s]\u001b[A\n",
            " 25%|██▌       | 490/1929 [02:56<08:37,  2.78batches/s]\u001b[A\n",
            " 25%|██▌       | 491/1929 [02:57<08:38,  2.78batches/s]\u001b[A\n",
            " 26%|██▌       | 492/1929 [02:57<08:39,  2.77batches/s]\u001b[A\n",
            " 26%|██▌       | 493/1929 [02:57<08:38,  2.77batches/s]\u001b[A\n",
            " 26%|██▌       | 494/1929 [02:58<08:37,  2.77batches/s]\u001b[A\n",
            " 26%|██▌       | 495/1929 [02:58<08:37,  2.77batches/s]\u001b[A\n",
            " 26%|██▌       | 496/1929 [02:58<08:38,  2.77batches/s]\u001b[A\n",
            " 26%|██▌       | 497/1929 [02:59<08:37,  2.77batches/s]\u001b[A\n",
            " 26%|██▌       | 498/1929 [02:59<08:38,  2.76batches/s]\u001b[A\n",
            " 26%|██▌       | 499/1929 [02:59<08:37,  2.76batches/s]\u001b[A\n",
            " 26%|██▌       | 500/1929 [03:00<08:36,  2.77batches/s]\u001b[A\n",
            " 26%|██▌       | 501/1929 [03:00<08:34,  2.77batches/s]\u001b[A\n",
            " 26%|██▌       | 502/1929 [03:01<08:34,  2.78batches/s]\u001b[A\n",
            " 26%|██▌       | 503/1929 [03:01<08:33,  2.78batches/s]\u001b[A\n",
            " 26%|██▌       | 504/1929 [03:01<08:32,  2.78batches/s]\u001b[A\n",
            " 26%|██▌       | 505/1929 [03:02<08:32,  2.78batches/s]\u001b[A\n",
            " 26%|██▌       | 506/1929 [03:02<08:32,  2.78batches/s]\u001b[A\n",
            " 26%|██▋       | 507/1929 [03:02<08:33,  2.77batches/s]\u001b[A\n",
            " 26%|██▋       | 508/1929 [03:03<08:34,  2.76batches/s]\u001b[A\n",
            " 26%|██▋       | 509/1929 [03:03<08:33,  2.77batches/s]\u001b[A\n",
            " 26%|██▋       | 510/1929 [03:03<08:32,  2.77batches/s]\u001b[A\n",
            " 26%|██▋       | 511/1929 [03:04<08:32,  2.77batches/s]\u001b[A\n",
            " 27%|██▋       | 512/1929 [03:04<08:31,  2.77batches/s]\u001b[A\n",
            " 27%|██▋       | 513/1929 [03:05<08:32,  2.76batches/s]\u001b[A\n",
            " 27%|██▋       | 514/1929 [03:05<08:31,  2.76batches/s]\u001b[A\n",
            " 27%|██▋       | 515/1929 [03:05<08:31,  2.77batches/s]\u001b[A\n",
            " 27%|██▋       | 516/1929 [03:06<08:30,  2.77batches/s]\u001b[A\n",
            " 27%|██▋       | 517/1929 [03:06<08:30,  2.76batches/s]\u001b[A\n",
            " 27%|██▋       | 518/1929 [03:06<08:31,  2.76batches/s]\u001b[A\n",
            " 27%|██▋       | 519/1929 [03:07<08:31,  2.75batches/s]\u001b[A\n",
            " 27%|██▋       | 520/1929 [03:07<08:30,  2.76batches/s]\u001b[A\n",
            " 27%|██▋       | 521/1929 [03:07<08:28,  2.77batches/s]\u001b[A\n",
            " 27%|██▋       | 522/1929 [03:08<08:27,  2.77batches/s]\u001b[A\n",
            " 27%|██▋       | 523/1929 [03:08<08:26,  2.78batches/s]\u001b[A\n",
            " 27%|██▋       | 524/1929 [03:09<08:25,  2.78batches/s]\u001b[A\n",
            " 27%|██▋       | 525/1929 [03:09<08:25,  2.78batches/s]\u001b[A\n",
            " 27%|██▋       | 526/1929 [03:09<08:24,  2.78batches/s]\u001b[A\n",
            " 27%|██▋       | 527/1929 [03:10<08:25,  2.78batches/s]\u001b[A\n",
            " 27%|██▋       | 528/1929 [03:10<08:25,  2.77batches/s]\u001b[A\n",
            " 27%|██▋       | 529/1929 [03:10<08:24,  2.77batches/s]\u001b[A\n",
            " 27%|██▋       | 530/1929 [03:11<08:23,  2.78batches/s]\u001b[A\n",
            " 28%|██▊       | 531/1929 [03:11<08:23,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 532/1929 [03:11<08:23,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 533/1929 [03:12<08:23,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 534/1929 [03:12<08:24,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 535/1929 [03:12<08:23,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 536/1929 [03:13<08:22,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 537/1929 [03:13<08:22,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 538/1929 [03:14<08:21,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 539/1929 [03:14<08:23,  2.76batches/s]\u001b[A\n",
            " 28%|██▊       | 540/1929 [03:14<08:23,  2.76batches/s]\u001b[A\n",
            " 28%|██▊       | 541/1929 [03:15<08:22,  2.76batches/s]\u001b[A\n",
            " 28%|██▊       | 542/1929 [03:15<08:21,  2.76batches/s]\u001b[A\n",
            " 28%|██▊       | 543/1929 [03:15<08:21,  2.76batches/s]\u001b[A\n",
            " 28%|██▊       | 544/1929 [03:16<08:21,  2.76batches/s]\u001b[A\n",
            " 28%|██▊       | 545/1929 [03:16<08:20,  2.76batches/s]\u001b[A\n",
            " 28%|██▊       | 546/1929 [03:16<08:20,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 547/1929 [03:17<08:18,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 548/1929 [03:17<08:18,  2.77batches/s]\u001b[A\n",
            " 28%|██▊       | 549/1929 [03:18<08:17,  2.78batches/s]\u001b[A\n",
            " 29%|██▊       | 550/1929 [03:18<08:15,  2.78batches/s]\u001b[A\n",
            " 29%|██▊       | 551/1929 [03:18<08:14,  2.79batches/s]\u001b[A\n",
            " 29%|██▊       | 552/1929 [03:19<08:14,  2.78batches/s]\u001b[A\n",
            " 29%|██▊       | 553/1929 [03:19<08:15,  2.78batches/s]\u001b[A\n",
            " 29%|██▊       | 554/1929 [03:19<08:15,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 555/1929 [03:20<08:14,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 556/1929 [03:20<08:14,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 557/1929 [03:20<08:15,  2.77batches/s]\u001b[A\n",
            " 29%|██▉       | 558/1929 [03:21<08:14,  2.77batches/s]\u001b[A\n",
            " 29%|██▉       | 559/1929 [03:21<08:13,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 560/1929 [03:21<08:12,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 561/1929 [03:22<08:12,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 562/1929 [03:22<08:13,  2.77batches/s]\u001b[A\n",
            " 29%|██▉       | 563/1929 [03:23<08:12,  2.77batches/s]\u001b[A\n",
            " 29%|██▉       | 564/1929 [03:23<08:11,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 565/1929 [03:23<08:11,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 566/1929 [03:24<08:11,  2.78batches/s]\u001b[A\n",
            " 29%|██▉       | 567/1929 [03:24<08:11,  2.77batches/s]\u001b[A\n",
            " 29%|██▉       | 568/1929 [03:24<08:11,  2.77batches/s]\u001b[A\n",
            " 29%|██▉       | 569/1929 [03:25<08:10,  2.77batches/s]\u001b[A\n",
            " 30%|██▉       | 570/1929 [03:25<08:10,  2.77batches/s]\u001b[A\n",
            " 30%|██▉       | 571/1929 [03:25<08:10,  2.77batches/s]\u001b[A\n",
            " 30%|██▉       | 572/1929 [03:26<08:09,  2.77batches/s]\u001b[A\n",
            " 30%|██▉       | 573/1929 [03:26<08:08,  2.78batches/s]\u001b[A\n",
            " 30%|██▉       | 574/1929 [03:27<08:07,  2.78batches/s]\u001b[A\n",
            " 30%|██▉       | 575/1929 [03:27<08:07,  2.78batches/s]\u001b[A\n",
            " 30%|██▉       | 576/1929 [03:27<08:07,  2.77batches/s]\u001b[A\n",
            " 30%|██▉       | 577/1929 [03:28<08:07,  2.77batches/s]\u001b[A\n",
            " 30%|██▉       | 578/1929 [03:28<08:07,  2.77batches/s]\u001b[A\n",
            " 30%|███       | 579/1929 [03:28<08:07,  2.77batches/s]\u001b[A\n",
            " 30%|███       | 580/1929 [03:29<08:07,  2.77batches/s]\u001b[A\n",
            " 30%|███       | 581/1929 [03:29<08:06,  2.77batches/s]\u001b[A\n",
            " 30%|███       | 582/1929 [03:29<08:05,  2.77batches/s]\u001b[A\n",
            " 30%|███       | 583/1929 [03:30<08:04,  2.78batches/s]\u001b[A\n",
            " 30%|███       | 584/1929 [03:30<08:03,  2.78batches/s]\u001b[A\n",
            " 30%|███       | 585/1929 [03:31<08:03,  2.78batches/s]\u001b[A\n",
            " 30%|███       | 586/1929 [03:31<08:04,  2.77batches/s]\u001b[A\n",
            " 30%|███       | 587/1929 [03:31<08:04,  2.77batches/s]\u001b[A\n",
            " 30%|███       | 588/1929 [03:32<08:05,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 589/1929 [03:32<08:04,  2.77batches/s]\u001b[A\n",
            " 31%|███       | 590/1929 [03:32<08:03,  2.77batches/s]\u001b[A\n",
            " 31%|███       | 591/1929 [03:33<08:04,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 592/1929 [03:33<08:03,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 593/1929 [03:33<08:03,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 594/1929 [03:34<08:03,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 595/1929 [03:34<08:03,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 596/1929 [03:34<08:03,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 597/1929 [03:35<08:02,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 598/1929 [03:35<08:00,  2.77batches/s]\u001b[A\n",
            " 31%|███       | 599/1929 [03:36<07:59,  2.77batches/s]\u001b[A\n",
            " 31%|███       | 600/1929 [03:36<07:59,  2.77batches/s]\u001b[A\n",
            " 31%|███       | 601/1929 [03:36<08:02,  2.76batches/s]\u001b[A\n",
            " 31%|███       | 602/1929 [03:37<08:00,  2.76batches/s]\u001b[A\n",
            " 31%|███▏      | 603/1929 [03:37<07:58,  2.77batches/s]\u001b[A\n",
            " 31%|███▏      | 604/1929 [03:37<07:58,  2.77batches/s]\u001b[A\n",
            " 31%|███▏      | 605/1929 [03:38<07:57,  2.77batches/s]\u001b[A\n",
            " 31%|███▏      | 606/1929 [03:38<07:57,  2.77batches/s]\u001b[A\n",
            " 31%|███▏      | 607/1929 [03:38<07:57,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 608/1929 [03:39<07:57,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 609/1929 [03:39<07:56,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 610/1929 [03:40<07:55,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 611/1929 [03:40<07:54,  2.78batches/s]\u001b[A\n",
            " 32%|███▏      | 612/1929 [03:40<07:55,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 613/1929 [03:41<07:55,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 614/1929 [03:41<07:55,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 615/1929 [03:41<07:56,  2.76batches/s]\u001b[A\n",
            " 32%|███▏      | 616/1929 [03:42<07:56,  2.76batches/s]\u001b[A\n",
            " 32%|███▏      | 617/1929 [03:42<07:55,  2.76batches/s]\u001b[A\n",
            " 32%|███▏      | 618/1929 [03:42<07:54,  2.76batches/s]\u001b[A\n",
            " 32%|███▏      | 619/1929 [03:43<07:52,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 620/1929 [03:43<07:51,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 621/1929 [03:44<07:52,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 622/1929 [03:44<07:51,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 623/1929 [03:44<07:51,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 624/1929 [03:45<07:50,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 625/1929 [03:45<07:50,  2.77batches/s]\u001b[A\n",
            " 32%|███▏      | 626/1929 [03:45<07:49,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 627/1929 [03:46<07:50,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 628/1929 [03:46<07:49,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 629/1929 [03:46<07:48,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 630/1929 [03:47<07:48,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 631/1929 [03:47<07:47,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 632/1929 [03:47<07:49,  2.76batches/s]\u001b[A\n",
            " 33%|███▎      | 633/1929 [03:48<07:47,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 634/1929 [03:48<07:47,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 635/1929 [03:49<07:46,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 636/1929 [03:49<07:46,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 637/1929 [03:49<07:47,  2.76batches/s]\u001b[A\n",
            " 33%|███▎      | 638/1929 [03:50<07:47,  2.76batches/s]\u001b[A\n",
            " 33%|███▎      | 639/1929 [03:50<07:47,  2.76batches/s]\u001b[A\n",
            " 33%|███▎      | 640/1929 [03:50<07:48,  2.75batches/s]\u001b[A\n",
            " 33%|███▎      | 641/1929 [03:51<07:47,  2.75batches/s]\u001b[A\n",
            " 33%|███▎      | 642/1929 [03:51<07:46,  2.76batches/s]\u001b[A\n",
            " 33%|███▎      | 643/1929 [03:51<07:45,  2.76batches/s]\u001b[A\n",
            " 33%|███▎      | 644/1929 [03:52<07:43,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 645/1929 [03:52<07:43,  2.77batches/s]\u001b[A\n",
            " 33%|███▎      | 646/1929 [03:53<07:42,  2.77batches/s]\u001b[A\n",
            " 34%|███▎      | 647/1929 [03:53<07:43,  2.77batches/s]\u001b[A\n",
            " 34%|███▎      | 648/1929 [03:53<07:43,  2.77batches/s]\u001b[A\n",
            " 34%|███▎      | 649/1929 [03:54<07:42,  2.77batches/s]\u001b[A\n",
            " 34%|███▎      | 650/1929 [03:54<07:42,  2.77batches/s]\u001b[A\n",
            " 34%|███▎      | 651/1929 [03:54<07:42,  2.76batches/s]\u001b[A\n",
            " 34%|███▍      | 652/1929 [03:55<07:40,  2.77batches/s]\u001b[A\n",
            " 34%|███▍      | 653/1929 [03:55<07:39,  2.78batches/s]\u001b[A\n",
            " 34%|███▍      | 654/1929 [03:55<07:39,  2.77batches/s]\u001b[A\n",
            " 34%|███▍      | 655/1929 [03:56<07:40,  2.77batches/s]\u001b[A\n",
            " 34%|███▍      | 656/1929 [03:56<07:40,  2.77batches/s]\u001b[A\n",
            " 34%|███▍      | 657/1929 [03:57<07:39,  2.77batches/s]\u001b[A\n",
            " 34%|███▍      | 658/1929 [03:57<07:39,  2.76batches/s]\u001b[A\n",
            " 34%|███▍      | 659/1929 [03:57<07:40,  2.76batches/s]\u001b[A\n",
            " 34%|███▍      | 660/1929 [03:58<07:39,  2.76batches/s]\u001b[A\n",
            " 34%|███▍      | 661/1929 [03:58<07:38,  2.76batches/s]\u001b[A\n",
            " 34%|███▍      | 662/1929 [03:58<07:39,  2.76batches/s]\u001b[A\n",
            " 34%|███▍      | 663/1929 [03:59<07:40,  2.75batches/s]\u001b[A\n",
            " 34%|███▍      | 664/1929 [03:59<07:41,  2.74batches/s]\u001b[A\n",
            " 34%|███▍      | 665/1929 [03:59<07:40,  2.75batches/s]\u001b[A\n",
            " 35%|███▍      | 666/1929 [04:00<07:38,  2.76batches/s]\u001b[A\n",
            " 35%|███▍      | 667/1929 [04:00<07:36,  2.76batches/s]\u001b[A\n",
            " 35%|███▍      | 668/1929 [04:01<07:36,  2.76batches/s]\u001b[A\n",
            " 35%|███▍      | 669/1929 [04:01<07:35,  2.77batches/s]\u001b[A\n",
            " 35%|███▍      | 670/1929 [04:01<07:34,  2.77batches/s]\u001b[A\n",
            " 35%|███▍      | 671/1929 [04:02<07:34,  2.77batches/s]\u001b[A\n",
            " 35%|███▍      | 672/1929 [04:02<07:33,  2.77batches/s]\u001b[A\n",
            " 35%|███▍      | 673/1929 [04:02<07:32,  2.77batches/s]\u001b[A\n",
            " 35%|███▍      | 674/1929 [04:03<07:32,  2.77batches/s]\u001b[A\n",
            " 35%|███▍      | 675/1929 [04:03<07:31,  2.78batches/s]\u001b[A\n",
            " 35%|███▌      | 676/1929 [04:03<07:31,  2.77batches/s]\u001b[A\n",
            " 35%|███▌      | 677/1929 [04:04<07:32,  2.77batches/s]\u001b[A\n",
            " 35%|███▌      | 678/1929 [04:04<07:32,  2.76batches/s]\u001b[A\n",
            " 35%|███▌      | 679/1929 [04:04<07:31,  2.77batches/s]\u001b[A\n",
            " 35%|███▌      | 680/1929 [04:05<07:30,  2.77batches/s]\u001b[A\n",
            " 35%|███▌      | 681/1929 [04:05<07:31,  2.77batches/s]\u001b[A\n",
            " 35%|███▌      | 682/1929 [04:06<07:30,  2.77batches/s]\u001b[A\n",
            " 35%|███▌      | 683/1929 [04:06<07:30,  2.77batches/s]\u001b[A\n",
            " 35%|███▌      | 684/1929 [04:06<07:30,  2.76batches/s]\u001b[A\n",
            " 36%|███▌      | 685/1929 [04:07<07:29,  2.76batches/s]\u001b[A\n",
            " 36%|███▌      | 686/1929 [04:07<07:29,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 687/1929 [04:07<07:28,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 688/1929 [04:08<07:28,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 689/1929 [04:08<07:27,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 690/1929 [04:08<07:27,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 691/1929 [04:09<07:26,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 692/1929 [04:09<07:25,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 693/1929 [04:10<07:26,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 694/1929 [04:10<07:25,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 695/1929 [04:10<07:25,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 696/1929 [04:11<07:25,  2.77batches/s]\u001b[A\n",
            " 36%|███▌      | 697/1929 [04:11<07:25,  2.76batches/s]\u001b[A\n",
            " 36%|███▌      | 698/1929 [04:11<07:25,  2.76batches/s]\u001b[A\n",
            " 36%|███▌      | 699/1929 [04:12<07:25,  2.76batches/s]\u001b[A\n",
            " 36%|███▋      | 700/1929 [04:12<07:24,  2.76batches/s]\u001b[A\n",
            " 36%|███▋      | 701/1929 [04:12<07:23,  2.77batches/s]\u001b[A\n",
            " 36%|███▋      | 702/1929 [04:13<07:24,  2.76batches/s]\u001b[A\n",
            " 36%|███▋      | 703/1929 [04:13<07:23,  2.76batches/s]\u001b[A\n",
            " 36%|███▋      | 704/1929 [04:14<07:24,  2.75batches/s]\u001b[A\n",
            " 37%|███▋      | 705/1929 [04:14<07:23,  2.76batches/s]\u001b[A\n",
            " 37%|███▋      | 706/1929 [04:14<07:23,  2.76batches/s]\u001b[A\n",
            " 37%|███▋      | 707/1929 [04:15<07:22,  2.76batches/s]\u001b[A\n",
            " 37%|███▋      | 708/1929 [04:15<07:22,  2.76batches/s]\u001b[A\n",
            " 37%|███▋      | 709/1929 [04:15<07:21,  2.76batches/s]\u001b[A\n",
            " 37%|███▋      | 710/1929 [04:16<07:22,  2.75batches/s]\u001b[A\n",
            " 37%|███▋      | 711/1929 [04:16<07:21,  2.76batches/s]\u001b[A\n",
            " 37%|███▋      | 712/1929 [04:16<07:20,  2.76batches/s]\u001b[A\n",
            " 37%|███▋      | 713/1929 [04:17<07:19,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 714/1929 [04:17<07:18,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 715/1929 [04:18<07:18,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 716/1929 [04:18<07:17,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 717/1929 [04:18<07:17,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 718/1929 [04:19<07:18,  2.76batches/s]\u001b[A\n",
            " 37%|███▋      | 719/1929 [04:19<07:17,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 720/1929 [04:19<07:16,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 721/1929 [04:20<07:16,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 722/1929 [04:20<07:16,  2.77batches/s]\u001b[A\n",
            " 37%|███▋      | 723/1929 [04:20<07:16,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 724/1929 [04:21<07:14,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 725/1929 [04:21<07:14,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 726/1929 [04:21<07:13,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 727/1929 [04:22<07:13,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 728/1929 [04:22<07:12,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 729/1929 [04:23<07:12,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 730/1929 [04:23<07:13,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 731/1929 [04:23<07:12,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 732/1929 [04:24<07:13,  2.76batches/s]\u001b[A\n",
            " 38%|███▊      | 733/1929 [04:24<07:13,  2.76batches/s]\u001b[A\n",
            " 38%|███▊      | 734/1929 [04:24<07:11,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 735/1929 [04:25<07:10,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 736/1929 [04:25<07:09,  2.78batches/s]\u001b[A\n",
            " 38%|███▊      | 737/1929 [04:25<07:10,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 738/1929 [04:26<07:10,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 739/1929 [04:26<07:09,  2.77batches/s]\u001b[A\n",
            " 38%|███▊      | 740/1929 [04:27<07:10,  2.76batches/s]\u001b[A\n",
            " 38%|███▊      | 741/1929 [04:27<07:09,  2.76batches/s]\u001b[A\n",
            " 38%|███▊      | 742/1929 [04:27<07:09,  2.76batches/s]\u001b[A\n",
            " 39%|███▊      | 743/1929 [04:28<07:09,  2.76batches/s]\u001b[A\n",
            " 39%|███▊      | 744/1929 [04:28<07:08,  2.77batches/s]\u001b[A\n",
            " 39%|███▊      | 745/1929 [04:28<07:07,  2.77batches/s]\u001b[A\n",
            " 39%|███▊      | 746/1929 [04:29<07:07,  2.77batches/s]\u001b[A\n",
            " 39%|███▊      | 747/1929 [04:29<07:06,  2.77batches/s]\u001b[A\n",
            " 39%|███▉      | 748/1929 [04:29<07:05,  2.77batches/s]\u001b[A\n",
            " 39%|███▉      | 749/1929 [04:30<07:04,  2.78batches/s]\u001b[A\n",
            " 39%|███▉      | 750/1929 [04:30<07:05,  2.77batches/s]\u001b[A\n",
            " 39%|███▉      | 751/1929 [04:31<07:05,  2.77batches/s]\u001b[A\n",
            " 39%|███▉      | 752/1929 [04:31<07:04,  2.77batches/s]\u001b[A\n",
            " 39%|███▉      | 753/1929 [04:31<07:05,  2.77batches/s]\u001b[A\n",
            " 39%|███▉      | 754/1929 [04:32<07:04,  2.77batches/s]\u001b[A\n",
            " 39%|███▉      | 755/1929 [04:32<07:04,  2.76batches/s]\u001b[A\n",
            " 39%|███▉      | 756/1929 [04:32<07:04,  2.76batches/s]\u001b[A\n",
            " 39%|███▉      | 757/1929 [04:33<07:03,  2.77batches/s]\u001b[A\n",
            " 39%|███▉      | 758/1929 [04:33<07:03,  2.76batches/s]\u001b[A\n",
            " 39%|███▉      | 759/1929 [04:33<07:04,  2.76batches/s]\u001b[A\n",
            " 39%|███▉      | 760/1929 [04:34<07:03,  2.76batches/s]\u001b[A\n",
            " 39%|███▉      | 761/1929 [04:34<07:02,  2.77batches/s]\u001b[A\n",
            " 40%|███▉      | 762/1929 [04:34<07:01,  2.77batches/s]\u001b[A\n",
            " 40%|███▉      | 763/1929 [04:35<07:00,  2.77batches/s]\u001b[A\n",
            " 40%|███▉      | 764/1929 [04:35<07:01,  2.77batches/s]\u001b[A\n",
            " 40%|███▉      | 765/1929 [04:36<07:00,  2.77batches/s]\u001b[A\n",
            " 40%|███▉      | 766/1929 [04:36<06:59,  2.77batches/s]\u001b[A\n",
            " 40%|███▉      | 767/1929 [04:36<06:59,  2.77batches/s]\u001b[A\n",
            " 40%|███▉      | 768/1929 [04:37<06:58,  2.78batches/s]\u001b[A\n",
            " 40%|███▉      | 769/1929 [04:37<06:57,  2.78batches/s]\u001b[A\n",
            " 40%|███▉      | 770/1929 [04:37<06:57,  2.78batches/s]\u001b[A\n",
            " 40%|███▉      | 771/1929 [04:38<06:57,  2.78batches/s]\u001b[A\n",
            " 40%|████      | 772/1929 [04:38<06:56,  2.78batches/s]\u001b[A\n",
            " 40%|████      | 773/1929 [04:38<06:56,  2.77batches/s]\u001b[A\n",
            " 40%|████      | 774/1929 [04:39<06:57,  2.76batches/s]\u001b[A\n",
            " 40%|████      | 775/1929 [04:39<06:57,  2.76batches/s]\u001b[A\n",
            " 40%|████      | 776/1929 [04:40<06:57,  2.76batches/s]\u001b[A\n",
            " 40%|████      | 777/1929 [04:40<06:57,  2.76batches/s]\u001b[A\n",
            " 40%|████      | 778/1929 [04:40<06:57,  2.76batches/s]\u001b[A\n",
            " 40%|████      | 779/1929 [04:41<06:55,  2.77batches/s]\u001b[A\n",
            " 40%|████      | 780/1929 [04:41<06:55,  2.76batches/s]\u001b[A\n",
            " 40%|████      | 781/1929 [04:41<06:54,  2.77batches/s]\u001b[A\n",
            " 41%|████      | 782/1929 [04:42<06:56,  2.75batches/s]\u001b[A\n",
            " 41%|████      | 783/1929 [04:42<06:56,  2.75batches/s]\u001b[A\n",
            " 41%|████      | 784/1929 [04:42<06:54,  2.76batches/s]\u001b[A\n",
            " 41%|████      | 785/1929 [04:43<06:54,  2.76batches/s]\u001b[A\n",
            " 41%|████      | 786/1929 [04:43<06:53,  2.76batches/s]\u001b[A\n",
            " 41%|████      | 787/1929 [04:44<06:53,  2.76batches/s]\u001b[A\n",
            " 41%|████      | 788/1929 [04:44<06:52,  2.77batches/s]\u001b[A\n",
            " 41%|████      | 789/1929 [04:44<06:51,  2.77batches/s]\u001b[A\n",
            " 41%|████      | 790/1929 [04:45<06:50,  2.78batches/s]\u001b[A\n",
            " 41%|████      | 791/1929 [04:45<06:50,  2.77batches/s]\u001b[A\n",
            " 41%|████      | 792/1929 [04:45<06:49,  2.77batches/s]\u001b[A\n",
            " 41%|████      | 793/1929 [04:46<06:50,  2.77batches/s]\u001b[A\n",
            " 41%|████      | 794/1929 [04:46<06:50,  2.77batches/s]\u001b[A\n",
            " 41%|████      | 795/1929 [04:46<06:50,  2.76batches/s]\u001b[A\n",
            " 41%|████▏     | 796/1929 [04:47<06:50,  2.76batches/s]\u001b[A\n",
            " 41%|████▏     | 797/1929 [04:47<06:48,  2.77batches/s]\u001b[A\n",
            " 41%|████▏     | 798/1929 [04:48<06:51,  2.75batches/s]\u001b[A\n",
            " 41%|████▏     | 799/1929 [04:48<06:50,  2.75batches/s]\u001b[A\n",
            " 41%|████▏     | 800/1929 [04:48<06:49,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 801/1929 [04:49<06:49,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 802/1929 [04:49<06:48,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 803/1929 [04:49<06:49,  2.75batches/s]\u001b[A\n",
            " 42%|████▏     | 804/1929 [04:50<06:48,  2.75batches/s]\u001b[A\n",
            " 42%|████▏     | 805/1929 [04:50<06:47,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 806/1929 [04:50<06:46,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 807/1929 [04:51<06:46,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 808/1929 [04:51<06:46,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 809/1929 [04:51<06:46,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 810/1929 [04:52<06:45,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 811/1929 [04:52<06:44,  2.77batches/s]\u001b[A\n",
            " 42%|████▏     | 812/1929 [04:53<06:45,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 813/1929 [04:53<06:43,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 814/1929 [04:53<06:43,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 815/1929 [04:54<06:43,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 816/1929 [04:54<06:43,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 817/1929 [04:54<06:43,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 818/1929 [04:55<06:43,  2.76batches/s]\u001b[A\n",
            " 42%|████▏     | 819/1929 [04:55<06:41,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 820/1929 [04:55<06:41,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 821/1929 [04:56<06:41,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 822/1929 [04:56<06:41,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 823/1929 [04:57<06:41,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 824/1929 [04:57<06:41,  2.75batches/s]\u001b[A\n",
            " 43%|████▎     | 825/1929 [04:57<06:40,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 826/1929 [04:58<06:41,  2.74batches/s]\u001b[A\n",
            " 43%|████▎     | 827/1929 [04:58<06:41,  2.75batches/s]\u001b[A\n",
            " 43%|████▎     | 828/1929 [04:58<06:40,  2.75batches/s]\u001b[A\n",
            " 43%|████▎     | 829/1929 [04:59<06:39,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 830/1929 [04:59<06:38,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 831/1929 [04:59<06:37,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 832/1929 [05:00<06:37,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 833/1929 [05:00<06:38,  2.75batches/s]\u001b[A\n",
            " 43%|████▎     | 834/1929 [05:01<06:38,  2.75batches/s]\u001b[A\n",
            " 43%|████▎     | 835/1929 [05:01<06:36,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 836/1929 [05:01<06:36,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 837/1929 [05:02<06:35,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 838/1929 [05:02<06:34,  2.76batches/s]\u001b[A\n",
            " 43%|████▎     | 839/1929 [05:02<06:34,  2.77batches/s]\u001b[A\n",
            " 44%|████▎     | 840/1929 [05:03<06:33,  2.77batches/s]\u001b[A\n",
            " 44%|████▎     | 841/1929 [05:03<06:33,  2.76batches/s]\u001b[A\n",
            " 44%|████▎     | 842/1929 [05:03<06:33,  2.76batches/s]\u001b[A\n",
            " 44%|████▎     | 843/1929 [05:04<06:33,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 844/1929 [05:04<06:32,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 845/1929 [05:05<06:32,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 846/1929 [05:05<06:31,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 847/1929 [05:05<06:32,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 848/1929 [05:06<06:31,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 849/1929 [05:06<06:30,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 850/1929 [05:06<06:31,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 851/1929 [05:07<06:30,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 852/1929 [05:07<06:30,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 853/1929 [05:07<06:29,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 854/1929 [05:08<06:29,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 855/1929 [05:08<06:30,  2.75batches/s]\u001b[A\n",
            " 44%|████▍     | 856/1929 [05:09<06:29,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 857/1929 [05:09<06:28,  2.76batches/s]\u001b[A\n",
            " 44%|████▍     | 858/1929 [05:09<06:27,  2.77batches/s]\u001b[A\n",
            " 45%|████▍     | 859/1929 [05:10<06:27,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 860/1929 [05:10<06:27,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 861/1929 [05:10<06:26,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 862/1929 [05:11<06:26,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 863/1929 [05:11<06:25,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 864/1929 [05:11<06:26,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 865/1929 [05:12<06:24,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 866/1929 [05:12<06:25,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 867/1929 [05:13<06:25,  2.76batches/s]\u001b[A\n",
            " 45%|████▍     | 868/1929 [05:13<06:24,  2.76batches/s]\u001b[A\n",
            " 45%|████▌     | 869/1929 [05:13<06:23,  2.76batches/s]\u001b[A\n",
            " 45%|████▌     | 870/1929 [05:14<06:24,  2.76batches/s]\u001b[A\n",
            " 45%|████▌     | 871/1929 [05:14<06:26,  2.74batches/s]\u001b[A\n",
            " 45%|████▌     | 872/1929 [05:14<06:24,  2.75batches/s]\u001b[A\n",
            " 45%|████▌     | 873/1929 [05:15<06:23,  2.75batches/s]\u001b[A\n",
            " 45%|████▌     | 874/1929 [05:15<06:22,  2.76batches/s]\u001b[A\n",
            " 45%|████▌     | 875/1929 [05:15<06:22,  2.76batches/s]\u001b[A\n",
            " 45%|████▌     | 876/1929 [05:16<06:22,  2.75batches/s]\u001b[A\n",
            " 45%|████▌     | 877/1929 [05:16<06:22,  2.75batches/s]\u001b[A\n",
            " 46%|████▌     | 878/1929 [05:17<06:21,  2.75batches/s]\u001b[A\n",
            " 46%|████▌     | 879/1929 [05:17<06:21,  2.76batches/s]\u001b[A\n",
            " 46%|████▌     | 880/1929 [05:17<06:21,  2.75batches/s]\u001b[A\n",
            " 46%|████▌     | 881/1929 [05:18<06:20,  2.75batches/s]\u001b[A\n",
            " 46%|████▌     | 882/1929 [05:18<06:20,  2.75batches/s]\u001b[A\n",
            " 46%|████▌     | 883/1929 [05:18<06:19,  2.75batches/s]\u001b[A\n",
            " 46%|████▌     | 884/1929 [05:19<06:18,  2.76batches/s]\u001b[A\n",
            " 46%|████▌     | 885/1929 [05:19<06:19,  2.75batches/s]\u001b[A\n",
            " 46%|████▌     | 886/1929 [05:19<06:18,  2.76batches/s]\u001b[A\n",
            " 46%|████▌     | 887/1929 [05:20<06:17,  2.76batches/s]\u001b[A\n",
            " 46%|████▌     | 888/1929 [05:20<06:17,  2.76batches/s]\u001b[A\n",
            " 46%|████▌     | 889/1929 [05:20<06:16,  2.76batches/s]\u001b[A\n",
            " 46%|████▌     | 890/1929 [05:21<06:16,  2.76batches/s]\u001b[A\n",
            " 46%|████▌     | 891/1929 [05:21<06:15,  2.76batches/s]\u001b[A\n",
            " 46%|████▌     | 892/1929 [05:22<06:15,  2.76batches/s]\u001b[A\n",
            " 46%|████▋     | 893/1929 [05:22<06:15,  2.76batches/s]\u001b[A\n",
            " 46%|████▋     | 894/1929 [05:22<06:15,  2.76batches/s]\u001b[A\n",
            " 46%|████▋     | 895/1929 [05:23<06:14,  2.76batches/s]\u001b[A\n",
            " 46%|████▋     | 896/1929 [05:23<06:13,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 897/1929 [05:23<06:12,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 898/1929 [05:24<06:11,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 899/1929 [05:24<06:12,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 900/1929 [05:24<06:11,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 901/1929 [05:25<06:10,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 902/1929 [05:25<06:10,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 903/1929 [05:26<06:10,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 904/1929 [05:26<06:10,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 905/1929 [05:26<06:09,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 906/1929 [05:27<06:10,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 907/1929 [05:27<06:10,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 908/1929 [05:27<06:09,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 909/1929 [05:28<06:09,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 910/1929 [05:28<06:09,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 911/1929 [05:28<06:08,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 912/1929 [05:29<06:07,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 913/1929 [05:29<06:07,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 914/1929 [05:30<06:06,  2.77batches/s]\u001b[A\n",
            " 47%|████▋     | 915/1929 [05:30<06:07,  2.76batches/s]\u001b[A\n",
            " 47%|████▋     | 916/1929 [05:30<06:07,  2.76batches/s]\u001b[A\n",
            " 48%|████▊     | 917/1929 [05:31<06:07,  2.76batches/s]\u001b[A\n",
            " 48%|████▊     | 918/1929 [05:31<06:06,  2.76batches/s]\u001b[A\n",
            " 48%|████▊     | 919/1929 [05:31<06:05,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 920/1929 [05:32<06:04,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 921/1929 [05:32<06:03,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 922/1929 [05:32<06:02,  2.78batches/s]\u001b[A\n",
            " 48%|████▊     | 923/1929 [05:33<06:03,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 924/1929 [05:33<06:02,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 925/1929 [05:34<06:03,  2.76batches/s]\u001b[A\n",
            " 48%|████▊     | 926/1929 [05:34<06:02,  2.76batches/s]\u001b[A\n",
            " 48%|████▊     | 927/1929 [05:34<06:01,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 928/1929 [05:35<06:01,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 929/1929 [05:35<06:01,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 930/1929 [05:35<06:02,  2.76batches/s]\u001b[A\n",
            " 48%|████▊     | 931/1929 [05:36<06:01,  2.76batches/s]\u001b[A\n",
            " 48%|████▊     | 932/1929 [05:36<06:00,  2.76batches/s]\u001b[A\n",
            " 48%|████▊     | 933/1929 [05:36<05:59,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 934/1929 [05:37<05:59,  2.77batches/s]\u001b[A\n",
            " 48%|████▊     | 935/1929 [05:37<05:58,  2.77batches/s]\u001b[A\n",
            " 49%|████▊     | 936/1929 [05:37<05:58,  2.77batches/s]\u001b[A\n",
            " 49%|████▊     | 937/1929 [05:38<05:58,  2.77batches/s]\u001b[A\n",
            " 49%|████▊     | 938/1929 [05:38<05:58,  2.76batches/s]\u001b[A\n",
            " 49%|████▊     | 939/1929 [05:39<05:57,  2.77batches/s]\u001b[A\n",
            " 49%|████▊     | 940/1929 [05:39<05:57,  2.76batches/s]\u001b[A\n",
            " 49%|████▉     | 941/1929 [05:39<05:57,  2.77batches/s]\u001b[A\n",
            " 49%|████▉     | 942/1929 [05:40<05:56,  2.77batches/s]\u001b[A\n",
            " 49%|████▉     | 943/1929 [05:40<05:56,  2.77batches/s]\u001b[A\n",
            " 49%|████▉     | 944/1929 [05:40<05:55,  2.77batches/s]\u001b[A\n",
            " 49%|████▉     | 945/1929 [05:41<05:55,  2.77batches/s]\u001b[A\n",
            " 49%|████▉     | 946/1929 [05:41<05:54,  2.77batches/s]\u001b[A\n",
            " 49%|████▉     | 947/1929 [05:41<05:53,  2.78batches/s]\u001b[A\n",
            " 49%|████▉     | 948/1929 [05:42<05:52,  2.78batches/s]\u001b[A\n",
            " 49%|████▉     | 949/1929 [05:42<05:53,  2.78batches/s]\u001b[A\n",
            " 49%|████▉     | 950/1929 [05:43<05:53,  2.77batches/s]\u001b[A\n",
            " 49%|████▉     | 951/1929 [05:43<05:52,  2.78batches/s]\u001b[A\n",
            " 49%|████▉     | 952/1929 [05:43<05:51,  2.78batches/s]\u001b[A\n",
            " 49%|████▉     | 953/1929 [05:44<05:51,  2.78batches/s]\u001b[A\n",
            " 49%|████▉     | 954/1929 [05:44<05:50,  2.78batches/s]\u001b[A\n",
            " 50%|████▉     | 955/1929 [05:44<05:50,  2.78batches/s]\u001b[A\n",
            " 50%|████▉     | 956/1929 [05:45<05:50,  2.77batches/s]\u001b[A\n",
            " 50%|████▉     | 957/1929 [05:45<05:50,  2.78batches/s]\u001b[A\n",
            " 50%|████▉     | 958/1929 [05:45<05:49,  2.78batches/s]\u001b[A\n",
            " 50%|████▉     | 959/1929 [05:46<05:49,  2.78batches/s]\u001b[A\n",
            " 50%|████▉     | 960/1929 [05:46<05:48,  2.78batches/s]\u001b[A\n",
            " 50%|████▉     | 961/1929 [05:47<05:48,  2.78batches/s]\u001b[A\n",
            " 50%|████▉     | 962/1929 [05:47<05:48,  2.78batches/s]\u001b[A\n",
            " 50%|████▉     | 963/1929 [05:47<05:48,  2.77batches/s]\u001b[A\n",
            " 50%|████▉     | 964/1929 [05:48<05:48,  2.77batches/s]\u001b[A\n",
            " 50%|█████     | 965/1929 [05:48<05:47,  2.77batches/s]\u001b[A\n",
            " 50%|█████     | 966/1929 [05:48<05:47,  2.77batches/s]\u001b[A\n",
            " 50%|█████     | 967/1929 [05:49<05:46,  2.78batches/s]\u001b[A\n",
            " 50%|█████     | 968/1929 [05:49<05:45,  2.78batches/s]\u001b[A\n",
            " 50%|█████     | 969/1929 [05:49<05:45,  2.78batches/s]\u001b[A\n",
            " 50%|█████     | 970/1929 [05:50<05:44,  2.78batches/s]\u001b[A\n",
            " 50%|█████     | 971/1929 [05:50<05:43,  2.79batches/s]\u001b[A\n",
            " 50%|█████     | 972/1929 [05:50<05:44,  2.78batches/s]\u001b[A\n",
            " 50%|█████     | 973/1929 [05:51<05:44,  2.78batches/s]\u001b[A\n",
            " 50%|█████     | 974/1929 [05:51<05:43,  2.78batches/s]\u001b[A\n",
            " 51%|█████     | 975/1929 [05:52<05:43,  2.78batches/s]\u001b[A\n",
            " 51%|█████     | 976/1929 [05:52<05:43,  2.78batches/s]\u001b[A\n",
            " 51%|█████     | 977/1929 [05:52<05:43,  2.77batches/s]\u001b[A\n",
            " 51%|█████     | 978/1929 [05:53<05:44,  2.76batches/s]\u001b[A\n",
            " 51%|█████     | 979/1929 [05:53<05:44,  2.76batches/s]\u001b[A\n",
            " 51%|█████     | 980/1929 [05:53<05:43,  2.76batches/s]\u001b[A\n",
            " 51%|█████     | 981/1929 [05:54<05:43,  2.76batches/s]\u001b[A\n",
            " 51%|█████     | 982/1929 [05:54<05:42,  2.77batches/s]\u001b[A\n",
            " 51%|█████     | 983/1929 [05:54<05:41,  2.77batches/s]\u001b[A\n",
            " 51%|█████     | 984/1929 [05:55<05:41,  2.77batches/s]\u001b[A\n",
            " 51%|█████     | 985/1929 [05:55<05:41,  2.77batches/s]\u001b[A\n",
            " 51%|█████     | 986/1929 [05:56<05:40,  2.77batches/s]\u001b[A\n",
            " 51%|█████     | 987/1929 [05:56<05:40,  2.77batches/s]\u001b[A\n",
            " 51%|█████     | 988/1929 [05:56<05:39,  2.77batches/s]\u001b[A\n",
            " 51%|█████▏    | 989/1929 [05:57<05:38,  2.77batches/s]\u001b[A\n",
            " 51%|█████▏    | 990/1929 [05:57<05:38,  2.77batches/s]\u001b[A\n",
            " 51%|█████▏    | 991/1929 [05:57<05:38,  2.77batches/s]\u001b[A\n",
            " 51%|█████▏    | 992/1929 [05:58<05:37,  2.77batches/s]\u001b[A\n",
            " 51%|█████▏    | 993/1929 [05:58<05:37,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 994/1929 [05:58<05:37,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 995/1929 [05:59<05:37,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 996/1929 [05:59<05:36,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 997/1929 [05:59<05:35,  2.78batches/s]\u001b[A\n",
            " 52%|█████▏    | 998/1929 [06:00<05:35,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 999/1929 [06:00<05:35,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 1000/1929 [06:01<05:35,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 1001/1929 [06:01<05:34,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 1002/1929 [06:01<05:34,  2.78batches/s]\u001b[A\n",
            " 52%|█████▏    | 1003/1929 [06:02<05:34,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 1004/1929 [06:02<05:32,  2.78batches/s]\u001b[A\n",
            " 52%|█████▏    | 1005/1929 [06:02<05:32,  2.78batches/s]\u001b[A\n",
            " 52%|█████▏    | 1006/1929 [06:03<05:32,  2.78batches/s]\u001b[A\n",
            " 52%|█████▏    | 1007/1929 [06:03<05:32,  2.78batches/s]\u001b[A\n",
            " 52%|█████▏    | 1008/1929 [06:03<05:31,  2.78batches/s]\u001b[A\n",
            " 52%|█████▏    | 1009/1929 [06:04<05:33,  2.76batches/s]\u001b[A\n",
            " 52%|█████▏    | 1010/1929 [06:04<05:32,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 1011/1929 [06:05<05:31,  2.77batches/s]\u001b[A\n",
            " 52%|█████▏    | 1012/1929 [06:05<05:31,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1013/1929 [06:05<05:30,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1014/1929 [06:06<05:30,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1015/1929 [06:06<05:29,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1016/1929 [06:06<05:29,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1017/1929 [06:07<05:28,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1018/1929 [06:07<05:28,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1019/1929 [06:07<05:27,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1020/1929 [06:08<05:27,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1021/1929 [06:08<05:26,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1022/1929 [06:09<05:26,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1023/1929 [06:09<05:27,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1024/1929 [06:09<05:26,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1025/1929 [06:10<05:26,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1026/1929 [06:10<05:25,  2.77batches/s]\u001b[A\n",
            " 53%|█████▎    | 1027/1929 [06:10<05:24,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1028/1929 [06:11<05:24,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1029/1929 [06:11<05:23,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1030/1929 [06:11<05:22,  2.78batches/s]\u001b[A\n",
            " 53%|█████▎    | 1031/1929 [06:12<05:22,  2.79batches/s]\u001b[A\n",
            " 53%|█████▎    | 1032/1929 [06:12<05:22,  2.78batches/s]\u001b[A\n",
            " 54%|█████▎    | 1033/1929 [06:12<05:22,  2.78batches/s]\u001b[A\n",
            " 54%|█████▎    | 1034/1929 [06:13<05:22,  2.78batches/s]\u001b[A\n",
            " 54%|█████▎    | 1035/1929 [06:13<05:22,  2.77batches/s]\u001b[A\n",
            " 54%|█████▎    | 1036/1929 [06:14<05:22,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1037/1929 [06:14<05:22,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1038/1929 [06:14<05:21,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1039/1929 [06:15<05:20,  2.78batches/s]\u001b[A\n",
            " 54%|█████▍    | 1040/1929 [06:15<05:20,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1041/1929 [06:15<05:21,  2.76batches/s]\u001b[A\n",
            " 54%|█████▍    | 1042/1929 [06:16<05:20,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1043/1929 [06:16<05:18,  2.78batches/s]\u001b[A\n",
            " 54%|█████▍    | 1044/1929 [06:16<05:19,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1045/1929 [06:17<05:19,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1046/1929 [06:17<05:18,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1047/1929 [06:18<05:18,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1048/1929 [06:18<05:18,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1049/1929 [06:18<05:17,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1050/1929 [06:19<05:17,  2.77batches/s]\u001b[A\n",
            " 54%|█████▍    | 1051/1929 [06:19<05:16,  2.77batches/s]\u001b[A\n",
            " 55%|█████▍    | 1052/1929 [06:19<05:17,  2.77batches/s]\u001b[A\n",
            " 55%|█████▍    | 1053/1929 [06:20<05:16,  2.77batches/s]\u001b[A\n",
            " 55%|█████▍    | 1054/1929 [06:20<05:15,  2.77batches/s]\u001b[A\n",
            " 55%|█████▍    | 1055/1929 [06:20<05:15,  2.77batches/s]\u001b[A\n",
            " 55%|█████▍    | 1056/1929 [06:21<05:15,  2.77batches/s]\u001b[A\n",
            " 55%|█████▍    | 1057/1929 [06:21<05:15,  2.76batches/s]\u001b[A\n",
            " 55%|█████▍    | 1058/1929 [06:21<05:15,  2.76batches/s]\u001b[A\n",
            " 55%|█████▍    | 1059/1929 [06:22<05:14,  2.76batches/s]\u001b[A\n",
            " 55%|█████▍    | 1060/1929 [06:22<05:14,  2.77batches/s]\u001b[A\n",
            " 55%|█████▌    | 1061/1929 [06:23<05:13,  2.77batches/s]\u001b[A\n",
            " 55%|█████▌    | 1062/1929 [06:23<05:12,  2.77batches/s]\u001b[A\n",
            " 55%|█████▌    | 1063/1929 [06:23<05:12,  2.77batches/s]\u001b[A\n",
            " 55%|█████▌    | 1064/1929 [06:24<05:12,  2.77batches/s]\u001b[A\n",
            " 55%|█████▌    | 1065/1929 [06:24<05:12,  2.77batches/s]\u001b[A\n",
            " 55%|█████▌    | 1066/1929 [06:24<05:11,  2.77batches/s]\u001b[A\n",
            " 55%|█████▌    | 1067/1929 [06:25<05:10,  2.78batches/s]\u001b[A\n",
            " 55%|█████▌    | 1068/1929 [06:25<05:09,  2.78batches/s]\u001b[A\n",
            " 55%|█████▌    | 1069/1929 [06:25<05:09,  2.78batches/s]\u001b[A\n",
            " 55%|█████▌    | 1070/1929 [06:26<05:09,  2.77batches/s]\u001b[A\n",
            " 56%|█████▌    | 1071/1929 [06:26<05:09,  2.77batches/s]\u001b[A\n",
            " 56%|█████▌    | 1072/1929 [06:27<05:09,  2.76batches/s]\u001b[A\n",
            " 56%|█████▌    | 1073/1929 [06:27<05:10,  2.76batches/s]\u001b[A\n",
            " 56%|█████▌    | 1074/1929 [06:27<05:10,  2.76batches/s]\u001b[A\n",
            " 56%|█████▌    | 1075/1929 [06:28<05:09,  2.76batches/s]\u001b[A\n",
            " 56%|█████▌    | 1076/1929 [06:28<05:09,  2.76batches/s]\u001b[A\n",
            " 56%|█████▌    | 1077/1929 [06:28<05:08,  2.76batches/s]\u001b[A\n",
            " 56%|█████▌    | 1078/1929 [06:29<05:08,  2.76batches/s]\u001b[A\n",
            " 56%|█████▌    | 1079/1929 [06:29<05:07,  2.77batches/s]\u001b[A\n",
            " 56%|█████▌    | 1080/1929 [06:29<05:06,  2.77batches/s]\u001b[A\n",
            " 56%|█████▌    | 1081/1929 [06:30<05:06,  2.77batches/s]\u001b[A\n",
            " 56%|█████▌    | 1082/1929 [06:30<05:06,  2.77batches/s]\u001b[A\n",
            " 56%|█████▌    | 1083/1929 [06:31<05:05,  2.77batches/s]\u001b[A\n",
            " 56%|█████▌    | 1084/1929 [06:31<05:04,  2.77batches/s]\u001b[A\n",
            " 56%|█████▌    | 1085/1929 [06:31<05:03,  2.78batches/s]\u001b[A\n",
            " 56%|█████▋    | 1086/1929 [06:32<05:03,  2.77batches/s]\u001b[A\n",
            " 56%|█████▋    | 1087/1929 [06:32<05:03,  2.78batches/s]\u001b[A\n",
            " 56%|█████▋    | 1088/1929 [06:32<05:03,  2.77batches/s]\u001b[A\n",
            " 56%|█████▋    | 1089/1929 [06:33<05:03,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1090/1929 [06:33<05:02,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1091/1929 [06:33<05:02,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1092/1929 [06:34<05:01,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1093/1929 [06:34<05:01,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1094/1929 [06:34<05:00,  2.78batches/s]\u001b[A\n",
            " 57%|█████▋    | 1095/1929 [06:35<05:00,  2.78batches/s]\u001b[A\n",
            " 57%|█████▋    | 1096/1929 [06:35<04:59,  2.78batches/s]\u001b[A\n",
            " 57%|█████▋    | 1097/1929 [06:36<04:59,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1098/1929 [06:36<04:59,  2.78batches/s]\u001b[A\n",
            " 57%|█████▋    | 1099/1929 [06:36<04:58,  2.78batches/s]\u001b[A\n",
            " 57%|█████▋    | 1100/1929 [06:37<04:58,  2.78batches/s]\u001b[A\n",
            " 57%|█████▋    | 1101/1929 [06:37<04:57,  2.78batches/s]\u001b[A\n",
            " 57%|█████▋    | 1102/1929 [06:37<04:57,  2.78batches/s]\u001b[A\n",
            " 57%|█████▋    | 1103/1929 [06:38<04:57,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1104/1929 [06:38<04:58,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1105/1929 [06:38<04:57,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1106/1929 [06:39<04:57,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1107/1929 [06:39<04:56,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1108/1929 [06:40<04:56,  2.77batches/s]\u001b[A\n",
            " 57%|█████▋    | 1109/1929 [06:40<04:56,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1110/1929 [06:40<04:55,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1111/1929 [06:41<04:55,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1112/1929 [06:41<04:54,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1113/1929 [06:41<04:53,  2.78batches/s]\u001b[A\n",
            " 58%|█████▊    | 1114/1929 [06:42<04:53,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1115/1929 [06:42<04:53,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1116/1929 [06:42<04:53,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1117/1929 [06:43<04:53,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1118/1929 [06:43<04:52,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1119/1929 [06:44<04:53,  2.76batches/s]\u001b[A\n",
            " 58%|█████▊    | 1120/1929 [06:44<04:53,  2.76batches/s]\u001b[A\n",
            " 58%|█████▊    | 1121/1929 [06:44<04:52,  2.76batches/s]\u001b[A\n",
            " 58%|█████▊    | 1122/1929 [06:45<04:52,  2.76batches/s]\u001b[A\n",
            " 58%|█████▊    | 1123/1929 [06:45<04:51,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1124/1929 [06:45<04:50,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1125/1929 [06:46<04:50,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1126/1929 [06:46<04:49,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1127/1929 [06:46<04:49,  2.77batches/s]\u001b[A\n",
            " 58%|█████▊    | 1128/1929 [06:47<04:48,  2.78batches/s]\u001b[A\n",
            " 59%|█████▊    | 1129/1929 [06:47<04:48,  2.78batches/s]\u001b[A\n",
            " 59%|█████▊    | 1130/1929 [06:47<04:47,  2.78batches/s]\u001b[A\n",
            " 59%|█████▊    | 1131/1929 [06:48<04:47,  2.77batches/s]\u001b[A\n",
            " 59%|█████▊    | 1132/1929 [06:48<04:46,  2.78batches/s]\u001b[A\n",
            " 59%|█████▊    | 1133/1929 [06:49<04:47,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1134/1929 [06:49<04:47,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1135/1929 [06:49<04:46,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1136/1929 [06:50<04:46,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1137/1929 [06:50<04:45,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1138/1929 [06:50<04:45,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1139/1929 [06:51<04:45,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1140/1929 [06:51<04:45,  2.76batches/s]\u001b[A\n",
            " 59%|█████▉    | 1141/1929 [06:51<04:44,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1142/1929 [06:52<04:44,  2.76batches/s]\u001b[A\n",
            " 59%|█████▉    | 1143/1929 [06:52<04:44,  2.76batches/s]\u001b[A\n",
            " 59%|█████▉    | 1144/1929 [06:53<04:43,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1145/1929 [06:53<04:43,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1146/1929 [06:53<04:42,  2.77batches/s]\u001b[A\n",
            " 59%|█████▉    | 1147/1929 [06:54<04:41,  2.78batches/s]\u001b[A\n",
            " 60%|█████▉    | 1148/1929 [06:54<04:40,  2.78batches/s]\u001b[A\n",
            " 60%|█████▉    | 1149/1929 [06:54<04:40,  2.78batches/s]\u001b[A\n",
            " 60%|█████▉    | 1150/1929 [06:55<04:40,  2.77batches/s]\u001b[A\n",
            " 60%|█████▉    | 1151/1929 [06:55<04:39,  2.78batches/s]\u001b[A\n",
            " 60%|█████▉    | 1152/1929 [06:55<04:39,  2.78batches/s]\u001b[A\n",
            " 60%|█████▉    | 1153/1929 [06:56<04:39,  2.78batches/s]\u001b[A\n",
            " 60%|█████▉    | 1154/1929 [06:56<04:39,  2.77batches/s]\u001b[A\n",
            " 60%|█████▉    | 1155/1929 [06:56<04:38,  2.77batches/s]\u001b[A\n",
            " 60%|█████▉    | 1156/1929 [06:57<04:38,  2.77batches/s]\u001b[A\n",
            " 60%|█████▉    | 1157/1929 [06:57<04:38,  2.77batches/s]\u001b[A\n",
            " 60%|██████    | 1158/1929 [06:58<04:37,  2.77batches/s]\u001b[A\n",
            " 60%|██████    | 1159/1929 [06:58<04:37,  2.77batches/s]\u001b[A\n",
            " 60%|██████    | 1160/1929 [06:58<04:37,  2.77batches/s]\u001b[A\n",
            " 60%|██████    | 1161/1929 [06:59<04:36,  2.78batches/s]\u001b[A\n",
            " 60%|██████    | 1162/1929 [06:59<04:36,  2.78batches/s]\u001b[A\n",
            " 60%|██████    | 1163/1929 [06:59<04:35,  2.78batches/s]\u001b[A\n",
            " 60%|██████    | 1164/1929 [07:00<04:35,  2.78batches/s]\u001b[A\n",
            " 60%|██████    | 1165/1929 [07:00<04:34,  2.78batches/s]\u001b[A\n",
            " 60%|██████    | 1166/1929 [07:00<04:34,  2.78batches/s]\u001b[A\n",
            " 60%|██████    | 1167/1929 [07:01<04:33,  2.78batches/s]\u001b[A\n",
            " 61%|██████    | 1168/1929 [07:01<04:33,  2.78batches/s]\u001b[A\n",
            " 61%|██████    | 1169/1929 [07:02<04:33,  2.78batches/s]\u001b[A\n",
            " 61%|██████    | 1170/1929 [07:02<04:33,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1171/1929 [07:02<04:33,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1172/1929 [07:03<04:33,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1173/1929 [07:03<04:32,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1174/1929 [07:03<04:32,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1175/1929 [07:04<04:32,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1176/1929 [07:04<04:31,  2.78batches/s]\u001b[A\n",
            " 61%|██████    | 1177/1929 [07:04<04:30,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1178/1929 [07:05<04:30,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1179/1929 [07:05<04:30,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1180/1929 [07:06<04:30,  2.77batches/s]\u001b[A\n",
            " 61%|██████    | 1181/1929 [07:06<04:30,  2.77batches/s]\u001b[A\n",
            " 61%|██████▏   | 1182/1929 [07:06<04:29,  2.77batches/s]\u001b[A\n",
            " 61%|██████▏   | 1183/1929 [07:07<04:28,  2.78batches/s]\u001b[A\n",
            " 61%|██████▏   | 1184/1929 [07:07<04:28,  2.78batches/s]\u001b[A\n",
            " 61%|██████▏   | 1185/1929 [07:07<04:29,  2.77batches/s]\u001b[A\n",
            " 61%|██████▏   | 1186/1929 [07:08<04:28,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1187/1929 [07:08<04:27,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1188/1929 [07:08<04:27,  2.78batches/s]\u001b[A\n",
            " 62%|██████▏   | 1189/1929 [07:09<04:26,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1190/1929 [07:09<04:26,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1191/1929 [07:09<04:26,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1192/1929 [07:10<04:25,  2.78batches/s]\u001b[A\n",
            " 62%|██████▏   | 1193/1929 [07:10<04:24,  2.78batches/s]\u001b[A\n",
            " 62%|██████▏   | 1194/1929 [07:11<04:25,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1195/1929 [07:11<04:25,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1196/1929 [07:11<04:24,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1197/1929 [07:12<04:23,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1198/1929 [07:12<04:23,  2.78batches/s]\u001b[A\n",
            " 62%|██████▏   | 1199/1929 [07:12<04:23,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1200/1929 [07:13<04:23,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1201/1929 [07:13<04:22,  2.77batches/s]\u001b[A\n",
            " 62%|██████▏   | 1202/1929 [07:13<04:23,  2.76batches/s]\u001b[A\n",
            " 62%|██████▏   | 1203/1929 [07:14<04:22,  2.76batches/s]\u001b[A\n",
            " 62%|██████▏   | 1204/1929 [07:14<04:22,  2.76batches/s]\u001b[A\n",
            " 62%|██████▏   | 1205/1929 [07:15<04:21,  2.76batches/s]\u001b[A\n",
            " 63%|██████▎   | 1206/1929 [07:15<04:21,  2.77batches/s]\u001b[A\n",
            " 63%|██████▎   | 1207/1929 [07:15<04:21,  2.76batches/s]\u001b[A\n",
            " 63%|██████▎   | 1208/1929 [07:16<04:20,  2.77batches/s]\u001b[A\n",
            " 63%|██████▎   | 1209/1929 [07:16<04:19,  2.78batches/s]\u001b[A\n",
            " 63%|██████▎   | 1210/1929 [07:16<04:19,  2.77batches/s]\u001b[A\n",
            " 63%|██████▎   | 1211/1929 [07:17<04:19,  2.77batches/s]\u001b[A\n",
            " 63%|██████▎   | 1212/1929 [07:17<04:20,  2.76batches/s]\u001b[A\n",
            " 63%|██████▎   | 1213/1929 [07:17<04:19,  2.76batches/s]\u001b[A\n",
            " 63%|██████▎   | 1214/1929 [07:18<04:18,  2.76batches/s]\u001b[A\n",
            " 63%|██████▎   | 1215/1929 [07:18<04:17,  2.77batches/s]\u001b[A\n",
            " 63%|██████▎   | 1216/1929 [07:19<04:17,  2.77batches/s]\u001b[A\n",
            " 63%|██████▎   | 1217/1929 [07:19<04:16,  2.77batches/s]\u001b[A\n",
            " 63%|██████▎   | 1218/1929 [07:19<04:15,  2.78batches/s]\u001b[A\n",
            " 63%|██████▎   | 1219/1929 [07:20<04:15,  2.78batches/s]\u001b[A\n",
            " 63%|██████▎   | 1220/1929 [07:20<04:14,  2.78batches/s]\u001b[A\n",
            " 63%|██████▎   | 1221/1929 [07:20<04:14,  2.79batches/s]\u001b[A\n",
            " 63%|██████▎   | 1222/1929 [07:21<04:13,  2.79batches/s]\u001b[A\n",
            " 63%|██████▎   | 1223/1929 [07:21<04:13,  2.78batches/s]\u001b[A\n",
            " 63%|██████▎   | 1224/1929 [07:21<04:12,  2.79batches/s]\u001b[A\n",
            " 64%|██████▎   | 1225/1929 [07:22<04:13,  2.78batches/s]\u001b[A\n",
            " 64%|██████▎   | 1226/1929 [07:22<04:12,  2.79batches/s]\u001b[A\n",
            " 64%|██████▎   | 1227/1929 [07:22<04:12,  2.78batches/s]\u001b[A\n",
            " 64%|██████▎   | 1228/1929 [07:23<04:12,  2.77batches/s]\u001b[A\n",
            " 64%|██████▎   | 1229/1929 [07:23<04:13,  2.76batches/s]\u001b[A\n",
            " 64%|██████▍   | 1230/1929 [07:24<04:13,  2.76batches/s]\u001b[A\n",
            " 64%|██████▍   | 1231/1929 [07:24<04:12,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1232/1929 [07:24<04:11,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1233/1929 [07:25<04:11,  2.76batches/s]\u001b[A\n",
            " 64%|██████▍   | 1234/1929 [07:25<04:10,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1235/1929 [07:25<04:10,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1236/1929 [07:26<04:10,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1237/1929 [07:26<04:09,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1238/1929 [07:26<04:08,  2.78batches/s]\u001b[A\n",
            " 64%|██████▍   | 1239/1929 [07:27<04:08,  2.78batches/s]\u001b[A\n",
            " 64%|██████▍   | 1240/1929 [07:27<04:08,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1241/1929 [07:28<04:08,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1242/1929 [07:28<04:07,  2.77batches/s]\u001b[A\n",
            " 64%|██████▍   | 1243/1929 [07:28<04:08,  2.76batches/s]\u001b[A\n",
            " 64%|██████▍   | 1244/1929 [07:29<04:07,  2.77batches/s]\u001b[A\n",
            " 65%|██████▍   | 1245/1929 [07:29<04:07,  2.77batches/s]\u001b[A\n",
            " 65%|██████▍   | 1246/1929 [07:29<04:06,  2.78batches/s]\u001b[A\n",
            " 65%|██████▍   | 1247/1929 [07:30<04:06,  2.77batches/s]\u001b[A\n",
            " 65%|██████▍   | 1248/1929 [07:30<04:06,  2.77batches/s]\u001b[A\n",
            " 65%|██████▍   | 1249/1929 [07:30<04:05,  2.77batches/s]\u001b[A\n",
            " 65%|██████▍   | 1250/1929 [07:31<04:05,  2.77batches/s]\u001b[A\n",
            " 65%|██████▍   | 1251/1929 [07:31<04:04,  2.77batches/s]\u001b[A\n",
            " 65%|██████▍   | 1252/1929 [07:31<04:05,  2.76batches/s]\u001b[A\n",
            " 65%|██████▍   | 1253/1929 [07:32<04:04,  2.77batches/s]\u001b[A\n",
            " 65%|██████▌   | 1254/1929 [07:32<04:04,  2.76batches/s]\u001b[A\n",
            " 65%|██████▌   | 1255/1929 [07:33<04:03,  2.77batches/s]\u001b[A\n",
            " 65%|██████▌   | 1256/1929 [07:33<04:02,  2.77batches/s]\u001b[A\n",
            " 65%|██████▌   | 1257/1929 [07:33<04:02,  2.77batches/s]\u001b[A\n",
            " 65%|██████▌   | 1258/1929 [07:34<04:02,  2.77batches/s]\u001b[A\n",
            " 65%|██████▌   | 1259/1929 [07:34<04:01,  2.77batches/s]\u001b[A\n",
            " 65%|██████▌   | 1260/1929 [07:34<04:02,  2.75batches/s]\u001b[A\n",
            " 65%|██████▌   | 1261/1929 [07:35<04:02,  2.76batches/s]\u001b[A\n",
            " 65%|██████▌   | 1262/1929 [07:35<04:01,  2.76batches/s]\u001b[A\n",
            " 65%|██████▌   | 1263/1929 [07:35<04:01,  2.76batches/s]\u001b[A\n",
            " 66%|██████▌   | 1264/1929 [07:36<04:00,  2.76batches/s]\u001b[A\n",
            " 66%|██████▌   | 1265/1929 [07:36<04:00,  2.77batches/s]\u001b[A\n",
            " 66%|██████▌   | 1266/1929 [07:37<03:59,  2.77batches/s]\u001b[A\n",
            " 66%|██████▌   | 1267/1929 [07:37<03:58,  2.77batches/s]\u001b[A\n",
            " 66%|██████▌   | 1268/1929 [07:37<03:58,  2.78batches/s]\u001b[A\n",
            " 66%|██████▌   | 1269/1929 [07:38<03:57,  2.78batches/s]\u001b[A\n",
            " 66%|██████▌   | 1270/1929 [07:38<03:56,  2.78batches/s]\u001b[A\n",
            " 66%|██████▌   | 1271/1929 [07:38<03:56,  2.78batches/s]\u001b[A\n",
            " 66%|██████▌   | 1272/1929 [07:39<03:56,  2.78batches/s]\u001b[A\n",
            " 66%|██████▌   | 1273/1929 [07:39<03:56,  2.78batches/s]\u001b[A\n",
            " 66%|██████▌   | 1274/1929 [07:39<03:55,  2.78batches/s]\u001b[A\n",
            " 66%|██████▌   | 1275/1929 [07:40<03:56,  2.77batches/s]\u001b[A\n",
            " 66%|██████▌   | 1276/1929 [07:40<03:55,  2.77batches/s]\u001b[A\n",
            " 66%|██████▌   | 1277/1929 [07:41<03:54,  2.78batches/s]\u001b[A\n",
            " 66%|██████▋   | 1278/1929 [07:41<03:54,  2.78batches/s]\u001b[A\n",
            " 66%|██████▋   | 1279/1929 [07:41<03:53,  2.78batches/s]\u001b[A\n",
            " 66%|██████▋   | 1280/1929 [07:42<03:52,  2.79batches/s]\u001b[A\n",
            " 66%|██████▋   | 1281/1929 [07:42<03:53,  2.78batches/s]\u001b[A\n",
            " 66%|██████▋   | 1282/1929 [07:42<03:52,  2.78batches/s]\u001b[A\n",
            " 67%|██████▋   | 1283/1929 [07:43<03:52,  2.78batches/s]\u001b[A\n",
            " 67%|██████▋   | 1284/1929 [07:43<03:51,  2.78batches/s]\u001b[A\n",
            " 67%|██████▋   | 1285/1929 [07:43<03:52,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1286/1929 [07:44<03:51,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1287/1929 [07:44<03:52,  2.76batches/s]\u001b[A\n",
            " 67%|██████▋   | 1288/1929 [07:44<03:51,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1289/1929 [07:45<03:51,  2.76batches/s]\u001b[A\n",
            " 67%|██████▋   | 1290/1929 [07:45<03:51,  2.76batches/s]\u001b[A\n",
            " 67%|██████▋   | 1291/1929 [07:46<03:51,  2.76batches/s]\u001b[A\n",
            " 67%|██████▋   | 1292/1929 [07:46<03:50,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1293/1929 [07:46<03:49,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1294/1929 [07:47<03:49,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1295/1929 [07:47<03:48,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1296/1929 [07:47<03:49,  2.76batches/s]\u001b[A\n",
            " 67%|██████▋   | 1297/1929 [07:48<03:48,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1298/1929 [07:48<03:47,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1299/1929 [07:48<03:47,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1300/1929 [07:49<03:47,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1301/1929 [07:49<03:46,  2.77batches/s]\u001b[A\n",
            " 67%|██████▋   | 1302/1929 [07:50<03:46,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1303/1929 [07:50<03:45,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1304/1929 [07:50<03:44,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1305/1929 [07:51<03:45,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1306/1929 [07:51<03:44,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1307/1929 [07:51<03:44,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1308/1929 [07:52<03:43,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1309/1929 [07:52<03:43,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1310/1929 [07:52<03:44,  2.76batches/s]\u001b[A\n",
            " 68%|██████▊   | 1311/1929 [07:53<03:43,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1312/1929 [07:53<03:42,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1313/1929 [07:53<03:41,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1314/1929 [07:54<03:41,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1315/1929 [07:54<03:40,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1316/1929 [07:55<03:41,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1317/1929 [07:55<03:40,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1318/1929 [07:55<03:39,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1319/1929 [07:56<03:39,  2.78batches/s]\u001b[A\n",
            " 68%|██████▊   | 1320/1929 [07:56<03:40,  2.77batches/s]\u001b[A\n",
            " 68%|██████▊   | 1321/1929 [07:56<03:39,  2.77batches/s]\u001b[A\n",
            " 69%|██████▊   | 1322/1929 [07:57<03:39,  2.76batches/s]\u001b[A\n",
            " 69%|██████▊   | 1323/1929 [07:57<03:38,  2.77batches/s]\u001b[A\n",
            " 69%|██████▊   | 1324/1929 [07:57<03:38,  2.77batches/s]\u001b[A\n",
            " 69%|██████▊   | 1325/1929 [07:58<03:37,  2.78batches/s]\u001b[A\n",
            " 69%|██████▊   | 1326/1929 [07:58<03:37,  2.77batches/s]\u001b[A\n",
            " 69%|██████▉   | 1327/1929 [07:59<03:36,  2.78batches/s]\u001b[A\n",
            " 69%|██████▉   | 1328/1929 [07:59<03:36,  2.78batches/s]\u001b[A\n",
            " 69%|██████▉   | 1329/1929 [07:59<03:35,  2.78batches/s]\u001b[A\n",
            " 69%|██████▉   | 1330/1929 [08:00<03:35,  2.78batches/s]\u001b[A\n",
            " 69%|██████▉   | 1331/1929 [08:00<03:34,  2.79batches/s]\u001b[A\n",
            " 69%|██████▉   | 1332/1929 [08:00<03:34,  2.78batches/s]\u001b[A\n",
            " 69%|██████▉   | 1333/1929 [08:01<03:34,  2.78batches/s]\u001b[A\n",
            " 69%|██████▉   | 1334/1929 [08:01<03:33,  2.78batches/s]\u001b[A\n",
            " 69%|██████▉   | 1335/1929 [08:01<03:34,  2.77batches/s]\u001b[A\n",
            " 69%|██████▉   | 1336/1929 [08:02<03:34,  2.77batches/s]\u001b[A\n",
            " 69%|██████▉   | 1337/1929 [08:02<03:33,  2.77batches/s]\u001b[A\n",
            " 69%|██████▉   | 1338/1929 [08:03<03:33,  2.77batches/s]\u001b[A\n",
            " 69%|██████▉   | 1339/1929 [08:03<03:33,  2.77batches/s]\u001b[A\n",
            " 69%|██████▉   | 1340/1929 [08:03<03:32,  2.77batches/s]\u001b[A\n",
            " 70%|██████▉   | 1341/1929 [08:04<03:32,  2.77batches/s]\u001b[A\n",
            " 70%|██████▉   | 1342/1929 [08:04<03:31,  2.77batches/s]\u001b[A\n",
            " 70%|██████▉   | 1343/1929 [08:04<03:31,  2.77batches/s]\u001b[A\n",
            " 70%|██████▉   | 1344/1929 [08:05<03:31,  2.77batches/s]\u001b[A\n",
            " 70%|██████▉   | 1345/1929 [08:05<03:31,  2.76batches/s]\u001b[A\n",
            " 70%|██████▉   | 1346/1929 [08:05<03:31,  2.75batches/s]\u001b[A\n",
            " 70%|██████▉   | 1347/1929 [08:06<03:31,  2.76batches/s]\u001b[A\n",
            " 70%|██████▉   | 1348/1929 [08:06<03:30,  2.76batches/s]\u001b[A\n",
            " 70%|██████▉   | 1349/1929 [08:06<03:29,  2.77batches/s]\u001b[A\n",
            " 70%|██████▉   | 1350/1929 [08:07<03:28,  2.77batches/s]\u001b[A\n",
            " 70%|███████   | 1351/1929 [08:07<03:28,  2.77batches/s]\u001b[A\n",
            " 70%|███████   | 1352/1929 [08:08<03:28,  2.77batches/s]\u001b[A\n",
            " 70%|███████   | 1353/1929 [08:08<03:27,  2.77batches/s]\u001b[A\n",
            " 70%|███████   | 1354/1929 [08:08<03:27,  2.77batches/s]\u001b[A\n",
            " 70%|███████   | 1355/1929 [08:09<03:27,  2.77batches/s]\u001b[A\n",
            " 70%|███████   | 1356/1929 [08:09<03:26,  2.77batches/s]\u001b[A\n",
            " 70%|███████   | 1357/1929 [08:09<03:27,  2.76batches/s]\u001b[A\n",
            " 70%|███████   | 1358/1929 [08:10<03:26,  2.76batches/s]\u001b[A\n",
            " 70%|███████   | 1359/1929 [08:10<03:26,  2.76batches/s]\u001b[A\n",
            " 71%|███████   | 1360/1929 [08:10<03:25,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1361/1929 [08:11<03:25,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1362/1929 [08:11<03:24,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1363/1929 [08:12<03:24,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1364/1929 [08:12<03:24,  2.76batches/s]\u001b[A\n",
            " 71%|███████   | 1365/1929 [08:12<03:24,  2.76batches/s]\u001b[A\n",
            " 71%|███████   | 1366/1929 [08:13<03:23,  2.76batches/s]\u001b[A\n",
            " 71%|███████   | 1367/1929 [08:13<03:22,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1368/1929 [08:13<03:22,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1369/1929 [08:14<03:22,  2.76batches/s]\u001b[A\n",
            " 71%|███████   | 1370/1929 [08:14<03:21,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1371/1929 [08:14<03:21,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1372/1929 [08:15<03:20,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1373/1929 [08:15<03:20,  2.77batches/s]\u001b[A\n",
            " 71%|███████   | 1374/1929 [08:16<03:20,  2.77batches/s]\u001b[A\n",
            " 71%|███████▏  | 1375/1929 [08:16<03:19,  2.77batches/s]\u001b[A\n",
            " 71%|███████▏  | 1376/1929 [08:16<03:19,  2.77batches/s]\u001b[A\n",
            " 71%|███████▏  | 1377/1929 [08:17<03:19,  2.77batches/s]\u001b[A\n",
            " 71%|███████▏  | 1378/1929 [08:17<03:19,  2.76batches/s]\u001b[A\n",
            " 71%|███████▏  | 1379/1929 [08:17<03:19,  2.76batches/s]\u001b[A\n",
            " 72%|███████▏  | 1380/1929 [08:18<03:18,  2.76batches/s]\u001b[A\n",
            " 72%|███████▏  | 1381/1929 [08:18<03:18,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1382/1929 [08:18<03:18,  2.76batches/s]\u001b[A\n",
            " 72%|███████▏  | 1383/1929 [08:19<03:17,  2.76batches/s]\u001b[A\n",
            " 72%|███████▏  | 1384/1929 [08:19<03:16,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1385/1929 [08:19<03:16,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1386/1929 [08:20<03:15,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1387/1929 [08:20<03:15,  2.78batches/s]\u001b[A\n",
            " 72%|███████▏  | 1388/1929 [08:21<03:14,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1389/1929 [08:21<03:15,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1390/1929 [08:21<03:15,  2.76batches/s]\u001b[A\n",
            " 72%|███████▏  | 1391/1929 [08:22<03:14,  2.76batches/s]\u001b[A\n",
            " 72%|███████▏  | 1392/1929 [08:22<03:14,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1393/1929 [08:22<03:13,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1394/1929 [08:23<03:13,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1395/1929 [08:23<03:12,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1396/1929 [08:23<03:12,  2.77batches/s]\u001b[A\n",
            " 72%|███████▏  | 1397/1929 [08:24<03:11,  2.78batches/s]\u001b[A\n",
            " 72%|███████▏  | 1398/1929 [08:24<03:11,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1399/1929 [08:25<03:10,  2.78batches/s]\u001b[A\n",
            " 73%|███████▎  | 1400/1929 [08:25<03:10,  2.78batches/s]\u001b[A\n",
            " 73%|███████▎  | 1401/1929 [08:25<03:10,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1402/1929 [08:26<03:10,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1403/1929 [08:26<03:09,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1404/1929 [08:26<03:09,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1405/1929 [08:27<03:09,  2.76batches/s]\u001b[A\n",
            " 73%|███████▎  | 1406/1929 [08:27<03:09,  2.76batches/s]\u001b[A\n",
            " 73%|███████▎  | 1407/1929 [08:27<03:08,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1408/1929 [08:28<03:07,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1409/1929 [08:28<03:07,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1410/1929 [08:29<03:06,  2.78batches/s]\u001b[A\n",
            " 73%|███████▎  | 1411/1929 [08:29<03:06,  2.78batches/s]\u001b[A\n",
            " 73%|███████▎  | 1412/1929 [08:29<03:06,  2.78batches/s]\u001b[A\n",
            " 73%|███████▎  | 1413/1929 [08:30<03:05,  2.78batches/s]\u001b[A\n",
            " 73%|███████▎  | 1414/1929 [08:30<03:05,  2.78batches/s]\u001b[A\n",
            " 73%|███████▎  | 1415/1929 [08:30<03:06,  2.76batches/s]\u001b[A\n",
            " 73%|███████▎  | 1416/1929 [08:31<03:05,  2.77batches/s]\u001b[A\n",
            " 73%|███████▎  | 1417/1929 [08:31<03:05,  2.76batches/s]\u001b[A\n",
            " 74%|███████▎  | 1418/1929 [08:31<03:04,  2.76batches/s]\u001b[A\n",
            " 74%|███████▎  | 1419/1929 [08:32<03:04,  2.77batches/s]\u001b[A\n",
            " 74%|███████▎  | 1420/1929 [08:32<03:03,  2.77batches/s]\u001b[A\n",
            " 74%|███████▎  | 1421/1929 [08:32<03:03,  2.78batches/s]\u001b[A\n",
            " 74%|███████▎  | 1422/1929 [08:33<03:02,  2.78batches/s]\u001b[A\n",
            " 74%|███████▍  | 1423/1929 [08:33<03:02,  2.78batches/s]\u001b[A\n",
            " 74%|███████▍  | 1424/1929 [08:34<03:02,  2.77batches/s]\u001b[A\n",
            " 74%|███████▍  | 1425/1929 [08:34<03:01,  2.77batches/s]\u001b[A\n",
            " 74%|███████▍  | 1426/1929 [08:34<03:01,  2.77batches/s]\u001b[A\n",
            " 74%|███████▍  | 1427/1929 [08:35<03:01,  2.77batches/s]\u001b[A\n",
            " 74%|███████▍  | 1428/1929 [08:35<03:00,  2.77batches/s]\u001b[A\n",
            " 74%|███████▍  | 1429/1929 [08:35<03:00,  2.77batches/s]\u001b[A\n",
            " 74%|███████▍  | 1430/1929 [08:36<02:59,  2.77batches/s]\u001b[A\n",
            " 74%|███████▍  | 1431/1929 [08:36<02:59,  2.78batches/s]\u001b[A\n",
            " 74%|███████▍  | 1432/1929 [08:36<02:59,  2.78batches/s]\u001b[A\n",
            " 74%|███████▍  | 1433/1929 [08:37<02:58,  2.78batches/s]\u001b[A\n",
            " 74%|███████▍  | 1434/1929 [08:37<02:58,  2.78batches/s]\u001b[A\n",
            " 74%|███████▍  | 1435/1929 [08:38<02:57,  2.78batches/s]\u001b[A\n",
            " 74%|███████▍  | 1436/1929 [08:38<02:57,  2.78batches/s]\u001b[A\n",
            " 74%|███████▍  | 1437/1929 [08:38<02:57,  2.77batches/s]\u001b[A\n",
            " 75%|███████▍  | 1438/1929 [08:39<02:56,  2.78batches/s]\u001b[A\n",
            " 75%|███████▍  | 1439/1929 [08:39<02:56,  2.77batches/s]\u001b[A\n",
            " 75%|███████▍  | 1440/1929 [08:39<02:56,  2.76batches/s]\u001b[A\n",
            " 75%|███████▍  | 1441/1929 [08:40<02:56,  2.76batches/s]\u001b[A\n",
            " 75%|███████▍  | 1442/1929 [08:40<02:56,  2.76batches/s]\u001b[A\n",
            " 75%|███████▍  | 1443/1929 [08:40<02:55,  2.76batches/s]\u001b[A\n",
            " 75%|███████▍  | 1444/1929 [08:41<02:55,  2.77batches/s]\u001b[A\n",
            " 75%|███████▍  | 1445/1929 [08:41<02:55,  2.77batches/s]\u001b[A\n",
            " 75%|███████▍  | 1446/1929 [08:42<02:55,  2.76batches/s]\u001b[A\n",
            " 75%|███████▌  | 1447/1929 [08:42<02:54,  2.75batches/s]\u001b[A\n",
            " 75%|███████▌  | 1448/1929 [08:42<02:54,  2.76batches/s]\u001b[A\n",
            " 75%|███████▌  | 1449/1929 [08:43<02:53,  2.77batches/s]\u001b[A\n",
            " 75%|███████▌  | 1450/1929 [08:43<02:53,  2.76batches/s]\u001b[A\n",
            " 75%|███████▌  | 1451/1929 [08:43<02:52,  2.77batches/s]\u001b[A\n",
            " 75%|███████▌  | 1452/1929 [08:44<02:52,  2.77batches/s]\u001b[A\n",
            " 75%|███████▌  | 1453/1929 [08:44<02:52,  2.77batches/s]\u001b[A\n",
            " 75%|███████▌  | 1454/1929 [08:44<02:51,  2.77batches/s]\u001b[A\n",
            " 75%|███████▌  | 1455/1929 [08:45<02:50,  2.77batches/s]\u001b[A\n",
            " 75%|███████▌  | 1456/1929 [08:45<02:50,  2.78batches/s]\u001b[A\n",
            " 76%|███████▌  | 1457/1929 [08:45<02:49,  2.78batches/s]\u001b[A\n",
            " 76%|███████▌  | 1458/1929 [08:46<02:49,  2.77batches/s]\u001b[A\n",
            " 76%|███████▌  | 1459/1929 [08:46<02:49,  2.77batches/s]\u001b[A\n",
            " 76%|███████▌  | 1460/1929 [08:47<02:49,  2.77batches/s]\u001b[A\n",
            " 76%|███████▌  | 1461/1929 [08:47<02:49,  2.77batches/s]\u001b[A\n",
            " 76%|███████▌  | 1462/1929 [08:47<02:48,  2.77batches/s]\u001b[A\n",
            " 76%|███████▌  | 1463/1929 [08:48<02:48,  2.76batches/s]\u001b[A\n",
            " 76%|███████▌  | 1464/1929 [08:48<02:48,  2.76batches/s]\u001b[A\n",
            " 76%|███████▌  | 1465/1929 [08:48<02:48,  2.76batches/s]\u001b[A\n",
            " 76%|███████▌  | 1466/1929 [08:49<02:47,  2.76batches/s]\u001b[A\n",
            " 76%|███████▌  | 1467/1929 [08:49<02:47,  2.76batches/s]\u001b[A\n",
            " 76%|███████▌  | 1468/1929 [08:49<02:47,  2.76batches/s]\u001b[A\n",
            " 76%|███████▌  | 1469/1929 [08:50<02:46,  2.77batches/s]\u001b[A\n",
            " 76%|███████▌  | 1470/1929 [08:50<02:45,  2.77batches/s]\u001b[A\n",
            " 76%|███████▋  | 1471/1929 [08:51<02:45,  2.77batches/s]\u001b[A\n",
            " 76%|███████▋  | 1472/1929 [08:51<02:45,  2.77batches/s]\u001b[A\n",
            " 76%|███████▋  | 1473/1929 [08:51<02:44,  2.77batches/s]\u001b[A\n",
            " 76%|███████▋  | 1474/1929 [08:52<02:44,  2.76batches/s]\u001b[A\n",
            " 76%|███████▋  | 1475/1929 [08:52<02:44,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1476/1929 [08:52<02:43,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1477/1929 [08:53<02:43,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1478/1929 [08:53<02:43,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1479/1929 [08:53<02:43,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1480/1929 [08:54<02:42,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1481/1929 [08:54<02:41,  2.77batches/s]\u001b[A\n",
            " 77%|███████▋  | 1482/1929 [08:55<02:41,  2.77batches/s]\u001b[A\n",
            " 77%|███████▋  | 1483/1929 [08:55<02:40,  2.77batches/s]\u001b[A\n",
            " 77%|███████▋  | 1484/1929 [08:55<02:40,  2.78batches/s]\u001b[A\n",
            " 77%|███████▋  | 1485/1929 [08:56<02:40,  2.77batches/s]\u001b[A\n",
            " 77%|███████▋  | 1486/1929 [08:56<02:39,  2.77batches/s]\u001b[A\n",
            " 77%|███████▋  | 1487/1929 [08:56<02:39,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1488/1929 [08:57<02:39,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1489/1929 [08:57<02:39,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1490/1929 [08:57<02:39,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1491/1929 [08:58<02:38,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1492/1929 [08:58<02:38,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1493/1929 [08:59<02:37,  2.76batches/s]\u001b[A\n",
            " 77%|███████▋  | 1494/1929 [08:59<02:37,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1495/1929 [08:59<02:37,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1496/1929 [09:00<02:36,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1497/1929 [09:00<02:36,  2.77batches/s]\u001b[A\n",
            " 78%|███████▊  | 1498/1929 [09:00<02:35,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1499/1929 [09:01<02:35,  2.77batches/s]\u001b[A\n",
            " 78%|███████▊  | 1500/1929 [09:01<02:34,  2.77batches/s]\u001b[A\n",
            " 78%|███████▊  | 1501/1929 [09:01<02:34,  2.77batches/s]\u001b[A\n",
            " 78%|███████▊  | 1502/1929 [09:02<02:33,  2.77batches/s]\u001b[A\n",
            " 78%|███████▊  | 1503/1929 [09:02<02:34,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1504/1929 [09:02<02:33,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1505/1929 [09:03<02:33,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1506/1929 [09:03<02:32,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1507/1929 [09:04<02:32,  2.77batches/s]\u001b[A\n",
            " 78%|███████▊  | 1508/1929 [09:04<02:32,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1509/1929 [09:04<02:32,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1510/1929 [09:05<02:31,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1511/1929 [09:05<02:31,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1512/1929 [09:05<02:30,  2.76batches/s]\u001b[A\n",
            " 78%|███████▊  | 1513/1929 [09:06<02:30,  2.77batches/s]\u001b[A\n",
            " 78%|███████▊  | 1514/1929 [09:06<02:29,  2.77batches/s]\u001b[A\n",
            " 79%|███████▊  | 1515/1929 [09:06<02:30,  2.76batches/s]\u001b[A\n",
            " 79%|███████▊  | 1516/1929 [09:07<02:29,  2.76batches/s]\u001b[A\n",
            " 79%|███████▊  | 1517/1929 [09:07<02:29,  2.76batches/s]\u001b[A\n",
            " 79%|███████▊  | 1518/1929 [09:08<02:28,  2.76batches/s]\u001b[A\n",
            " 79%|███████▊  | 1519/1929 [09:08<02:28,  2.77batches/s]\u001b[A\n",
            " 79%|███████▉  | 1520/1929 [09:08<02:28,  2.76batches/s]\u001b[A\n",
            " 79%|███████▉  | 1521/1929 [09:09<02:27,  2.76batches/s]\u001b[A\n",
            " 79%|███████▉  | 1522/1929 [09:09<02:28,  2.74batches/s]\u001b[A\n",
            " 79%|███████▉  | 1523/1929 [09:09<02:27,  2.75batches/s]\u001b[A\n",
            " 79%|███████▉  | 1524/1929 [09:10<02:27,  2.75batches/s]\u001b[A\n",
            " 79%|███████▉  | 1525/1929 [09:10<02:26,  2.76batches/s]\u001b[A\n",
            " 79%|███████▉  | 1526/1929 [09:10<02:26,  2.76batches/s]\u001b[A\n",
            " 79%|███████▉  | 1527/1929 [09:11<02:25,  2.76batches/s]\u001b[A\n",
            " 79%|███████▉  | 1528/1929 [09:11<02:25,  2.76batches/s]\u001b[A\n",
            " 79%|███████▉  | 1529/1929 [09:12<02:24,  2.76batches/s]\u001b[A\n",
            " 79%|███████▉  | 1530/1929 [09:12<02:24,  2.77batches/s]\u001b[A\n",
            " 79%|███████▉  | 1531/1929 [09:12<02:23,  2.77batches/s]\u001b[A\n",
            " 79%|███████▉  | 1532/1929 [09:13<02:23,  2.77batches/s]\u001b[A\n",
            " 79%|███████▉  | 1533/1929 [09:13<02:23,  2.76batches/s]\u001b[A\n",
            " 80%|███████▉  | 1534/1929 [09:13<02:23,  2.76batches/s]\u001b[A\n",
            " 80%|███████▉  | 1535/1929 [09:14<02:22,  2.76batches/s]\u001b[A\n",
            " 80%|███████▉  | 1536/1929 [09:14<02:22,  2.76batches/s]\u001b[A\n",
            " 80%|███████▉  | 1537/1929 [09:14<02:21,  2.76batches/s]\u001b[A\n",
            " 80%|███████▉  | 1538/1929 [09:15<02:21,  2.76batches/s]\u001b[A\n",
            " 80%|███████▉  | 1539/1929 [09:15<02:21,  2.76batches/s]\u001b[A\n",
            " 80%|███████▉  | 1540/1929 [09:16<02:20,  2.77batches/s]\u001b[A\n",
            " 80%|███████▉  | 1541/1929 [09:16<02:20,  2.77batches/s]\u001b[A\n",
            " 80%|███████▉  | 1542/1929 [09:16<02:19,  2.77batches/s]\u001b[A\n",
            " 80%|███████▉  | 1543/1929 [09:17<02:19,  2.76batches/s]\u001b[A\n",
            " 80%|████████  | 1544/1929 [09:17<02:19,  2.76batches/s]\u001b[A\n",
            " 80%|████████  | 1545/1929 [09:17<02:18,  2.77batches/s]\u001b[A\n",
            " 80%|████████  | 1546/1929 [09:18<02:18,  2.76batches/s]\u001b[A\n",
            " 80%|████████  | 1547/1929 [09:18<02:18,  2.76batches/s]\u001b[A\n",
            " 80%|████████  | 1548/1929 [09:18<02:18,  2.76batches/s]\u001b[A\n",
            " 80%|████████  | 1549/1929 [09:19<02:18,  2.75batches/s]\u001b[A\n",
            " 80%|████████  | 1550/1929 [09:19<02:17,  2.76batches/s]\u001b[A\n",
            " 80%|████████  | 1551/1929 [09:20<02:17,  2.76batches/s]\u001b[A\n",
            " 80%|████████  | 1552/1929 [09:20<02:16,  2.76batches/s]\u001b[A\n",
            " 81%|████████  | 1553/1929 [09:20<02:15,  2.77batches/s]\u001b[A\n",
            " 81%|████████  | 1554/1929 [09:21<02:16,  2.76batches/s]\u001b[A\n",
            " 81%|████████  | 1555/1929 [09:21<02:15,  2.75batches/s]\u001b[A\n",
            " 81%|████████  | 1556/1929 [09:21<02:15,  2.76batches/s]\u001b[A\n",
            " 81%|████████  | 1557/1929 [09:22<02:14,  2.76batches/s]\u001b[A\n",
            " 81%|████████  | 1558/1929 [09:22<02:14,  2.77batches/s]\u001b[A\n",
            " 81%|████████  | 1559/1929 [09:22<02:13,  2.77batches/s]\u001b[A\n",
            " 81%|████████  | 1560/1929 [09:23<02:13,  2.77batches/s]\u001b[A\n",
            " 81%|████████  | 1561/1929 [09:23<02:12,  2.78batches/s]\u001b[A\n",
            " 81%|████████  | 1562/1929 [09:23<02:12,  2.78batches/s]\u001b[A\n",
            " 81%|████████  | 1563/1929 [09:24<02:11,  2.78batches/s]\u001b[A\n",
            " 81%|████████  | 1564/1929 [09:24<02:11,  2.77batches/s]\u001b[A\n",
            " 81%|████████  | 1565/1929 [09:25<02:11,  2.77batches/s]\u001b[A\n",
            " 81%|████████  | 1566/1929 [09:25<02:10,  2.77batches/s]\u001b[A\n",
            " 81%|████████  | 1567/1929 [09:25<02:11,  2.76batches/s]\u001b[A\n",
            " 81%|████████▏ | 1568/1929 [09:26<02:10,  2.77batches/s]\u001b[A\n",
            " 81%|████████▏ | 1569/1929 [09:26<02:10,  2.77batches/s]\u001b[A\n",
            " 81%|████████▏ | 1570/1929 [09:26<02:09,  2.77batches/s]\u001b[A\n",
            " 81%|████████▏ | 1571/1929 [09:27<02:09,  2.77batches/s]\u001b[A\n",
            " 81%|████████▏ | 1572/1929 [09:27<02:08,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1573/1929 [09:27<02:08,  2.78batches/s]\u001b[A\n",
            " 82%|████████▏ | 1574/1929 [09:28<02:08,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1575/1929 [09:28<02:07,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1576/1929 [09:29<02:07,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1577/1929 [09:29<02:06,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1578/1929 [09:29<02:06,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1579/1929 [09:30<02:06,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1580/1929 [09:30<02:06,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1581/1929 [09:30<02:05,  2.76batches/s]\u001b[A\n",
            " 82%|████████▏ | 1582/1929 [09:31<02:05,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1583/1929 [09:31<02:05,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1584/1929 [09:31<02:04,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1585/1929 [09:32<02:04,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1586/1929 [09:32<02:03,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1587/1929 [09:33<02:03,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1588/1929 [09:33<02:03,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1589/1929 [09:33<02:02,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1590/1929 [09:34<02:02,  2.77batches/s]\u001b[A\n",
            " 82%|████████▏ | 1591/1929 [09:34<02:01,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1592/1929 [09:34<02:01,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1593/1929 [09:35<02:01,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1594/1929 [09:35<02:00,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1595/1929 [09:35<02:00,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1596/1929 [09:36<01:59,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1597/1929 [09:36<01:59,  2.79batches/s]\u001b[A\n",
            " 83%|████████▎ | 1598/1929 [09:36<01:59,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1599/1929 [09:37<01:58,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1600/1929 [09:37<01:58,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1601/1929 [09:38<01:58,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1602/1929 [09:38<01:57,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1603/1929 [09:38<01:57,  2.78batches/s]\u001b[A\n",
            " 83%|████████▎ | 1604/1929 [09:39<01:57,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1605/1929 [09:39<01:57,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1606/1929 [09:39<01:56,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1607/1929 [09:40<01:56,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1608/1929 [09:40<01:55,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1609/1929 [09:40<01:55,  2.77batches/s]\u001b[A\n",
            " 83%|████████▎ | 1610/1929 [09:41<01:55,  2.77batches/s]\u001b[A\n",
            " 84%|████████▎ | 1611/1929 [09:41<01:54,  2.78batches/s]\u001b[A\n",
            " 84%|████████▎ | 1612/1929 [09:42<01:54,  2.77batches/s]\u001b[A\n",
            " 84%|████████▎ | 1613/1929 [09:42<01:53,  2.78batches/s]\u001b[A\n",
            " 84%|████████▎ | 1614/1929 [09:42<01:53,  2.78batches/s]\u001b[A\n",
            " 84%|████████▎ | 1615/1929 [09:43<01:53,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1616/1929 [09:43<01:52,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1617/1929 [09:43<01:52,  2.76batches/s]\u001b[A\n",
            " 84%|████████▍ | 1618/1929 [09:44<01:52,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1619/1929 [09:44<01:51,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1620/1929 [09:44<01:51,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1621/1929 [09:45<01:51,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1622/1929 [09:45<01:50,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1623/1929 [09:45<01:50,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1624/1929 [09:46<01:50,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1625/1929 [09:46<01:49,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1626/1929 [09:47<01:49,  2.76batches/s]\u001b[A\n",
            " 84%|████████▍ | 1627/1929 [09:47<01:49,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1628/1929 [09:47<01:48,  2.77batches/s]\u001b[A\n",
            " 84%|████████▍ | 1629/1929 [09:48<01:48,  2.75batches/s]\u001b[A\n",
            " 84%|████████▍ | 1630/1929 [09:48<01:48,  2.75batches/s]\u001b[A\n",
            " 85%|████████▍ | 1631/1929 [09:48<01:48,  2.76batches/s]\u001b[A\n",
            " 85%|████████▍ | 1632/1929 [09:49<01:47,  2.76batches/s]\u001b[A\n",
            " 85%|████████▍ | 1633/1929 [09:49<01:47,  2.77batches/s]\u001b[A\n",
            " 85%|████████▍ | 1634/1929 [09:49<01:46,  2.76batches/s]\u001b[A\n",
            " 85%|████████▍ | 1635/1929 [09:50<01:46,  2.77batches/s]\u001b[A\n",
            " 85%|████████▍ | 1636/1929 [09:50<01:45,  2.77batches/s]\u001b[A\n",
            " 85%|████████▍ | 1637/1929 [09:51<01:45,  2.77batches/s]\u001b[A\n",
            " 85%|████████▍ | 1638/1929 [09:51<01:45,  2.77batches/s]\u001b[A\n",
            " 85%|████████▍ | 1639/1929 [09:51<01:44,  2.76batches/s]\u001b[A\n",
            " 85%|████████▌ | 1640/1929 [09:52<01:44,  2.76batches/s]\u001b[A\n",
            " 85%|████████▌ | 1641/1929 [09:52<01:43,  2.77batches/s]\u001b[A\n",
            " 85%|████████▌ | 1642/1929 [09:52<01:43,  2.77batches/s]\u001b[A\n",
            " 85%|████████▌ | 1643/1929 [09:53<01:43,  2.77batches/s]\u001b[A\n",
            " 85%|████████▌ | 1644/1929 [09:53<01:42,  2.77batches/s]\u001b[A\n",
            " 85%|████████▌ | 1645/1929 [09:53<01:42,  2.78batches/s]\u001b[A\n",
            " 85%|████████▌ | 1646/1929 [09:54<01:41,  2.78batches/s]\u001b[A\n",
            " 85%|████████▌ | 1647/1929 [09:54<01:41,  2.77batches/s]\u001b[A\n",
            " 85%|████████▌ | 1648/1929 [09:55<01:41,  2.77batches/s]\u001b[A\n",
            " 85%|████████▌ | 1649/1929 [09:55<01:41,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1650/1929 [09:55<01:40,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1651/1929 [09:56<01:40,  2.76batches/s]\u001b[A\n",
            " 86%|████████▌ | 1652/1929 [09:56<01:39,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1653/1929 [09:56<01:39,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1654/1929 [09:57<01:39,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1655/1929 [09:57<01:38,  2.78batches/s]\u001b[A\n",
            " 86%|████████▌ | 1656/1929 [09:57<01:38,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1657/1929 [09:58<01:38,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1658/1929 [09:58<01:37,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1659/1929 [09:58<01:37,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1660/1929 [09:59<01:37,  2.76batches/s]\u001b[A\n",
            " 86%|████████▌ | 1661/1929 [09:59<01:36,  2.76batches/s]\u001b[A\n",
            " 86%|████████▌ | 1662/1929 [10:00<01:36,  2.77batches/s]\u001b[A\n",
            " 86%|████████▌ | 1663/1929 [10:00<01:35,  2.77batches/s]\u001b[A\n",
            " 86%|████████▋ | 1664/1929 [10:00<01:35,  2.78batches/s]\u001b[A\n",
            " 86%|████████▋ | 1665/1929 [10:01<01:35,  2.77batches/s]\u001b[A\n",
            " 86%|████████▋ | 1666/1929 [10:01<01:34,  2.77batches/s]\u001b[A\n",
            " 86%|████████▋ | 1667/1929 [10:01<01:34,  2.77batches/s]\u001b[A\n",
            " 86%|████████▋ | 1668/1929 [10:02<01:34,  2.77batches/s]\u001b[A\n",
            " 87%|████████▋ | 1669/1929 [10:02<01:33,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1670/1929 [10:02<01:33,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1671/1929 [10:03<01:32,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1672/1929 [10:03<01:32,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1673/1929 [10:04<01:32,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1674/1929 [10:04<01:31,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1675/1929 [10:04<01:31,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1676/1929 [10:05<01:31,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1677/1929 [10:05<01:30,  2.77batches/s]\u001b[A\n",
            " 87%|████████▋ | 1678/1929 [10:05<01:30,  2.77batches/s]\u001b[A\n",
            " 87%|████████▋ | 1679/1929 [10:06<01:29,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1680/1929 [10:06<01:29,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1681/1929 [10:06<01:29,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1682/1929 [10:07<01:28,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1683/1929 [10:07<01:28,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1684/1929 [10:07<01:28,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1685/1929 [10:08<01:27,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1686/1929 [10:08<01:27,  2.78batches/s]\u001b[A\n",
            " 87%|████████▋ | 1687/1929 [10:09<01:27,  2.78batches/s]\u001b[A\n",
            " 88%|████████▊ | 1688/1929 [10:09<01:26,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1689/1929 [10:09<01:26,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1690/1929 [10:10<01:25,  2.78batches/s]\u001b[A\n",
            " 88%|████████▊ | 1691/1929 [10:10<01:25,  2.78batches/s]\u001b[A\n",
            " 88%|████████▊ | 1692/1929 [10:10<01:25,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1693/1929 [10:11<01:25,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1694/1929 [10:11<01:24,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1695/1929 [10:11<01:24,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1696/1929 [10:12<01:24,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1697/1929 [10:12<01:23,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1698/1929 [10:13<01:23,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1699/1929 [10:13<01:22,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1700/1929 [10:13<01:22,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1701/1929 [10:14<01:22,  2.78batches/s]\u001b[A\n",
            " 88%|████████▊ | 1702/1929 [10:14<01:21,  2.78batches/s]\u001b[A\n",
            " 88%|████████▊ | 1703/1929 [10:14<01:21,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1704/1929 [10:15<01:21,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1705/1929 [10:15<01:20,  2.77batches/s]\u001b[A\n",
            " 88%|████████▊ | 1706/1929 [10:15<01:20,  2.78batches/s]\u001b[A\n",
            " 88%|████████▊ | 1707/1929 [10:16<01:19,  2.78batches/s]\u001b[A\n",
            " 89%|████████▊ | 1708/1929 [10:16<01:20,  2.75batches/s]\u001b[A\n",
            " 89%|████████▊ | 1709/1929 [10:17<01:19,  2.76batches/s]\u001b[A\n",
            " 89%|████████▊ | 1710/1929 [10:17<01:19,  2.76batches/s]\u001b[A\n",
            " 89%|████████▊ | 1711/1929 [10:17<01:18,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1712/1929 [10:18<01:18,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1713/1929 [10:18<01:17,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1714/1929 [10:18<01:17,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1715/1929 [10:19<01:17,  2.78batches/s]\u001b[A\n",
            " 89%|████████▉ | 1716/1929 [10:19<01:16,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1717/1929 [10:19<01:16,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1718/1929 [10:20<01:16,  2.76batches/s]\u001b[A\n",
            " 89%|████████▉ | 1719/1929 [10:20<01:16,  2.76batches/s]\u001b[A\n",
            " 89%|████████▉ | 1720/1929 [10:20<01:15,  2.76batches/s]\u001b[A\n",
            " 89%|████████▉ | 1721/1929 [10:21<01:15,  2.76batches/s]\u001b[A\n",
            " 89%|████████▉ | 1722/1929 [10:21<01:14,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1723/1929 [10:22<01:14,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1724/1929 [10:22<01:14,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1725/1929 [10:22<01:13,  2.77batches/s]\u001b[A\n",
            " 89%|████████▉ | 1726/1929 [10:23<01:13,  2.77batches/s]\u001b[A\n",
            " 90%|████████▉ | 1727/1929 [10:23<01:12,  2.78batches/s]\u001b[A\n",
            " 90%|████████▉ | 1728/1929 [10:23<01:12,  2.78batches/s]\u001b[A\n",
            " 90%|████████▉ | 1729/1929 [10:24<01:12,  2.78batches/s]\u001b[A\n",
            " 90%|████████▉ | 1730/1929 [10:24<01:11,  2.78batches/s]\u001b[A\n",
            " 90%|████████▉ | 1731/1929 [10:24<01:11,  2.78batches/s]\u001b[A\n",
            " 90%|████████▉ | 1732/1929 [10:25<01:11,  2.77batches/s]\u001b[A\n",
            " 90%|████████▉ | 1733/1929 [10:25<01:10,  2.76batches/s]\u001b[A\n",
            " 90%|████████▉ | 1734/1929 [10:26<01:10,  2.77batches/s]\u001b[A\n",
            " 90%|████████▉ | 1735/1929 [10:26<01:10,  2.77batches/s]\u001b[A\n",
            " 90%|████████▉ | 1736/1929 [10:26<01:09,  2.76batches/s]\u001b[A\n",
            " 90%|█████████ | 1737/1929 [10:27<01:09,  2.77batches/s]\u001b[A\n",
            " 90%|█████████ | 1738/1929 [10:27<01:08,  2.77batches/s]\u001b[A\n",
            " 90%|█████████ | 1739/1929 [10:27<01:08,  2.76batches/s]\u001b[A\n",
            " 90%|█████████ | 1740/1929 [10:28<01:08,  2.75batches/s]\u001b[A\n",
            " 90%|█████████ | 1741/1929 [10:28<01:08,  2.75batches/s]\u001b[A\n",
            " 90%|█████████ | 1742/1929 [10:28<01:07,  2.76batches/s]\u001b[A\n",
            " 90%|█████████ | 1743/1929 [10:29<01:07,  2.77batches/s]\u001b[A\n",
            " 90%|█████████ | 1744/1929 [10:29<01:06,  2.77batches/s]\u001b[A\n",
            " 90%|█████████ | 1745/1929 [10:30<01:06,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1746/1929 [10:30<01:06,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1747/1929 [10:30<01:05,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1748/1929 [10:31<01:05,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1749/1929 [10:31<01:04,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1750/1929 [10:31<01:04,  2.78batches/s]\u001b[A\n",
            " 91%|█████████ | 1751/1929 [10:32<01:04,  2.78batches/s]\u001b[A\n",
            " 91%|█████████ | 1752/1929 [10:32<01:03,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1753/1929 [10:32<01:03,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1754/1929 [10:33<01:03,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1755/1929 [10:33<01:02,  2.78batches/s]\u001b[A\n",
            " 91%|█████████ | 1756/1929 [10:33<01:02,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1757/1929 [10:34<01:02,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1758/1929 [10:34<01:01,  2.77batches/s]\u001b[A\n",
            " 91%|█████████ | 1759/1929 [10:35<01:01,  2.76batches/s]\u001b[A\n",
            " 91%|█████████ | 1760/1929 [10:35<01:01,  2.76batches/s]\u001b[A\n",
            " 91%|█████████▏| 1761/1929 [10:35<01:00,  2.75batches/s]\u001b[A\n",
            " 91%|█████████▏| 1762/1929 [10:36<01:00,  2.76batches/s]\u001b[A\n",
            " 91%|█████████▏| 1763/1929 [10:36<01:00,  2.76batches/s]\u001b[A\n",
            " 91%|█████████▏| 1764/1929 [10:36<00:59,  2.76batches/s]\u001b[A\n",
            " 91%|█████████▏| 1765/1929 [10:37<00:59,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1766/1929 [10:37<00:59,  2.76batches/s]\u001b[A\n",
            " 92%|█████████▏| 1767/1929 [10:37<00:58,  2.76batches/s]\u001b[A\n",
            " 92%|█████████▏| 1768/1929 [10:38<00:58,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1769/1929 [10:38<00:57,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1770/1929 [10:39<00:57,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1771/1929 [10:39<00:57,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1772/1929 [10:39<00:56,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1773/1929 [10:40<00:56,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1774/1929 [10:40<00:55,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1775/1929 [10:40<00:55,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1776/1929 [10:41<00:55,  2.76batches/s]\u001b[A\n",
            " 92%|█████████▏| 1777/1929 [10:41<00:54,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1778/1929 [10:41<00:54,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1779/1929 [10:42<00:54,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1780/1929 [10:42<00:53,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1781/1929 [10:43<00:53,  2.78batches/s]\u001b[A\n",
            " 92%|█████████▏| 1782/1929 [10:43<00:53,  2.76batches/s]\u001b[A\n",
            " 92%|█████████▏| 1783/1929 [10:43<00:52,  2.77batches/s]\u001b[A\n",
            " 92%|█████████▏| 1784/1929 [10:44<00:52,  2.76batches/s]\u001b[A\n",
            " 93%|█████████▎| 1785/1929 [10:44<00:52,  2.77batches/s]\u001b[A\n",
            " 93%|█████████▎| 1786/1929 [10:44<00:51,  2.77batches/s]\u001b[A\n",
            " 93%|█████████▎| 1787/1929 [10:45<00:51,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1788/1929 [10:45<00:50,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1789/1929 [10:45<00:50,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1790/1929 [10:46<00:50,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1791/1929 [10:46<00:49,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1792/1929 [10:46<00:49,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1793/1929 [10:47<00:48,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1794/1929 [10:47<00:48,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1795/1929 [10:48<00:48,  2.78batches/s]\u001b[A\n",
            " 93%|█████████▎| 1796/1929 [10:48<00:47,  2.77batches/s]\u001b[A\n",
            " 93%|█████████▎| 1797/1929 [10:48<00:47,  2.77batches/s]\u001b[A\n",
            " 93%|█████████▎| 1798/1929 [10:49<00:47,  2.77batches/s]\u001b[A\n",
            " 93%|█████████▎| 1799/1929 [10:49<00:47,  2.76batches/s]\u001b[A\n",
            " 93%|█████████▎| 1800/1929 [10:49<00:46,  2.76batches/s]\u001b[A\n",
            " 93%|█████████▎| 1801/1929 [10:50<00:46,  2.72batches/s]\u001b[A\n",
            " 93%|█████████▎| 1802/1929 [10:50<00:46,  2.73batches/s]\u001b[A\n",
            " 93%|█████████▎| 1803/1929 [10:51<00:46,  2.72batches/s]\u001b[A\n",
            " 94%|█████████▎| 1804/1929 [10:51<00:46,  2.71batches/s]\u001b[A\n",
            " 94%|█████████▎| 1805/1929 [10:51<00:45,  2.72batches/s]\u001b[A\n",
            " 94%|█████████▎| 1806/1929 [10:52<00:44,  2.74batches/s]\u001b[A\n",
            " 94%|█████████▎| 1807/1929 [10:52<00:44,  2.75batches/s]\u001b[A\n",
            " 94%|█████████▎| 1808/1929 [10:52<00:43,  2.75batches/s]\u001b[A\n",
            " 94%|█████████▍| 1809/1929 [10:53<00:43,  2.76batches/s]\u001b[A\n",
            " 94%|█████████▍| 1810/1929 [10:53<00:43,  2.76batches/s]\u001b[A\n",
            " 94%|█████████▍| 1811/1929 [10:53<00:42,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1812/1929 [10:54<00:42,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1813/1929 [10:54<00:41,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1814/1929 [10:54<00:41,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1815/1929 [10:55<00:41,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1816/1929 [10:55<00:40,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1817/1929 [10:56<00:40,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1818/1929 [10:56<00:39,  2.78batches/s]\u001b[A\n",
            " 94%|█████████▍| 1819/1929 [10:56<00:39,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1820/1929 [10:57<00:39,  2.76batches/s]\u001b[A\n",
            " 94%|█████████▍| 1821/1929 [10:57<00:38,  2.77batches/s]\u001b[A\n",
            " 94%|█████████▍| 1822/1929 [10:57<00:38,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▍| 1823/1929 [10:58<00:38,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▍| 1824/1929 [10:58<00:37,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▍| 1825/1929 [10:58<00:37,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▍| 1826/1929 [10:59<00:37,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▍| 1827/1929 [10:59<00:36,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▍| 1828/1929 [11:00<00:36,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▍| 1829/1929 [11:00<00:36,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▍| 1830/1929 [11:00<00:35,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▍| 1831/1929 [11:01<00:35,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▍| 1832/1929 [11:01<00:35,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▌| 1833/1929 [11:01<00:34,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▌| 1834/1929 [11:02<00:34,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▌| 1835/1929 [11:02<00:33,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▌| 1836/1929 [11:02<00:33,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▌| 1837/1929 [11:03<00:33,  2.78batches/s]\u001b[A\n",
            " 95%|█████████▌| 1838/1929 [11:03<00:32,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▌| 1839/1929 [11:03<00:32,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▌| 1840/1929 [11:04<00:32,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▌| 1841/1929 [11:04<00:31,  2.77batches/s]\u001b[A\n",
            " 95%|█████████▌| 1842/1929 [11:05<00:31,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1843/1929 [11:05<00:31,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1844/1929 [11:05<00:30,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1845/1929 [11:06<00:30,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1846/1929 [11:06<00:29,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1847/1929 [11:06<00:29,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1848/1929 [11:07<00:29,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1849/1929 [11:07<00:28,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1850/1929 [11:07<00:28,  2.78batches/s]\u001b[A\n",
            " 96%|█████████▌| 1851/1929 [11:08<00:28,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1852/1929 [11:08<00:27,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1853/1929 [11:09<00:27,  2.76batches/s]\u001b[A\n",
            " 96%|█████████▌| 1854/1929 [11:09<00:27,  2.76batches/s]\u001b[A\n",
            " 96%|█████████▌| 1855/1929 [11:09<00:26,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▌| 1856/1929 [11:10<00:26,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▋| 1857/1929 [11:10<00:26,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▋| 1858/1929 [11:10<00:25,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▋| 1859/1929 [11:11<00:25,  2.77batches/s]\u001b[A\n",
            " 96%|█████████▋| 1860/1929 [11:11<00:24,  2.76batches/s]\u001b[A\n",
            " 96%|█████████▋| 1861/1929 [11:11<00:24,  2.76batches/s]\u001b[A\n",
            " 97%|█████████▋| 1862/1929 [11:12<00:24,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1863/1929 [11:12<00:23,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1864/1929 [11:13<00:23,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1865/1929 [11:13<00:23,  2.78batches/s]\u001b[A\n",
            " 97%|█████████▋| 1866/1929 [11:13<00:22,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1867/1929 [11:14<00:22,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1868/1929 [11:14<00:21,  2.78batches/s]\u001b[A\n",
            " 97%|█████████▋| 1869/1929 [11:14<00:21,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1870/1929 [11:15<00:21,  2.78batches/s]\u001b[A\n",
            " 97%|█████████▋| 1871/1929 [11:15<00:20,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1872/1929 [11:15<00:20,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1873/1929 [11:16<00:20,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1874/1929 [11:16<00:19,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1875/1929 [11:16<00:19,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1876/1929 [11:17<00:19,  2.77batches/s]\u001b[A\n",
            " 97%|█████████▋| 1877/1929 [11:17<00:18,  2.74batches/s]\u001b[A\n",
            " 97%|█████████▋| 1878/1929 [11:18<00:18,  2.74batches/s]\u001b[A\n",
            " 97%|█████████▋| 1879/1929 [11:18<00:18,  2.75batches/s]\u001b[A\n",
            " 97%|█████████▋| 1880/1929 [11:18<00:17,  2.76batches/s]\u001b[A\n",
            " 98%|█████████▊| 1881/1929 [11:19<00:17,  2.76batches/s]\u001b[A\n",
            " 98%|█████████▊| 1882/1929 [11:19<00:16,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1883/1929 [11:19<00:16,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1884/1929 [11:20<00:16,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1885/1929 [11:20<00:15,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1886/1929 [11:20<00:15,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1887/1929 [11:21<00:15,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1888/1929 [11:21<00:14,  2.78batches/s]\u001b[A\n",
            " 98%|█████████▊| 1889/1929 [11:22<00:14,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1890/1929 [11:22<00:14,  2.78batches/s]\u001b[A\n",
            " 98%|█████████▊| 1891/1929 [11:22<00:13,  2.75batches/s]\u001b[A\n",
            " 98%|█████████▊| 1892/1929 [11:23<00:13,  2.76batches/s]\u001b[A\n",
            " 98%|█████████▊| 1893/1929 [11:23<00:13,  2.76batches/s]\u001b[A\n",
            " 98%|█████████▊| 1894/1929 [11:23<00:12,  2.76batches/s]\u001b[A\n",
            " 98%|█████████▊| 1895/1929 [11:24<00:12,  2.76batches/s]\u001b[A\n",
            " 98%|█████████▊| 1896/1929 [11:24<00:11,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1897/1929 [11:24<00:11,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1898/1929 [11:25<00:11,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1899/1929 [11:25<00:10,  2.77batches/s]\u001b[A\n",
            " 98%|█████████▊| 1900/1929 [11:26<00:10,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▊| 1901/1929 [11:26<00:10,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▊| 1902/1929 [11:26<00:09,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▊| 1903/1929 [11:27<00:09,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▊| 1904/1929 [11:27<00:09,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1905/1929 [11:27<00:08,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1906/1929 [11:28<00:08,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1907/1929 [11:28<00:07,  2.78batches/s]\u001b[A\n",
            " 99%|█████████▉| 1908/1929 [11:28<00:07,  2.78batches/s]\u001b[A\n",
            " 99%|█████████▉| 1909/1929 [11:29<00:07,  2.78batches/s]\u001b[A\n",
            " 99%|█████████▉| 1910/1929 [11:29<00:06,  2.78batches/s]\u001b[A\n",
            " 99%|█████████▉| 1911/1929 [11:29<00:06,  2.78batches/s]\u001b[A\n",
            " 99%|█████████▉| 1912/1929 [11:30<00:06,  2.78batches/s]\u001b[A\n",
            " 99%|█████████▉| 1913/1929 [11:30<00:05,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1914/1929 [11:31<00:05,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1915/1929 [11:31<00:05,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1916/1929 [11:31<00:04,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1917/1929 [11:32<00:04,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1918/1929 [11:32<00:03,  2.77batches/s]\u001b[A\n",
            " 99%|█████████▉| 1919/1929 [11:32<00:03,  2.78batches/s]\u001b[A\n",
            "100%|█████████▉| 1920/1929 [11:33<00:03,  2.79batches/s]\u001b[A\n",
            "100%|█████████▉| 1921/1929 [11:33<00:02,  2.79batches/s]\u001b[A\n",
            "100%|█████████▉| 1922/1929 [11:33<00:02,  2.78batches/s]\u001b[A\n",
            "100%|█████████▉| 1923/1929 [11:34<00:02,  2.78batches/s]\u001b[A\n",
            "100%|█████████▉| 1924/1929 [11:34<00:01,  2.78batches/s]\u001b[A\n",
            "100%|█████████▉| 1925/1929 [11:35<00:01,  2.78batches/s]\u001b[A\n",
            "100%|█████████▉| 1926/1929 [11:35<00:01,  2.78batches/s]\u001b[A\n",
            "100%|█████████▉| 1927/1929 [11:35<00:00,  2.77batches/s]\u001b[A\n",
            "100%|█████████▉| 1928/1929 [11:36<00:00,  2.77batches/s]\u001b[A\n",
            "100%|██████████| 1929/1929 [11:36<00:00,  2.77batches/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Evaluation**\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/119 [11:50<?, ?batches/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.667 | epoch: 2\n",
            "Test Loss: 0.930 | epoch: 2\n",
            "Post-processing 921 example predictions split into 945 features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 921/921 [00:00<00:00, 1788.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction  {'id': 'arabic-2387335860751143628-1', 'prediction_text': '5'}\n",
            "references  {'id': 'arabic-2387335860751143628-1', 'answers': [{'text': 'بطولتين', 'answer_start': 411}]}\n",
            "{'exact_match': 71.66123778501628, 'f1': 84.34441641003214}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Test \\n# Test Phase \\nprint(\\'**Test**\\')        \\nmodel.eval()\\nstart_logits_test=[]\\nend_logits_test=[]\\nrunning_loss=0   \\n\\nwith torch.no_grad():\\n        for i_batch, batch in enumerate(testloader):\\n                    \\n               model.zero_grad(set_to_none=True)\\n               #add data to device (features and labels)\\n               input_ids = batch[0].to(device)\\n               attention_mask = batch[1].to(device)\\n               token_type_ids = batch[2].to(device)\\n               start_positions = batch[3].to(device)\\n               end_positions = batch[4].to(device)\\n               concept_ids = batch[5].to(device)\\n               all_padded_concepts_mask= batch[6].to(device)\\n   \\n               start_logits, end_logits, loss= model(input_ids, attention_mask, token_type_ids, start_positions, end_positions, concept_ids, all_padded_concepts_mask, i_batch, False, False) \\n               running_loss+=loss.item()\\n             \\n               for s in start_logits:\\n                   start_logits_test.append(s)\\n               for e in end_logits:\\n                   end_logits_test.append(e)\\n\\n# validation loss for this epoch\\ntest_loss=running_loss/len(testloader)\\n\\n  \\n \\nfinal_predictions = proc.postprocess_qa_predictions(test_data, test_features, test_example_id, test_offsetmapping , start_logits_test , end_logits_test) \\n          \\nsquad_v2= False\\nmetric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\\n\\n  \\nif squad_v2:\\n       formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\\nelse:\\n       formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\\n       references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in test_data]\\n\\n  # validate on k-fold data\\nfinal_score=metric.compute(predictions=formatted_predictions, references=references)\\nEM = final_score[\\'exact_match\\']\\nF1 = final_score[\\'f1\\']\\n  \\nprint(\\' EM: %.3f | F1: %0.3f \\'%( EM, F1 ))\\nprint(\\' EM: %.3f | F1: %0.3f \\'%( EM, F1))\\n               \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from kge.model import KgeModel\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch.utils.data \n",
        "import pickle\n",
        "import json\n",
        "import sys\n",
        "from typing import List\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from KGE_enriched_PLM.dataloader import DatasetArabicTyDiQA, DataLoaderTyDiQA\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from KGE_enriched_PLM.model_main import KGEEnrichment\n",
        "from KGE_enriched_PLM.preprocess_main import Processing\n",
        "from tqdm import tqdm\n",
        "\n",
        "from kge.util.io import load_checkpoint\n",
        "import torch\n",
        "import pandas as pd\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#**********************main*************************#\n",
        "#******************train******************************#\n",
        "print(len(all_offsetmapping))\n",
        "nb_epochs= 2\n",
        "#lr= 3e-5 \n",
        "lr= 5e-5\n",
        "validate_every=10\n",
        "model_name=\"lanwuwei/GigaBERT-v3-Arabic-and-English\"\n",
        "batch_size= 8\n",
        "# \n",
        "\n",
        "\n",
        "#**********************************\n",
        "\n",
        "\n",
        "# use CUDA\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "train = DatasetArabicTyDiQA(train_data_tensor)\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        " \n",
        "dev = DatasetArabicTyDiQA(dev_data_tensor)\n",
        "dev_loader = DataLoader(dev, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "#test = DatasetArabicTyDiQA(test_data_tensor)\n",
        "#testloader = DataLoader(test, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "#building the model\n",
        "print('Creating model...')\n",
        "model = KGEEnrichment(model_name, embedding_matrix, idx2entity)\n",
        "print('Model created!')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "#initilize optimaization\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "num_training_steps = nb_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_losses=[]\n",
        "test_losses=[]\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "      # training phase  \n",
        "      model.train()\n",
        "      # model.apply(set_bn_eval)\n",
        "      trainloader = tqdm(train_loader, total=len(train_loader), unit=\"batches\")\n",
        "      running_loss = 0\n",
        "      print('****') \n",
        "      for i_batch, batch in enumerate(trainloader):\n",
        "                    \n",
        "           model.zero_grad(set_to_none=True)\n",
        "           #add data to device (features and labels)\n",
        "           input_ids = batch[0].to(device)\n",
        "           attention_mask = batch[1].to(device)\n",
        "           token_type_ids = batch[2].to(device)\n",
        "           start_positions = batch[3].to(device)\n",
        "           end_positions = batch[4].to(device)\n",
        "           concept_ids = batch[5].to(device)\n",
        "           all_padded_concepts_mask= batch[6].to(device)\n",
        "                    \n",
        "           '''\n",
        "           print(\"input_ids\", input_ids.shape)\n",
        "           print(\"attention_mask\", attention_mask.shape)\n",
        "           print(\"token_type_ids\", token_type_ids.shape)\n",
        "           print(\"concept_ids\", concept_ids.shape)\n",
        "           print(\"end_positions\", end_positions.shape)\n",
        "           print(\"all_padded_concepts_mask\", all_padded_concepts_mask.shape)\n",
        "           '''\n",
        "           is_training= True\n",
        "\n",
        "           loss = model(input_ids, attention_mask, token_type_ids, start_positions, end_positions, concept_ids, all_padded_concepts_mask, i_batch,is_training, analysis= False)\n",
        "           loss.backward()\n",
        "           optimizer.step()\n",
        "           running_loss += loss.item()\n",
        "           #loader.set_postfix(Loss=running_loss/((i_batch+1)*batch_size), Epoch=epoch)\n",
        "           #loader.set_postfix(Loss=loss.item(), Epoch=epoch)\n",
        "           #loader.set_description('{}/{}'.format(epoch+1, nb_epochs))\n",
        "           #loader.update()   \n",
        "           lr_scheduler.step()\n",
        "           \n",
        "      train_loss = running_loss/len(trainloader)\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "           \n",
        "      # Evaluation Phase \n",
        "      print('**Evaluation**')        \n",
        "      model.eval()\n",
        "      test_loader = tqdm(dev_loader, total=len(dev_loader), unit=\"batches\")\n",
        "      start_logits_dev=[]\n",
        "      end_logits_dev=[]\n",
        "      running_loss=0\n",
        "\n",
        "    \n",
        "      with torch.no_grad():\n",
        "        for i_batch, batch in enumerate(dev_loader):\n",
        "                    \n",
        "             model.zero_grad(set_to_none=True)\n",
        "             #add data to device (features and labels)\n",
        "             input_ids = batch[0].to(device)\n",
        "             attention_mask = batch[1].to(device)\n",
        "             token_type_ids = batch[2].to(device)\n",
        "             start_positions = batch[3].to(device)\n",
        "             end_positions = batch[4].to(device)\n",
        "             concept_ids = batch[5].to(device)\n",
        "             all_padded_concepts_mask= batch[6].to(device)\n",
        "   \n",
        "             start_logits, end_logits, loss= model(input_ids, attention_mask, token_type_ids, start_positions, end_positions, concept_ids, all_padded_concepts_mask, i_batch, is_training= False, analysis= False) \n",
        "             running_loss+=loss.item()\n",
        "           \n",
        "             for s in start_logits:\n",
        "               start_logits_dev.append(s)\n",
        "             for e in end_logits:\n",
        "               end_logits_dev.append(e)\n",
        "\n",
        "      # validation loss for this epoch\n",
        "      test_loss=running_loss/len(dev_loader)\n",
        "      test_losses.append(test_loss)\n",
        "      \n",
        "      print('Train Loss: %.3f | epoch: %d'%(train_loss,epoch+1))\n",
        "      print('Test Loss: %.3f | epoch: %d'%(test_loss,epoch+1))\n",
        "  \n",
        "'''  \n",
        "plt.plot(train_losses,'-o')\n",
        "plt.plot(test_losses,'-o')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('losses')\n",
        "plt.legend(['Train','Valid'])\n",
        "plt.title('Train vs Valid Losses')\n",
        " \n",
        "plt.show() \n",
        "'''\n",
        "#post processing  validation data to compte mertics\n",
        "final_predictions = proc.postprocess_qa_predictions(dev_data, val_features, val_example_id, all_offsetmapping , start_logits_dev , end_logits_dev)          \n",
        "squad_v2= False\n",
        "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n",
        "\n",
        "if squad_v2:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
        "else:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in dev_data]\n",
        "\n",
        "print(\"prediction \", formatted_predictions[0])\n",
        "print(\"references \", references[0])\n",
        "\n",
        "print(metric.compute(predictions=formatted_predictions, references=references))\n",
        "'''\n",
        "# Test \n",
        "# Test Phase \n",
        "print('**Test**')        \n",
        "model.eval()\n",
        "start_logits_test=[]\n",
        "end_logits_test=[]\n",
        "running_loss=0   \n",
        "\n",
        "with torch.no_grad():\n",
        "        for i_batch, batch in enumerate(testloader):\n",
        "                    \n",
        "               model.zero_grad(set_to_none=True)\n",
        "               #add data to device (features and labels)\n",
        "               input_ids = batch[0].to(device)\n",
        "               attention_mask = batch[1].to(device)\n",
        "               token_type_ids = batch[2].to(device)\n",
        "               start_positions = batch[3].to(device)\n",
        "               end_positions = batch[4].to(device)\n",
        "               concept_ids = batch[5].to(device)\n",
        "               all_padded_concepts_mask= batch[6].to(device)\n",
        "   \n",
        "               start_logits, end_logits, loss= model(input_ids, attention_mask, token_type_ids, start_positions, end_positions, concept_ids, all_padded_concepts_mask, i_batch, False, False) \n",
        "               running_loss+=loss.item()\n",
        "             \n",
        "               for s in start_logits:\n",
        "                   start_logits_test.append(s)\n",
        "               for e in end_logits:\n",
        "                   end_logits_test.append(e)\n",
        "\n",
        "# validation loss for this epoch\n",
        "test_loss=running_loss/len(testloader)\n",
        "\n",
        "  \n",
        " \n",
        "final_predictions = proc.postprocess_qa_predictions(test_data, test_features, test_example_id, test_offsetmapping , start_logits_test , end_logits_test) \n",
        "          \n",
        "squad_v2= False\n",
        "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n",
        "\n",
        "  \n",
        "if squad_v2:\n",
        "       formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
        "else:\n",
        "       formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "       references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in test_data]\n",
        "\n",
        "  # validate on k-fold data\n",
        "final_score=metric.compute(predictions=formatted_predictions, references=references)\n",
        "EM = final_score['exact_match']\n",
        "F1 = final_score['f1']\n",
        "  \n",
        "print(' EM: %.3f | F1: %0.3f '%( EM, F1 ))\n",
        "print(' EM: %.3f | F1: %0.3f '%( EM, F1))\n",
        "               \n",
        "'''    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metric.compute(predictions=formatted_predictions, references=references))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtBCbjO81Ucu",
        "outputId": "afa3bfab-0980-4a75-9825-bc0cbaf8cdbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'exact_match': 16.552315608919383, 'f1': 25.465192720956743}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test \n",
        "# Test Phase \n",
        "print('**Test**')        \n",
        "model.eval()\n",
        "start_logits_test=[]\n",
        "end_logits_test=[]\n",
        "running_loss=0   \n",
        "\n",
        "with torch.no_grad():\n",
        "        for i_batch, batch in enumerate(testloader):\n",
        "                    \n",
        "               model.zero_grad(set_to_none=True)\n",
        "               #add data to device (features and labels)\n",
        "               input_ids = batch[0].to(device)\n",
        "               attention_mask = batch[1].to(device)\n",
        "               token_type_ids = batch[2].to(device)\n",
        "               start_positions = batch[3].to(device)\n",
        "               end_positions = batch[4].to(device)\n",
        "               concept_ids = batch[5].to(device)\n",
        "               all_padded_concepts_mask= batch[6].to(device)\n",
        "   \n",
        "               start_logits, end_logits, loss= model(input_ids, attention_mask, token_type_ids, start_positions, end_positions, concept_ids, all_padded_concepts_mask, i_batch, False, False) \n",
        "               running_loss+=loss.item()\n",
        "             \n",
        "               for s in start_logits:\n",
        "                   start_logits_test.append(s)\n",
        "               for e in end_logits:\n",
        "                   end_logits_test.append(e)\n",
        "\n",
        "# validation loss for this epoch\n",
        "test_loss=running_loss/len(testloader)\n",
        "\n",
        "  \n",
        " \n",
        "final_predictions = proc.postprocess_qa_predictions(test_data, test_features, test_example_id, test_offsetmapping , start_logits_test , end_logits_test) \n",
        "          \n",
        "squad_v2= False\n",
        "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n",
        "\n",
        "  \n",
        "if squad_v2:\n",
        "       formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
        "else:\n",
        "       formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "       references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in test_data]\n",
        "\n",
        "  # validate on k-fold data\n",
        "final_score=metric.compute(predictions=formatted_predictions, references=references)\n",
        "EM = final_score['exact_match']\n",
        "F1 = final_score['f1']\n",
        "  \n",
        "print(' EM: %.3f | F1: %0.3f '%( EM, F1 ))\n",
        "print(' EM: %.3f | F1: %0.3f '%( EM, F1))"
      ],
      "metadata": {
        "id": "cTgsIj58Osop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save predictions and references"
      ],
      "metadata": {
        "id": "bNTC8StSBzm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# save reference\n",
        "pred= pd.DataFrame(formatted_predictions)\n",
        "pred.to_json(\"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/predictions/wikidata_KaifLematha_test_GigaBERT_pred.json\")\n",
        "\n",
        "#ref= pd.DataFrame(references)\n",
        "#ref.to_json(\"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/predictions/wikidata_KaifLematha_test_ref.json\")"
      ],
      "metadata": {
        "id": "2X5BxnlPBy1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sV7NZb8CCDPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2fpwbB9YYg2"
      },
      "source": [
        "## Save and load model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uflsoVDXYYNr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "PATH= \"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/model_dicts/state_dict_model_tydiqa_self_scored_40.pt\"\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7f-uRmJxwq_",
        "outputId": "c80b4b89-7085-47cf-f2a5-e78914c5c86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at lanwuwei/GigaBERT-v3-Arabic-and-English were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KGEEnrichment(\n",
              "  (bert_model): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (embeddings): Embedding(1330292, 128)\n",
              "  (a1): Linear(in_features=768, out_features=128, bias=False)\n",
              "  (output_layer): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (concat_output_layer): Linear(in_features=896, out_features=2, bias=True)\n",
              "  (g_lin1): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (g_lin2): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (projecting_mem): Linear(in_features=128, out_features=768, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "PATH= \"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/model_dicts/state_dict_model_tydiqa_self_scored_40.pt\"\n",
        "model= KGEEnrichment(model_name, embedding_matrix, idx2entity)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEfuZRbZf3nv"
      },
      "source": [
        "# Case Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVaGf_JZ_xEC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df_pred= pd.DataFrame(formatted_predictions)\n",
        "#df_ref= pd.DataFrame(references)\n",
        "\n",
        "#df_pred.to_json(\"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/predictions/GigaBERT_preds.json\")\n",
        "#df_ref.to_json(\"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/predictions/references.json\")\n",
        "\n",
        "#df_model= pd.DataFrame(formatted_predictions)\n",
        "df_pred.to_json(\"/content/drive/MyDrive/KGE_checkpoints/kge-master/KGE_enriched_PLM/predictions/tydiqa_gate_scored_40_preds.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P6DOv9rHrCSY",
        "outputId": "39ee3f8a-a8d3-4403-b104-44bf58a6eae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'بحر مرمرة', 'id': 'arabic-2634668245002151659-0', 'context': 'Coordinates : تصغير | بحر مرمرة كما يبدو من مدينة إسطنبول ويظهر فنار . بحر مرمرة بحر داخلي يربط البحر الأسود ببحر إيجة ويفصل الجزء الآسيوي لتركيا عن جزئها الأوروبي فتطل عليه مدينة إسطنبول . [1][2][3] يتصل بحر مرمرة بالبحر الأسود عن طريق مضيق البسفور وببحر إيجة عن طريق مضيق الدردنيل . تصل مساحته الإجمالية لحوالي 11 . 350 و يبلغ طوله حوالي 225 كم أما عرضه 65 كم عند أقصى اتساع له ، ومتوسط ملوحته هي 22 جزءا في الألف ، وهذه نسبة أعلى من نسبة ملوحة البحر الأسود .', 'question': 'ما هو الممر المائي الذي يربط بحر إيجة ببحر مرمرة ؟', 'answers': [{'text': 'مضيق الدردنيل', 'answer_start': 269}, {'text': 'مضيق الدردنيل', 'answer_start': 269}]}\n",
            "{'id': 'arabic-2634668245002151659-0', 'doc_entities': [{'entity': 'بحر_مرمرة', 'start': 22, 'label': 'B-LOC', 'end': 25, 'kb_entity': [], 'concepts': []}, {'entity': 'بحر_مرمرة', 'start': 26, 'label': 'I-LOC', 'end': 31, 'kb_entity': [], 'concepts': []}, {'entity': 'إسطنبول', 'start': 50, 'label': 'B-LOC', 'end': 57, 'kb_entity': ['اسطنبول', 'اسطنبول'], 'concepts': [4158, 4158, 21170, 553086, 503935, 40329, 8640, 8640, 416002, 665511, 58328, 10988, 978755, 500242, 1032051, 625192, 586408, 589156, 785573, 85034]}, {'entity': 'فنار', 'start': 64, 'label': 'B-LOC', 'end': 68, 'kb_entity': ['نار', 'نار'], 'concepts': [361, 83911, 82787, 935356, 26612, 320888, 212791]}, {'entity': 'بحر_مرمرة', 'start': 71, 'label': 'B-LOC', 'end': 74, 'kb_entity': [], 'concepts': []}, {'entity': 'بحر_مرمرة', 'start': 75, 'label': 'I-LOC', 'end': 80, 'kb_entity': [], 'concepts': []}, {'entity': 'البحر_الأسود', 'start': 96, 'label': 'B-LOC', 'end': 101, 'kb_entity': ['البحر_الاسود', 'البحر_الاسود'], 'concepts': [189, 77292, 4096, 4096, 25206, 11481, 1088200, 553086]}, {'entity': 'البحر_الأسود', 'start': 102, 'label': 'I-LOC', 'end': 108, 'kb_entity': ['البحر_الاسود', 'البحر_الاسود'], 'concepts': [189, 77292, 4096, 4096, 25206, 11481, 1088200, 553086]}, {'entity': 'إيجة', 'start': 114, 'label': 'I-LOC', 'end': 118, 'kb_entity': [], 'concepts': []}, {'entity': 'لتركيا', 'start': 139, 'label': 'B-LOC', 'end': 145, 'kb_entity': ['تركيا', 'تركيا'], 'concepts': [645637, 180661, 572009, 1029401, 4835, 146605, 811223, 906038, 22568, 1045324, 4333, 900836, 385704, 494086, 6582, 1066102, 945273, 252949, 4835, 107718, 4835, 604928, 10467, 1070710, 561986, 175403, 16795, 69649, 106993, 174356, 542281, 226521, 1956, 12504, 35067, 520186, 60248, 43509, 10349, 165]}, {'entity': 'إسطنبول', 'start': 180, 'label': 'B-LOC', 'end': 187, 'kb_entity': ['اسطنبول', 'اسطنبول'], 'concepts': [4158, 4158, 21170, 553086, 503935, 40329, 8640, 8640, 416002, 665511, 58328, 10988, 978755, 500242, 1032051, 625192, 586408, 589156, 785573, 85034]}, {'entity': 'بحر_مرمرة', 'start': 213, 'label': 'B-LOC', 'end': 216, 'kb_entity': [], 'concepts': []}, {'entity': 'بحر_مرمرة', 'start': 217, 'label': 'I-LOC', 'end': 222, 'kb_entity': [], 'concepts': []}, {'entity': 'بالبحر_الأسود', 'start': 223, 'label': 'B-LOC', 'end': 229, 'kb_entity': ['البحر_الاسود', 'البحر_الاسود'], 'concepts': [189, 77292, 4096, 4096, 25206, 11481, 1088200, 553086]}, {'entity': 'بالبحر_الأسود', 'start': 230, 'label': 'I-LOC', 'end': 236, 'kb_entity': ['البحر_الاسود', 'البحر_الاسود'], 'concepts': [189, 77292, 4096, 4096, 25206, 11481, 1088200, 553086]}, {'entity': 'البسفور', 'start': 250, 'label': 'B-LOC', 'end': 257, 'kb_entity': [], 'concepts': []}, {'entity': 'إيجة', 'start': 264, 'label': 'B-LOC', 'end': 268, 'kb_entity': [], 'concepts': []}, {'entity': 'الدردنيل', 'start': 282, 'label': 'B-LOC', 'end': 290, 'kb_entity': ['الدردنيل', 'الدردنيل'], 'concepts': [4158, 17770, 791830, 23937, 672738]}, {'entity': 'البحر_الأسود', 'start': 455, 'label': 'B-LOC', 'end': 460, 'kb_entity': ['البحر_الاسود', 'البحر_الاسود'], 'concepts': [189, 77292, 4096, 4096, 25206, 11481, 1088200, 553086]}, {'entity': 'البحر_الأسود', 'start': 461, 'label': 'I-LOC', 'end': 467, 'kb_entity': ['البحر_الاسود', 'البحر_الاسود'], 'concepts': [189, 77292, 4096, 4096, 25206, 11481, 1088200, 553086]}], 'question_entities': [{'entity': 'بحر_إيجة', 'start': 29, 'label': 'B-LOC', 'end': 32, 'kb_entity': ['بحر_ايجه', 'بحر_ايجه'], 'q_concepts': [1956, 58170, 1956, 4231, 40212, 742134, 198210]}, {'entity': 'بحر_إيجة', 'start': 33, 'label': 'I-LOC', 'end': 37, 'kb_entity': ['بحر_ايجه', 'بحر_ايجه'], 'q_concepts': [1956, 58170, 1956, 4231, 40212, 742134, 198210]}, {'entity': 'ببحر_مرمرة', 'start': 38, 'label': 'B-LOC', 'end': 42, 'kb_entity': [], 'q_concepts': []}, {'entity': 'ببحر_مرمرة', 'start': 43, 'label': 'I-LOC', 'end': 48, 'kb_entity': [], 'q_concepts': []}]}\n",
            "all_input_id shape:  torch.Size([1, 384])\n",
            "all_attention_masks shape:  torch.Size([1, 384])\n",
            "all_token_type_ids shape:  torch.Size([1, 384])\n",
            "all_concept_ids shape:  torch.Size([1, 384, 40])\n",
            "all_padded_concepts_mask shape:  torch.Size([1, 384, 40])\n",
            "all_start_positions shape:  torch.Size([1])\n",
            "all_end_positions shape:  torch.Size([1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?batches/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ouput shape  torch.Size([1, 384, 768])\n",
            "torch.Size([23, 768])\n",
            "torch.Size([16, 768])\n",
            "['[CLS]', 'ما', 'هو', 'الممر', 'المايي', 'الذي', 'يربط', 'بحر', 'ايج', '##ة', 'بب', '##حر', 'مرم', '##رة', '؟', '[SEP]', 'coordinates', ':', 'تص', '##غير', '|', 'بحر', 'مرم', '##رة', 'كما', 'يبدو', 'من', 'مدينة', 'اسطنبول', 'ويظهر', 'فن', '##ار', '.', 'بحر', 'مرم', '##رة', 'بحر', 'داخلي', 'يربط', 'البحر', 'الاسود', 'بب', '##حر', 'ايج', '##ة', 'ويف', '##صل', 'الجزء', 'الاسيوي', 'لتركيا', 'عن', 'جز', '##يها', 'الاوروبي', 'فت', '##طل', 'عليه', 'مدينة', 'اسطنبول', '.', '[', '1', ']', '[', '2', ']', '[', '3', ']', 'يتصل', 'بحر', 'مرم', '##رة', 'بالب', '##حر', 'الاسود', 'عن', 'طريق', 'مضيق', 'الب', '##سف', '##ور', 'وب', '##بحر', 'ايج', '##ة', 'عن', 'طريق', 'مضيق', 'الدرد', '##ني', '##ل', '.', 'تصل', 'مسا', '##حته', 'الاجمالية', 'لح', '##والي', '11', '.', '350', 'و', 'يبلغ', 'طوله', 'حوالي', '225', 'كم', 'اما', 'عرضه', '65', 'كم', 'عند', 'اقصى', 'اتساع', 'له', '،', 'ومت', '##وسط', 'مل', '##وحت', '##ه', 'هي', '22', 'جزءا', 'في', 'الالف', '،', 'وهذه', 'نسبة', 'اعلى', 'من', 'نسبة', 'مل', '##وحة', 'البحر', 'الاسود', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.97batches/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post-processing 1 example predictions split into 1 features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'ما', 'هو', 'الممر', 'المايي', 'الذي', 'يربط', 'بحر', 'ايج', '##ة', 'بب', '##حر', 'مرم', '##رة', '؟', '[SEP]', 'coordinates', ':', 'تص', '##غير', '|', 'بحر', 'مرم', '##رة', 'كما', 'يبدو', 'من', 'مدينة', 'اسطنبول', 'ويظهر', 'فن', '##ار', '.', 'بحر', 'مرم', '##رة', 'بحر', 'داخلي', 'يربط', 'البحر', 'الاسود', 'بب', '##حر', 'ايج', '##ة', 'ويف', '##صل', 'الجزء', 'الاسيوي', 'لتركيا', 'عن', 'جز', '##يها', 'الاوروبي', 'فت', '##طل', 'عليه', 'مدينة', 'اسطنبول', '.', '[', '1', ']', '[', '2', ']', '[', '3', ']', 'يتصل', 'بحر', 'مرم', '##رة', 'بالب', '##حر', 'الاسود', 'عن', 'طريق', 'مضيق', 'الب', '##سف', '##ور', 'وب', '##بحر', 'ايج', '##ة', 'عن', 'طريق', 'مضيق', 'الدرد', '##ني', '##ل', '.', 'تصل', 'مسا', '##حته', 'الاجمالية', 'لح', '##والي', '11', '.', '350', 'و', 'يبلغ', 'طوله', 'حوالي', '225', 'كم', 'اما', 'عرضه', '65', 'كم', 'عند', 'اقصى', 'اتساع', 'له', '،', 'ومت', '##وسط', 'مل', '##وحت', '##ه', 'هي', '22', 'جزءا', 'في', 'الالف', '،', 'وهذه', 'نسبة', 'اعلى', 'من', 'نسبة', 'مل', '##وحة', 'البحر', 'الاسود', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "{'score': 6.766859, 'text': 'مضيق الدردنيل', 'start_logits': 2.0966053, 'end_logits': 4.6702538, 'start_index': 88, 'end_index': 91}\n",
            "{'score': 6.5390224, 'text': 'مضيق البسفور وببحر إيجة عن طريق مضيق الدردنيل', 'start_logits': 1.8687688, 'end_logits': 4.6702538, 'start_index': 78, 'end_index': 91}\n",
            "{'score': 5.8203087, 'text': 'مضيق البسفور', 'start_logits': 1.8687688, 'end_logits': 3.95154, 'start_index': 78, 'end_index': 81}\n",
            "{'score': 4.8115153, 'text': 'يتصل بحر مرمرة بالبحر الأسود عن طريق مضيق البسفور وببحر إيجة عن طريق مضيق الدردنيل', 'start_logits': 0.14126158, 'end_logits': 4.6702538, 'start_index': 69, 'end_index': 91}\n",
            "{'score': 4.3365893, 'text': 'عن طريق مضيق البسفور وببحر إيجة عن طريق مضيق الدردنيل', 'start_logits': -0.3336645, 'end_logits': 4.6702538, 'start_index': 76, 'end_index': 91}\n",
            "384\n",
            "384\n",
            "384\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('arabic-2634668245002151659-0', 'مضيق الدردنيل')])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# check example one \n",
        "# preprocess data\n",
        "import pandas as pd\n",
        "\n",
        "print(dev_data[368])\n",
        "print(wikidata_dev_concepts[368])\n",
        "\n",
        "proc = Processing(entity2idx, model_name)\n",
        "val_features= []\n",
        "all_offsetmapping = []\n",
        "val_example_id = []\n",
        "\n",
        "ft, offset_mapping, example_id = proc.prepare_validation_features(dev_data[368], wikidata_dev_concepts[368])\n",
        "\n",
        "for f in ft:\n",
        "     val_features.append(f)   \n",
        "for off in offset_mapping:\n",
        "     all_offsetmapping.append(off)\n",
        "for ex in example_id:\n",
        "     val_example_id.append(ex)\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Tensors and build dev set\n",
        "\n",
        "all_input_ids = torch.tensor([f['input_ids'] for f in val_features], dtype=torch.long)\n",
        "all_attention_masks = torch.tensor([f['attention_masks'] for f in val_features], dtype=torch.long)\n",
        "all_token_type_ids = torch.tensor([f['token_type_ids'] for f in val_features], dtype=torch.long)\n",
        "all_concept_ids = torch.tensor([f['concept_ids'] for f in val_features], dtype=torch.long)\n",
        "all_padded_concepts_mask = torch.tensor([f['padded_concepts_mask'] for f in val_features], dtype=torch.bool)\n",
        "all_start_positions = torch.tensor([f['start_positions'] for f in val_features], dtype=torch.long)\n",
        "all_end_positions = torch.tensor([f['end_positions'] for f in val_features], dtype=torch.long)\n",
        "\n",
        "print('all_input_id shape: ', all_input_ids.shape)\n",
        "print('all_attention_masks shape: ', all_attention_masks.shape)\n",
        "print('all_token_type_ids shape: ', all_token_type_ids.shape)\n",
        "print('all_concept_ids shape: ', all_concept_ids.shape)\n",
        "print('all_padded_concepts_mask shape: ', all_padded_concepts_mask.shape)\n",
        "print('all_start_positions shape: ', all_start_positions.shape)\n",
        "print('all_end_positions shape: ', all_end_positions.shape)\n",
        "\n",
        "\n",
        "\n",
        "dev_data_tensor = TensorDataset(\n",
        "                all_input_ids,\n",
        "                all_attention_masks,\n",
        "                all_token_type_ids,\n",
        "                all_start_positions,\n",
        "                all_end_positions, \n",
        "                all_concept_ids,\n",
        "                all_padded_concepts_mask\n",
        "                \n",
        "            )\n",
        "\n",
        "\n",
        "dev = DatasetArabicTyDiQA(dev_data_tensor)\n",
        "dev_loader = DataLoader(dev, batch_size=1, num_workers=0)\n",
        "\n",
        "# use CUDA\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_loader = tqdm(dev_loader, total=len(dev_loader), unit=\"batches\")\n",
        "\n",
        "with torch.no_grad():\n",
        "         for i_batch, batch in enumerate(test_loader):\n",
        "                    \n",
        "           model.zero_grad(set_to_none=True)\n",
        "           #add data to device (features and labels)\n",
        "           input_ids = batch[0].to(device)\n",
        "           attention_mask = batch[1].to(device)\n",
        "           token_type_ids = batch[2].to(device)\n",
        "           start_positions = batch[3].to(device)\n",
        "           end_positions = batch[4].to(device)\n",
        "           concept_ids = batch[5].to(device)\n",
        "           all_padded_concepts_mask= batch[6].to(device)\n",
        "   \n",
        "           start_logits, end_logits, loss= model(input_ids, attention_mask, token_type_ids, start_positions, end_positions, concept_ids, all_padded_concepts_mask, 1, is_training= False, analysis= True)\n",
        "\n",
        "proc.postprocess_qa_predictions_analysis(dev_data[368], val_features, val_example_id, all_offsetmapping , start_logits , end_logits)  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}